---
title: "dissertation analysis"
author: "Ying Chen"
date: "19/06/2021"
output: html_document
---

```{r message=FALSE}
library(sf)
library(tidyverse)
library(dplyr)
library(janitor)
library(RColorBrewer)
library(classInt)
library(sp)
library(rgeos)
library(rgdal)
library(spatstat)
library(here)
library(maptools)
library(GISTools)
library(tmap)
library(geojson)
library(geojsonio)
library(tmaptools)
library(hexbin)
library(ggspatial)
library(ggsn)
library(raster)
library(fpc)
library(dbscan)
library(plotrix)
library(spdep)
library(ggplot2)
library(ggpubr)
```

## Understand London Crime in general

-   spatial data: downloaded from London Datastore at <https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london> and stored in the local folder "data".
-   crime data: downloaded from <https://data.police.uk/data/> by selecting only Metropolitan Police Service for data from April 2019 to April 2021, renamed the folder as "crime_data" stored in the local folder "data".

```{r message=FALSE}
boroMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

lsoaMap  <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "LSOA_2011_London_gen_MHW.shp"))%>%
  st_transform(., 27700)

wardMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Ward_CityMerged.shp")) %>% 
  st_transform(., 27700)
```

The following function used to search files that follow a specific pattern is obtained from <https://github.com/sjaraha/clustering-spatiotemporal-data>.

```{r}
list_data_paths <- function(pattern, rec){
  # searches working directory for files that match the specified pattern
  # on match, adds file path to a list
  # returns list the list of matching file paths
  ## pattern (str): regex pattern to match
  ## rec (boolean): recurse into directories (True) or don't (False)
  
  # initialize list
  data_path_list <- c()
  # loop through directories
  for (pd in list.dirs(recursive = rec)){
    # loop through files in directories
    for (f in list.files(pd)){
      # find files that match the pattern
      if (grepl(pattern, f, ignore.case = FALSE)==TRUE){
        # construct path to matching file
        data_path <- paste(pd,f, sep="/")
        # add path to list
        data_path_list <- c(data_path_list,data_path)
      }}}
  # return list of paths to matching files
  return(data_path_list)
}
```

Use the function above and search for all crime data (ended with metropolitan-street.csv) in my local folder.

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
crime_list <- list_data_paths("\\-metropolitan-street.csv$",FALSE) 

crime <- crime_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

Remove duplicated rows, clean names and select points within London.

```{r message = FALSE}
crime<- crime %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

crime_london <- crime[boroMap,]
```

Create a table counting crime numbers for each type.

```{r message=False, warning=False, results='asis'}
library(knitr)
crime_london %>% 
  group_by(crime_type) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London crimes by types from April 2019 to April 2021')
```

I am interested in comparing violent crimes with stop and searches under one particular legislation (section 60) because section 60 is specifically targeted for violent crimes.

```{r}
violence_london <- crime_london %>% 
  filter(., crime_type=='Violence and sexual offences') 
```

What are the most common outcome for violent crimes in London?

```{r message=False, warning=False, results='asis'}
violence_london %>% 
  group_by(last_outcome_category) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by outcomes from April 2019 to April 2021')
```

I want to count the number of violent crimes by boroughs but they record crimes by LSOA, so I need to extract borough names from LSOA names. (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
violence_london$boro_name = substr(violence_london$lsoa_name,1,nchar(violence_london$lsoa_name)-4)

violence_london %>% 
  group_by(boro_name) %>% 
  summarise(., count=n(),) %>% 
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by boroughs from April 2019 to April 2021')
```

## Understand stop and search (S&S) data

Search for stop and search data (ends with -metropolitan-stop-and-search.csv).

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
ss_list <- list_data_paths("\\-metropolitan-stop-and-search.csv$",FALSE) 

ss <- ss_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

clean names, remove duplicated records and select points within London boroughs.

```{r message=FALSE, warning=False}
ss <- ss %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

ss_london <- ss[boroMap,]
```

What are the common legislation and search object during S&S?

```{r message=False, warning=False, results='asis'}
ss_london %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London S&S by legislation and search objects from April 2019 to April 2021')
```

-   The Criminal Justice and Public Order 1994 (section 60) aims to reduce violence in London. From April 2014, the UK government implements the Best Use of Stop and Search Scheme (BUSSS) and it gives more police power to stop and search people without having reasonable grounds for suspicion if violence is anticipated by police. The main purpose of the scheme is to prevent violent crimes (such as knife crime) before it happens.
-   Therefore, comparing the police usage of section 60 in terms of their locations, ethnicity of the person being S&S, time of a day, season of a year, and actual violent crime locationns within MSOA, LSOA or ward can help understand whether S&S is an effective way of deterring violent crimes at local level.

```{r}
ss60_london <- ss_london %>% 
  filter(legislation == "Criminal Justice and Public Order Act 1994 (section 60)") 
```

What are the most common ethnicity of people being S&S for potential violence? (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
ss60_london %>% 
  group_by(outcome, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  arrange(outcome, officer_defined_ethnicity, desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'Ethnicities for people being S&S (s60) from April 2019 to April 2021')
```

### Trend figure

```{r}
# ss60_london %>% 
#   count(year = year(date), officer_defined_ethnicity) %>% 
#   ggplot(aes(x = year, y = n, color = officer_defined_ethnicity)) +
#   geom_point() +
#   geom_line() 
```

## Simple point distribution

```{r}
tm_shape(wardMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(violence_london) +
  tm_dots(col = "blue") +
  tm_layout(main.title="Violent Crimes London April 2019 to April 2021",
            main.title.size=1)

tm_shape(wardMap) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss_london) +
  tm_dots(col = "yellow") +
  tm_layout(main.title="Stop and Search London April 2019 to April 2021", 
            main.title.size=1)
  
tm_shape(wardMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss60_london) +
  tm_dots(col = "red") +
  tm_layout(main.title= "S&S (section 60) London April 2019 to April 2021", 
            main.title.size=1)
```

## dbscan

DBSCAN requires two parameters: 1. Epsilon - the radius within which the algorithm with search for clusters 2. MinPts - the minimum number of points that should be considered a cluster.

Two ways to determine Epsilon: 1. Ripley's K - where K values above the Poisson distribution indicates areas of clustering; needs to find the K values' cutoff between above and below the Poisson distribution. 2. KNNDistance - it is the distance of each point to its k-th nearest neighbor; needs to find a knee in the plot. "The idea behind this heuristic is that points located inside of clusters will have a small k-nearest neighbor distance, because they are close to other points in the same cluster, while noise points are more isolated and will have a rather large kNN distance (Hahsler et al., 2019).

Potential way to determine MinPts: The rule of thumb for setting minPts is to use at least the number of dimensions of the dataset plus one (Hahsler et al., 2019). However, by setting MinPts = 3 (which is what Hahsler et al suggest) it results in 229 unique cluster for all S&S (s60) points, which is too many.

### Black

Observations: 1. if setting the eps as Ripley's K test suggests, the number is small and resulting in some small clusters with very few points within. 2. if setting as kNNdistplot suggests, the "knee" is usually quite large and it will give us a large cluster + some small clusters + isolated noises.

```{r message=FALSE, warning=FALSE}
window <- as.owin(wardMap)

ss60_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") 

ss60_blackSP <- ss60_black %>%  
  as(., 'Spatial') 

ss60_blackPPP <- ppp(x=ss60_blackSP@coords[,1],
                     y=ss60_blackSP@coords[,2],
                     window=window)

ss60_blackPoints<- ss60_blackSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Choose Ripley's K method to select suitable parameters:

```{r message=FALSE, warning=FALSE}
ss60_blackPPP %>%
  Kest(., correction="border") %>%
  plot()
  #abline(v = 450, col = "blue", lty = 2)
```

Plot:

```{r}
library(factoextra)

db_b <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 1000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_b, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=450, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  Choose KNN distance method to select suitable parameters:

```{r}
ss60_blackPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```

plot:

```{r}
db_b_KNN <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_b_KNN, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=2000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

It gives us 2 unique clusters.

### White

```{r message=FALSE, warning=FALSE}
ss60_white<- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") 

ss60_whiteSP <- ss60_white %>%  
  as(., 'Spatial') 

ss60_whitePPP <- ppp(x=ss60_whiteSP@coords[,1],
                     y=ss60_whiteSP@coords[,2],
                     window=window)

ss60_whitePoints<- ss60_whiteSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K method:

```{r message=FALSE, warning=FALSE}
ss60_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 750, col = "blue", lty = 2)
```

```{r}
db_w <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 750, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_w, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=750, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  KNNDistance method:

```{r}
ss60_whitePoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=3000)") %>% 
  abline(h = 3000, col = "red", lty = 2)
```

```{r}
db_w_KNN <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 3000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_w_KNN, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=3000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

### Asian

```{r message=FALSE, warning=FALSE}
ss60_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") 

ss60_asianSP <- ss60_asian %>% 
  as(., 'Spatial') 

ss60_asianPPP <- ppp(x=ss60_asianSP@coords[,1],
                     y=ss60_asianSP@coords[,2],
                     window=window)

ss60_asianPoints<- ss60_asianSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K:

```{r message=FALSE, warning=FALSE}
ss60_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 500, col = "blue", lty = 2)
```

```{r}
db_a <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 500, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_a, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=500, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
db_a2 <- ss60_asianPoints %>%
  dbscan::optics(.,eps = 500, minPts = 50)

plot(db_a2)

test <- extractDBSCAN(db_a2, eps_cl = 400)
plot(test)

hullplot(ss60_asianPoints, test,
         cex = TRUE,
         main="OPTICS Cluster Hulls") 
```

2.  KNNDistance

```{r}
ss60_asianPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=4000)") %>% 
  abline(h = 4000, col = "red", lty = 2)
```

```{r}
db_a_KNN <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 4000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_a_KNN, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=4000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()
```

### With an openstreet basemap

```{r message=FALSE}
library(OpenStreetMap)

# before plotting, we need to transform geometry to (x,y) coordinates and the function was found at <https://maczokni.github.io/crimemapping_textbook_bookdown/more-on-thematic-maps.html>
sfc_as_cols <- function(x, names = c("x","y")) {
  stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
  ret <- sf::st_coordinates(x)
  ret <- tibble::as_tibble(ret)
  stopifnot(length(names) == ncol(ret))
  x <- x[ , !names(x) %in% names]
  ret <- setNames(ret,names)
  dplyr::bind_cols(x,ret)
}

# add (x,y) coordinate and clusters to new columns  
ss60_asian <- sfc_as_cols(ss60_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_a$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_a <- ss60_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# create a bounding box of London
LondonBB <- boroMap %>%
  st_transform(., 4326)%>%
  st_bbox()

# set the basemap showing London 
basemap <- OpenStreetMap::openmap(c(51.2867602,-0.5103751),c(51.6918741,0.3340156), 
                                  zoom=NULL,
                                  "stamen-toner")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

# plot
dbPlotAsian <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsian
```

How about for the black people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_black <- sfc_as_cols(ss60_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_b$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_b <- ss60_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotBlack <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_black, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlack
```

For white people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_white <- sfc_as_cols(ss60_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_w$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_w <- ss60_white %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotWhite <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_white, 
             aes(longitude,latitude), 
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to White People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotWhite
```

### Overlapping map

```{r}
dbplotALL <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  geom_point(data=ss60_black, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  geom_point(data=ss60_white, 
             aes(longitude,latitude), 
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("ALL")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALL
```

## dbscan - only arrest

### Asian

```{r message=FALSE,warning=FALSE}
ss60Arr_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_asianSP <- ss60Arr_asian %>% 
  as(., 'Spatial') 

ss60Arr_asianPPP <- ppp(x=ss60Arr_asianSP@coords[,1],
                     y=ss60Arr_asianSP@coords[,2],
                     window=window)

ss60Arr_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,400)) %>% 
  abline(v = 285, col = "blue", lty = 2)
```

```{r}
ss60Arr_asianPoints<- ss60Arr_asianSP %>%
  geometry(.)%>%
  as.data.frame()

db_aArr <- ss60Arr_asianPoints %>%
  fpc::dbscan(.,eps = 285, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_aArr, ss60Arr_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()
```

```{r}
db_aArr2 <- ss60Arr_asianPoints %>%
  dbscan::optics(.,eps = 500, minPts = 3)

plot(db_aArr2)

test <- extractDBSCAN(db_aArr2, eps_cl = 400)
plot(test)

hullplot(ss60Arr_asianPoints, test,
         cex = TRUE,
         main="OPTICS Cluster Hulls")
```

```{r message=FALSE, warning=FALSE}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_asian <- sfc_as_cols(ss60Arr_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=test$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_aArr <- ss60Arr_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotAsianArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=1)+
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsianArr
```

### Black People

```{r message=False, warning=False}
ss60Arr_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_blackSP <- ss60Arr_black %>% 
  as(., 'Spatial') 

ss60Arr_blackPPP <- ppp(x=ss60Arr_blackSP@coords[,1],
                     y=ss60Arr_blackSP@coords[,2],
                     window=window)

ss60Arr_blackPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1000)) %>% 
  abline(v = 400, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}
ss60Arr_blackPoints<- ss60Arr_blackSP %>%
  geometry(.)%>%
  as.data.frame()

db_bArr <- ss60Arr_blackPoints %>%
  fpc::dbscan(.,eps = 400, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_bArr, ss60Arr_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_black <- sfc_as_cols(ss60Arr_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_bArr$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_bArr <- ss60Arr_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotBlackArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=0.3)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "left") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlackArr
```

### White

```{r message=False, warning=False}
ss60Arr_white <- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_whiteSP <- ss60Arr_white %>% 
  as(., 'Spatial') 

ss60Arr_whitePPP <- ppp(x=ss60Arr_whiteSP@coords[,1],
                     y=ss60Arr_whiteSP@coords[,2],
                     window=window)

ss60Arr_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,600)) %>% 
  abline(v = 330, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}
ss60Arr_whitePoints<- ss60Arr_whiteSP %>%
  geometry(.)%>%
  as.data.frame()

db_wArr <- ss60Arr_whitePoints %>%
  fpc::dbscan(.,eps = 330, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_wArr, ss60Arr_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_white <- sfc_as_cols(ss60Arr_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_wArr$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_wArr <- ss60Arr_white %>% 
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotWhiteArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=0.3)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "left") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotWhiteArr
```

### Overlapping map

```{r}
dbplotALLArr <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.1) +
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.1)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude), 
             size=0.1)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("ALL")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALLArr
```

## st-dbscan

The 'stdbscanr' package and related functions was obtained from Dr.Gordon McDonald's github at <https://github.com/gdmcdonald/stdbscanr>

```{r message=FALSE}
# install.packages("devtools")
# devtools::install_github("gdmcdonald/stdbscanr")
# install.packages("data.table")          

library("data.table")
library(stdbscanr)

ss60_londonTEST <- ss60_london[sample(nrow(ss60_london), 100), ] %>%  # random select 100 rows
  st_transform(., 4326) %>% 
  sfc_as_cols(., c("longitude", "latitude")) %>%    # convert geometry to coordinates
  setDT(.) %>%     # convert to data.table
  setkey(., date)  # sort by data 

# add time intervals in minues between points
ss60_londonTEST[,time_inc := as.numeric(date - shift(date), units = "mins")]

# run st-dbscan
location_with_visits <- 
  get_clusters_from_data(df = ss60_londonTEST,
                         x = "longitude", 
                         y = "latitude", 
                         t = "date",
                         eps = 0.005,  # 0.005 latitude/longitude ~ 500m either way in London
                         eps_t = 1440, # 1440 minutes = 1day
                         minpts = 2)
```

```{r results='asis'}
#Define a mode function to get the most common label
mode <- function(x) { names(which.max(table(x))) }

#data.table summary table of visits
clusters <-                                   
  location_with_visits[     
    !is.na(cluster),                         
    .(n = .N,                               
      latitude = mean(latitude),            
      longitude = mean(longitude), 
      time_spent = sum(time_inc,na.rm = T),
      ethnicity_label = mode(officer_defined_ethnicity)            
    ),    
    by=cluster] 

kable(clusters, caption = "ST-DBSCAN Cluster Labels")
```

```{r message=FALSE}
# order by time
setkey(location_with_visits, date)

library(leaflet)
# plot on leaflet map
leaflet(data = clusters) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addMarkers(popup = ~paste0("Time spent: ",round(time_spent/60, 1), " hours.<br>",
                             "S&S Ethnicity Cluster ",cluster,": ",ethnicity_label,"<br>",
                             "Counts: ", n)) %>% 
  addPolylines(data = location_with_visits, 
               lat = ~latitude, 
               lng = ~longitude)
```

## Add demographic data

We have explored the point pattern analysis for London S&S and how about looking at MSOAs, wards or boroughs as a whole? Where are the hotspots and how to explain them? To do that, we need to count the number of crimes and S&S for each geographic unit. Here, I chose to group by MSOA (averaged with 8,346 population in 2010) and there are 983 MSOAs in London.

The demographic data was obtained from <https://data.london.gov.uk/dataset/msoa-atlas>.

```{r message=FALSE, warning=FALSE}
wardData <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/150584ff-3509-4e17-91d1-315ed4557419/ward-atlas-data.csv",na = c("NA", "n/a")) %>% 
  clean_names() 

```

Clean up column names

```{r}
wardData <- wardData %>% 
  rename(old_code = "x1",
         new_code = "x2",
         borough_name = "x3",
         ward_name = "x4")
```

Notice wardData has 629 rows while the wardMap has 625 rows. After inspection, wardData's last three rows are national statistics and the first row is a part of column name, which need to be removed.

```{r message=FALSE}
wardData <- wardData[2:626,]
```

Columns we need:

> -   "population_and_age_all_ages_2011",
> -   "household_composition_household_composition_2001_census_all_households",
>
> *the above can calculate violent crime rate and overall S&S rate by dividing by the number of crimes/S&S in each geographic unit*
>
> -   "diversity_ethnic_group_5\_groups_2011_census_white",
> -   "diversity_ethnic_group_5\_groups_2011_census_asian_or_asian_british",
> -   "diversity_ethnic_group_5\_groups_2011_census_black_or_black_british",
> -   "diversity_ethnic_group_5\_groups_2011_census_other",
>
> *the above can calculate specific S&S rate for each ethnic group because the S&S data also has ethnic data*
>
> -   "household_income_mean_modelled_household_income_2012_13",
> -   "employment_economic_activity_percentages_2011_census_economically_inactive_percent",
> -   "employment_economic_activity_percentages_2011_census_aged_16_to_24_percent_unemployed",
> -   "employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
> -   "qualifications_qualifications_and_students_2011_census_percent_no_qualifications",
>
> *the above might be useful if to calculate spatial autocorrelation between S&S rate and income/economically inactive rate*

```{r message=FALSE, warning=FALSE}
cols <- c(
"new_code",
"population_and_age_all_ages_2011",
"household_composition_household_composition_2001_census_all_households",
"diversity_ethnic_group_5_groups_2011_census_white",
"diversity_ethnic_group_5_groups_2011_census_asian_or_asian_british",
"diversity_ethnic_group_5_groups_2011_census_black_or_black_british",                         
"diversity_ethnic_group_5_groups_2011_census_other",                        
"employment_economic_activity_percentages_2011_census_economically_inactive_percent",
"employment_economic_activity_percentages_2011_census_aged_16_to_24_percent_unemployed",
"employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
"qualifications_qualifications_and_students_2011_census_percent_no_qualifications")

wardDataMerged <-
  left_join(wardMap %>% dplyr::select(NAME,GSS_CODE,geometry),
            wardData %>% dplyr::select(all_of(cols)),
            by = c("GSS_CODE" = "new_code")) %>% 
  rename(all_population = "population_and_age_all_ages_2011",
         all_households = "household_composition_household_composition_2001_census_all_households",
         white_pop = "diversity_ethnic_group_5_groups_2011_census_white",
         asian_pop = "diversity_ethnic_group_5_groups_2011_census_asian_or_asian_british",
         black_pop = "diversity_ethnic_group_5_groups_2011_census_black_or_black_british",
         otherEth_pop = "diversity_ethnic_group_5_groups_2011_census_other",
         econ_inac_rate = "employment_economic_activity_percentages_2011_census_economically_inactive_percent",
         unemployed_16to24_rate = "employment_economic_activity_percentages_2011_census_aged_16_to_24_percent_unemployed",
         unemployed_loneParent_rate = "employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
         no_qualification_rate = "qualifications_qualifications_and_students_2011_census_percent_no_qualifications",
         ward_name = "NAME",
         ward_code = "GSS_CODE") 

# convert column types to numeric
cols.num <- c("all_population", "all_households", "white_pop", "asian_pop",
              "black_pop", "otherEth_pop", "econ_inac_rate", "unemployed_16to24_rate",
              "unemployed_loneParent_rate", "no_qualification_rate")
wardDataMerged[cols.num] <- sapply(wardDataMerged[cols.num],as.numeric)


# calculate population percent
wardDataMerged <- wardDataMerged %>% 
  dplyr::mutate(white_pop_rate = white_pop / all_population*100,
         asian_pop_rate = asian_pop / all_population*100,
         black_pop_rate = black_pop / all_population*100,
         otherEth_pop_rate = otherEth_pop / all_population*100) %>% 
  mutate_if(is.numeric, ~round(., 2))
         
```

Count all S&S (section 60) for each ethnic group that fall within wards.

```{r message=FALSE}
ss60_count <- st_join(ss60_london, wardMap) %>% 
  group_by(GSS_CODE, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_ss60 = sum(c_across(Asian:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(ss60_black = "Black",
                ss60_white = "White",
                ss60_asian = "Asian",
                ss60_other = "Other",
                ss60_na = "NA_eth") 
```

Count all violent crimes that fall within wards.

```{r message=FALSE}
vio_count <- st_join(violence_london, wardMap) %>% 
  group_by(GSS_CODE) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(GSS_CODE)) %>% 
  st_drop_geometry() %>% 
  na.omit(.) %>% 
  dplyr::rename(total_violent_crime = "count")
```

Count successful S&S (s60) that leading to an arrest.

```{r message=FALSE, warning=FALSE}
arrest_count <- st_join(ss60_london, wardMap) %>% 
  filter(outcome=="Arrest") %>% 
  group_by(GSS_CODE, officer_defined_ethnicity) %>% 
  summarise(., arrest_count=n(),) %>%
  arrange(desc(GSS_CODE)) %>% 
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = arrest_count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_arrest = sum(c_across(Black:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(arrest_black = "Black",
                arrest_white = "White",
                arrest_asian = "Asian",
                arrest_other = "Other",
                arrest_na = "NA_eth") 

merged <- left_join(wardDataMerged, vio_count, by = c("ward_code" = "GSS_CODE")) %>% 
  left_join(., ss60_count, by = c("ward_code" = "GSS_CODE")) %>% 
  left_join(., arrest_count, by = c("ward_code" = "GSS_CODE"))
  
```

Calculate rates.

```{r message=FALSE}
all_data <- merged %>% 
  clean_names() %>%   
  # violent crime rate 
  mutate(household_vio_rate = total_violent_crime/
           all_households *100) %>% 
  # total S&S rate
  mutate(household_search_rate = total_ss60 / 
           all_households *100) %>% 
  # total arrest rate 
  mutate(arrest_rate = total_arrest/total_ss60 *100) %>% 
  # search rate
  mutate(black_search_rate = ss60_black/ total_ss60 *100) %>% 
  mutate(white_search_rate = ss60_white/ total_ss60 *100) %>% 
  mutate(asian_search_rate = ss60_asian/ total_ss60 *100) %>% 
  mutate(other_search_rate = ss60_other/ total_ss60 *100) %>% 
  mutate(na_search_rate = ss60_na/ total_ss60 *100) %>% 
  # hit rates
  mutate(black_hit_rate = arrest_black/ ss60_black *100) %>% 
  mutate(white_hit_rate = arrest_white/ ss60_white *100) %>% 
  mutate(asian_hit_rate = arrest_asian/ ss60_asian *100) %>% 
  mutate(other_hit_rate = arrest_other/ ss60_other *100) %>% 
  mutate(na_hit_rate = arrest_na/ ss60_na *100) %>% 
  mutate_if(is.numeric, ~round(., 2))

```

```{r}
  # mutate(black_search_rate = ss60_black/
  #          ethnic_group_2011_census_black_african_caribbean_black_british*100) %>% 
  # mutate(white_search_rate = ss60_white/
  #          ethnic_group_2011_census_white*100) %>% 
  # mutate(ss60_asian_rate = ss60_asian/
  #          ethnic_group_2011_census_asian_asian_british*100) %>% 
  # mutate(ss60_other_ethnicity_rate = ss60_other/
  #          ethnic_group_2011_census_other_ethnic_group*100) %>% 
```

### Overall trend

Hit rate is, what proportion of searches, by race, were successful? If racial groups have different hit rates, it can imply that racial groups are being subjected to different standards.

```{r message=False, warning=False, results='asis'}
# count the number of searches by race
HR1 <- ss60_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., searches=n()) %>% 
  st_drop_geometry()
  
# count the number of successful searches leading to arrest 
HR2 <- ss60_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  filter(outcome=="Arrest") %>% 
  summarise(., arrests=n()) %>% 
  st_drop_geometry()

# calculate total population for each race based on 2011 census 
aP <- sum(as.numeric(all_data$asian_pop), na.rm = TRUE)
bP <- sum(as.numeric(all_data$black_pop), na.rm = TRUE)
wP <- sum(as.numeric(all_data$white_pop), na.rm = TRUE)
oP <- sum(as.numeric(all_data$other_eth_pop), na.rm = TRUE)

left_join(HR1, HR2) %>% 
  mutate(population = c(aP, bP, oP, wP, 0),
         search_prop = searches/sum(searches),
         search_rate = searches/population *100,
         hit_rate = arrests/searches *100) %>% 
  dplyr::rename(race = "officer_defined_ethnicity") %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  kable(.,)
```

Create a subset data of all rates for later parts.

```{r}
all_rates <- all_data %>% 
  dplyr::select(ends_with("rate") | ward_code) 
```

## Spatial autocorrelation

Create a neighborhoods list and spatial weight matrix.

```{r message=FALSE, warning=FALSE}
library(spdep)
# calculate centroids of all msoas in London
coordsM<- wardMap%>%
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
Lward_nb <- wardMap %>%
  poly2nb(., queen=T)  
plot(Lward_nb, st_geometry(coordsM), col="red")

# create a spatial weight object
Lward_lw <- Lward_nb %>%
  nb2listw(., style="C")
```

### Global Moran's I

variables to test for spatial autocorrelation:

```{r}
colnames(all_rates)
```

Make a table of all statistical results for all my variables

```{r}
# variables used for the test
variable_names <- c( "household_vio_rate",
                     "household_search_rate",
                     "arrest_rate",
                     "black_search_rate",
                     "white_search_rate",
                     "asian_search_rate",
                     "other_search_rate",
                     "na_search_rate",
                     "black_hit_rate",
                     "white_hit_rate",
                     "asian_hit_rate",
                     "other_hit_rate",
                     "na_hit_rate"
                     )

# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    moran.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist

```

Global Moran's I tells us almost all my variables have some clustering except for arrest rate and ss60 for white people, showing police tend to randomly stop and search white people but intentionally or on purpose to search black and Asian people.

### Geary's C

```{r}
all_rates %>%
  na.omit(all_rates) %>%
  pull(na_hit_rate) %>%
  as.vector()%>%
  geary.test(., Lward_lw, zero.policy = TRUE)

# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    geary.test(., LMsoa_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist
```

### General G

```{r eval=FALSE, include=FALSE}
all_rates %>%
  na.omit(all_rates) %>% 
  pull(black_search_rate) %>%
  as.vector()%>%
  globalG.test(., Lward_lw, zero.policy = TRUE)

all_rates %>%
  pull(black_search_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>% 
  geary.test(., Lward_lw, zero.policy = TRUE)
```

### Local Moran's I

```{r eval=FALSE, include=FALSE}
cal_loc_moran <- all_rates %>%
  pull(black_search_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>% 
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

```

## Agglomerative Hierarchical Clustering

Detailed tutorial found at <https://uc-r.github.io/hc_clustering>

```{r message=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization

df <- subset(all_rates,select=-msoa11cd) %>% 
  st_drop_geometry() %>% 
  scale(.)

```

To determine which clustering method I choose use, agnes function can calculate the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac) 
```

Clearly, Ward method can identify the strongest clustering structures.

```{r}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
hc <- hclust(d, method = "ward.D2")

# Plot the obtained dendrogram
plot(hc, hang=-1, labels=FALSE)
```

To determine the optimal cluster number: 1. Elbow Method 2. Average Silhouette Method (haven't explored yet)

```{r}
# use Elbow Method first  
library(factoextra)
fviz_nbclust(df, FUN = hcut, method = "wss")

# k=5
plot(hc, cex = 0.6, hang=-1, labels=FALSE)
rect.hclust(hc, k = 5, border = 2:5)
```

```{r}
# Cut tree into 5 groups
sub_grp <- cutree(hc, k = 5)

# Number of members in each cluster
table(sub_grp)
```

We can see cluster 4 and 5 has very few obervations so the density of clusters is very different.

### Explore cluster characteristics

[**Q: Do I need to scale df_cluster before taking the median?** ]

```{r message=False, warning=False, results='asis'}
# assign clusters back to rate df
df_cluster <- subset(all_rates) %>% 
  mutate(cluster=sub_grp)

df_cluster%>% 
  st_drop_geometry() %>% 
  group_by(cluster) %>% 
  na.omit(.) %>% 
  summarise(across(economic_activity_2011_census_unemployment_rate:ss60_other_ethnicity_rate, median)) %>% 
  dplyr::rename(unemploymentR = "economic_activity_2011_census_unemployment_rate",
                violeceR = "avg_household_vio_rate",
                total_ss60R = "avg_household_ss60_rate",
                black_ss60R = "ss60_black_rate",
                white_ss60R = "ss60_white_rate",
                asian_ss60R = "ss60_asian_rate",
                other_ss60R = "ss60_other_ethnicity_rate") %>% 
  kable(., caption = 'Cluster Statistics')
```

Make Facet Grid (Looks like this which is made by seaborn in python)

```{r echo = FALSE}
knitr::include_graphics("/Users/yingchen/Documents2/CASA/spatial_capture/final project/Output/sample_in_python.jpg")
```

Plot the cluster map

```{r}
tm_shape(df_cluster) + 
  tm_fill("cluster", palette = "viridis")
```

## GWR
