---
title: "dissertation analysis"
author: "Ying Chen"
date: "19/06/2021"
output: html_document
---

```{r message=FALSE}
library(sf)
library(tidyverse)
library(dplyr)
library(janitor)
library(RColorBrewer)
library(classInt)
library(sp)
library(rgeos)
library(rgdal)
library(spatstat)
library(here)
library(maptools)
library(GISTools)
library(tmap)
library(geojson)
library(geojsonio)
library(tmaptools)
library(hexbin)
library(ggspatial)
library(ggsn)
library(raster)
library(fpc)
library(dbscan)
library(plotrix)
library(spdep)
library(ggplot2)
library(ggpubr)
```

## Understand London Crime in general

-   spatial data: downloaded from London Datastore at <https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london> and stored in the local folder "data".
-   crime data: downloaded from <https://data.police.uk/data/> by selecting only Metropolitan Police Service for data from April 2019 to April 2021, renamed the folder as "crime_data" stored in the local folder "data".

```{r message=FALSE}
boroMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

lsoaMap  <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "LSOA_2011_London_gen_MHW.shp"))%>%
  st_transform(., 27700)

msoaMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "MSOA_2011_London_gen_MHW.shp")) %>% 
  st_transform(., 27700)
```

The following function used to search files that follow a specific pattern is obtained from <https://github.com/sjaraha/clustering-spatiotemporal-data>.

```{r}
list_data_paths <- function(pattern, rec){
  # searches working directory for files that match the specified pattern
  # on match, adds file path to a list
  # returns list the list of matching file paths
  ## pattern (str): regex pattern to match
  ## rec (boolean): recurse into directories (True) or don't (False)
  
  # initialize list
  data_path_list <- c()
  # loop through directories
  for (pd in list.dirs(recursive = rec)){
    # loop through files in directories
    for (f in list.files(pd)){
      # find files that match the pattern
      if (grepl(pattern, f, ignore.case = FALSE)==TRUE){
        # construct path to matching file
        data_path <- paste(pd,f, sep="/")
        # add path to list
        data_path_list <- c(data_path_list,data_path)
      }}}
  # return list of paths to matching files
  return(data_path_list)
}
```

Use the function above and search for all crime data (ended with metropolitan-street.csv) in my local folder.

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
crime_list <- list_data_paths("\\-metropolitan-street.csv$",FALSE) 

crime <- crime_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

Remove duplicated rows, clean names and select points within London.

```{r message = FALSE}
crime<- crime %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

crime_london <- crime[boroMap,]
```

Create a table counting crime numbers for each type.

```{r message=False, warning=False, results='asis'}
library(knitr)
crime_london %>% 
  group_by(crime_type) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London crimes by types from April 2019 to April 2021')
```

I am interested in comparing violent crimes with stop and searches under one particular legislation (section 60) because section 60 is specifically targeted for violent crimes.

```{r}
violence_london <- crime_london %>% 
  filter(., crime_type=='Violence and sexual offences') 
```

What are the most common outcome for violent crimes in London? 

```{r message=False, warning=False, results='asis'}
violence_london %>% 
  group_by(last_outcome_category) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by outcomes from April 2019 to April 2021')
```

I want to count the number of violent crimes by boroughs but they record crimes by LSOA, so I need to extract borough names from LSOA names. (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
violence_london$boro_name = substr(violence_london$lsoa_name,1,nchar(violence_london$lsoa_name)-4)

violence_london %>% 
  group_by(boro_name) %>% 
  summarise(., count=n(),) %>% 
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by boroughs from April 2019 to April 2021')
```

## Understand stop and search (S&S) data

Search for stop and search data (ends with -metropolitan-stop-and-search.csv).

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
ss_list <- list_data_paths("\\-metropolitan-stop-and-search.csv$",FALSE) 

ss <- ss_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

clean names, remove duplicated records and select points within London boroughs.

```{r message=FALSE, warning=False}
ss <- ss %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

ss_london <- ss[boroMap,]
```

What are the common legislation and search object during S&S?

```{r message=False, warning=False, results='asis'}
ss_london %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London S&S by legislation and search objects from April 2019 to April 2021')
```

-   The Criminal Justice and Public Order 1994 (section 60) aims to reduce violence in London. From April 2014, the UK government implements the Best Use of Stop and Search Scheme (BUSSS) and it gives more police power to stop and search people without having reasonable grounds for suspicion if violence is anticipated by police. The main purpose of the scheme is to prevent violent crimes (such as knife crime) before it happens.
-   Therefore, comparing the police usage of section 60 in terms of their locations, ethnicity of the person being S&S, time of a day, season of a year, and actual violent crime locationns within MSOA, LSOA or ward can help understand whether S&S is an effective way of deterring violent crimes at local level.

```{r}
ss60_london <- ss_london %>% 
  filter(legislation == "Criminal Justice and Public Order Act 1994 (section 60)") 
```

What are the most common ethnicity of people being S&S for potential violence? (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
ss60_london %>% 
  group_by(outcome, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  arrange(outcome, officer_defined_ethnicity, desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'Ethnicities for people being S&S (s60) from April 2019 to April 2021')
```


## Simple point distribution

```{r}
tm_shape(msoaMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(violence_london) +
  tm_dots(col = "blue") +
  tm_layout(main.title="Violent Crimes London April 2019 to April 2021",
            main.title.size=1)

tm_shape(msoaMap) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss_london) +
  tm_dots(col = "yellow") +
  tm_layout(main.title="Stop and Search London April 2019 to April 2021", 
            main.title.size=1)
  
tm_shape(msoaMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss60_london) +
  tm_dots(col = "red") +
  tm_layout(main.title= "S&S (section 60) London April 2019 to April 2021", 
            main.title.size=1)
```


## dbscan
DBSCAN requires two parameters: 
1. Epsilon -  the radius within which the algorithm with search for clusters 
2. MinPts - the minimum number of points that should be considered a cluster.

Two ways to determine Epsilon: 
1. Ripley's K - where K values above the Poisson distribution indicates areas of clustering; needs to find the K values' cutoff between above and below the Poisson distribution. 
2. KNNDistance - it is the distance of each point to its k-th nearest neighbor; needs to find a knee in the plot. "The idea behind this heuristic is that points located inside of clusters will have a small k-nearest neighbor distance, because they are close to other points in the same cluster, while noise points are more isolated and will have a rather large kNN distance (Hahsler et al., 2019).

Potential way to determine MinPts:
The rule of thumb for setting minPts is to use at least the number of dimensions of the dataset plus one (Hahsler et al., 2019). However, by setting MinPts = 3 (which is what Hahsler et al suggest) it results in 229 unique cluster for all S&S (s60) points, which is too many. 

### All S&S (s60)
```{r message=FALSE, warning=FALSE}
ss60_londonSP <- ss60_london %>%
  as(., 'Spatial') 

ss60_londonPoints <- ss60_londonSP %>%
  geometry(.)%>%
  as.data.frame()

window <- as.owin(msoaMap)
ss60_londonPPP <- ppp(x=ss60_londonSP@coords[,1],
                     y=ss60_londonSP@coords[,2],
                     window=window)
```

1. Ripley's K method
```{r message=FALSE}
ss60_londonPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 450, col = "blue", lty = 2)
```
From the output graph, we can see that up until distances of around 450 meters, S&S(s60) appear to be clustered in London MSOAs and the largest bulge in the graph at around 400 meters Increasing the distance beyond 450 meters leads the distribution becomes random and dispersed.
Therefore, setting eps to 400m or 450m is reasonable.

```{r warning=False, message=False}
library(factoextra)

db <- ss60_londonPoints %>%
  fpc::dbscan(.,eps = 450, MinPts = 50)  # 450m radius of at least 50 S&S(s60) points

theme_set(theme_minimal())

dbPlot <- fviz_cluster(db, ss60_londonPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls All S&S (s60) \n (ep=450, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot
```
It gives us 33 unique clusters. 
The graph tells us this high intensity (450 meters radius --> 50 S&S for violence) is not widespread the country, only shown in few neighborhoods at Northen London.


2. KNNDistance method:

```{r message=FALSE, warning=FALSE}
ss60_londonPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=1600)") %>% 
  abline(h = 1600, col = "red", lty = 2)
```

```{r}
db_KNN <- ss60_londonPoints %>%
  fpc::dbscan(.,eps = 1600, MinPts = 50)

theme_set(theme_minimal())

dbPlot_KNN <- fviz_cluster(db_KNN, ss60_londonPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls All S&S (s60) \n (ep=1600, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_KNN
```
It gives us 5 unique clusters.
This graph tells us if we increase the neighborhood radius range to 1.6 km <more explaination needed>  



### Towards Black people

Observations: 
1. if setting the eps as Ripley's K test suggests, the number is small and resulting in some small clusters with very few points within. 
2. if setting as kNNdistplot suggests, the "knee" is usually quite large and it will give us a large cluster + some small clusters + isolated noises. 

```{r message=FALSE, warning=FALSE}
ss60_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") 

ss60_blackSP <- ss60_black %>%  
  as(., 'Spatial') 

ss60_blackPPP <- ppp(x=ss60_blackSP@coords[,1],
                     y=ss60_blackSP@coords[,2],
                     window=window)

ss60_blackPoints<- ss60_blackSP %>%
  geometry(.)%>%
  as.data.frame()
```

1. Choose Ripley's K method to select suitable parameters:

```{r message=FALSE, warning=FALSE}
ss60_blackPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 450, col = "blue", lty = 2)
```

Plot:
```{r}
db_b <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 450, MinPts = 50)

theme_set(theme_minimal())

dbPlot_b <- fviz_cluster(db_b, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=450, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_b
```
It gives us 14 unique clusters. The distribuion is very like the previous counting all S&S. 


2. Choose KNN distance method to select suitable parameters:

```{r}
ss60_blackPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```

plot:
```{r}
db_b_KNN <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 50)

theme_set(theme_minimal())

dbPlot_b_KNN <- fviz_cluster(db_b_KNN, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=2000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_b_KNN
```
It gives us 2 unique clusters.

### Towars White People

```{r message=FALSE, warning=FALSE}
ss60_white<- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") 

ss60_whiteSP <- ss60_white %>%  
  as(., 'Spatial') 

ss60_whitePPP <- ppp(x=ss60_whiteSP@coords[,1],
                     y=ss60_whiteSP@coords[,2],
                     window=window)

ss60_whitePoints<- ss60_whiteSP %>%
  geometry(.)%>%
  as.data.frame()
```

1. Ripley's K method:

```{r message=FALSE, warning=FALSE}
ss60_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 750, col = "blue", lty = 2)
```

```{r}
db_w <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 750, MinPts = 50)

theme_set(theme_minimal())

dbPlot_w <- fviz_cluster(db_w, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=750, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_w
```

2. KNNDistance method:

```{r}
ss60_whitePoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=3000)") %>% 
  abline(h = 3000, col = "red", lty = 2)
```

```{r}
db_w_KNN <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 3000, MinPts = 50)

theme_set(theme_minimal())

dbPlot_w_KNN <- fviz_cluster(db_w_KNN, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=3000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_w_KNN
```


### Towards Asian people

```{r message=FALSE, warning=FALSE}
ss60_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") 

ss60_asianSP <- ss60_asian %>% 
  as(., 'Spatial') 

ss60_asianPPP <- ppp(x=ss60_asianSP@coords[,1],
                     y=ss60_asianSP@coords[,2],
                     window=window)

ss60_asianPoints<- ss60_asianSP %>%
  geometry(.)%>%
  as.data.frame()
```

1. Ripley's K:

```{r message=FALSE, warning=FALSE}
ss60_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 500, col = "blue", lty = 2)
```

```{r}
db_a <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 500, MinPts = 50)

theme_set(theme_minimal())

dbPlot_a <- fviz_cluster(db_a, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=500, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_a
```


2. KNNDistance

```{r}
ss60_asianPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=4000)") %>% 
  abline(h = 4000, col = "red", lty = 2)
```

```{r}
db_a_KNN <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 4000, MinPts = 50)

theme_set(theme_minimal())

dbPlot_a_KNN <- fviz_cluster(db_a_KNN, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=4000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_a_KNN
```

#### Plot cluster results on a openstreet basemap

```{r message=FALSE}
library(OpenStreetMap)

# before plotting, we need to transform geometry to (x,y) coordinates and the function was found at <https://maczokni.github.io/crimemapping_textbook_bookdown/more-on-thematic-maps.html>
sfc_as_cols <- function(x, names = c("x","y")) {
  stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
  ret <- sf::st_coordinates(x)
  ret <- tibble::as_tibble(ret)
  stopifnot(length(names) == ncol(ret))
  x <- x[ , !names(x) %in% names]
  ret <- setNames(ret,names)
  dplyr::bind_cols(x,ret)
}

# add (x,y) coordinate and clusters to new columns  
ss60_asian <- sfc_as_cols(ss60_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_a$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_a <- ss60_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# create a bounding box of London
LondonBB <- boroMap %>%
  st_transform(., 4326)%>%
  st_bbox()

# set the basemap showing London 
basemap <- OpenStreetMap::openmap(c(51.2867602,-0.5103751),c(51.6918741,0.3340156), 
                                  zoom=NULL,
                                  "stamen-toner")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

# plot
autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_asian, 
             aes(longitude,latitude, 
                 colour=dbcluster,
                 fill=dbcluster),size=0.3)+
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "red") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
```

How about for the black people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_black <- sfc_as_cols(ss60_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_b$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_b <- ss60_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_black, 
             aes(longitude,latitude, 
                 colour=dbcluster,
                 fill=dbcluster),size=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "red") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
```

For white people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_white <- sfc_as_cols(ss60_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_w$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_w <- ss60_white %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_white, 
             aes(longitude,latitude, 
                 colour=dbcluster,
                 fill=dbcluster),size=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "red") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to White People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
```

## st-dbscan 
The 'stdbscanr' package and related functions was obtained from Dr.Gordon McDonald's github at 
<https://github.com/gdmcdonald/stdbscanr>

```{r message=FALSE}
# install.packages("devtools")
# devtools::install_github("gdmcdonald/stdbscanr")
# install.packages("data.table")          

library("data.table")
library(stdbscanr)

ss60_londonTEST <- ss60_london[sample(nrow(ss60_london), 100), ] %>%  # random select 100 rows
  st_transform(., 4326) %>% 
  sfc_as_cols(., c("longitude", "latitude")) %>%    # convert geometry to coordinates
  setDT(.) %>%     # convert to data.table
  setkey(., date)  # sort by data 

# add time intervals in minues between points
ss60_londonTEST[,time_inc := as.numeric(date - shift(date), units = "mins")]

# run st-dbscan
location_with_visits <- 
  get_clusters_from_data(df = ss60_londonTEST,
                         x = "longitude", 
                         y = "latitude", 
                         t = "date",
                         eps = 0.005,  # 0.005 latitude/longitude ~ 500m either way in London
                         eps_t = 1440, # 1440 minutes = 1day
                         minpts = 2)
```

```{r results='asis'}
#Define a mode function to get the most common label
mode <- function(x) { names(which.max(table(x))) }

#data.table summary table of visits
clusters <-                                   
  location_with_visits[     
    !is.na(cluster),                         
    .(n = .N,                               
      latitude = mean(latitude),            
      longitude = mean(longitude), 
      time_spent = sum(time_inc,na.rm = T),
      ethnicity_label = mode(officer_defined_ethnicity)            
    ),    
    by=cluster] 

kable(clusters, caption = "ST-DBSCAN Cluster Labels")
```

```{r message=FALSE}
# order by time
setkey(location_with_visits, date)

library(leaflet)
# plot on leaflet map
leaflet(data = clusters) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addMarkers(popup = ~paste0("Time spent: ",round(time_spent/60, 1), " hours.<br>",
                             "S&S Ethnicity Cluster ",cluster,": ",ethnicity_label,"<br>",
                             "Counts: ", n)) %>% 
  addPolylines(data = location_with_visits, 
               lat = ~latitude, 
               lng = ~longitude)
```

## Make continuous observations

We have explored the point pattern analysis for London S&S and how about looking at MSOAs, wards or boroughs as a whole? Where are the hotspots and how to explain them? To do that, we need to count the number of crimes and S&S for each geographic unit. Here, I chose to group by MSOA (averaged with 8,346 population in 2010) and there are 983 MSOAs in London.

The demographic data was obtained from <https://data.london.gov.uk/dataset/msoa-atlas>.

```{r message=FALSE, warning=FALSE}
msoaData <- read_csv("https://data.london.gov.uk/download/msoa-atlas/20264159-36cb-4aa2-8371-ae884ae83e88/msoa-data.csv",na = c("NA", "n/a")) %>% 
  clean_names() 
```

Notice msoaData has 984 rows while the msoaMap has 983 rows. After inspection, the msoaData has an extra row showing the average statistics for all London MSOA, which needs to be removed.

```{r message=FALSE}
msoaData <- msoaData[1:983,]
```

Columns we need:

> -   "age_structure_2011_census_all_ages",
> -   "households_2011_all_households",
>
> *the above can calculate violent crime rate and overall S&S rate by dividing by the number of crimes/S&S in each geographic unit*
>
> -   "ethnic_group_2011_census_white",
> -   "ethnic_group_2011_census_mixed_multiple_ethnic_groups"，
> -   "ethnic_group_2011_census_asian_asian_british",
> -   "ethnic_group_2011_census_black_african_caribbean_black_british",
> -   "ethnic_group_2011_census_other_ethnic_group",
> -   "ethnic_group_2011_census_bame", ------------ ***what does "bame"mean???***
>
> *the above can calculate specific S&S rate for each ethnic group because the S&S data also has ethnic data*
>
> -   "economic_activity_2011_census_unemployment_rate",
> -   "economic_activity_2011_census_economically_inactive_percent",
>
> *the above might be useful if to calculate spatial autocorrelation between S&S rate and unemployment/economically inactive rate*

```{r message=FALSE}
cols <- c(
"middle_super_output_area",
"msoa_name",
"age_structure_2011_census_all_ages",
"households_2011_all_households",
"ethnic_group_2011_census_white",                                               
"ethnic_group_2011_census_mixed_multiple_ethnic_groups",                        
"ethnic_group_2011_census_asian_asian_british",
"ethnic_group_2011_census_black_african_caribbean_black_british",
"ethnic_group_2011_census_other_ethnic_group",
"ethnic_group_2011_census_bame",
"economic_activity_2011_census_unemployment_rate",                                
"economic_activity_2011_census_economically_inactive_percent")

msoaDataMerged <-
  left_join(msoaMap %>% dplyr::select(MSOA11CD,MSOA11NM,geometry),
            msoaData %>% dplyr::select(all_of(cols)),
            by = c("MSOA11CD" = "middle_super_output_area"))
```

Count all S&S (section 60) for each ethnic group that fall within MSOAs.

```{r message=FALSE}
ss60_joined <- st_join(ss60_london, msoaMap) %>% 
  group_by(MSOA11CD, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry()

ss60_ethnic <- ss60_joined %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = count)


ss60_ethnicAll <- ss60_joined %>% 
  group_by(MSOA11CD) %>% 
  summarise(., sum(count),) %>%
  arrange(desc(MSOA11CD)) 

ss60_ethicFinal <- left_join(ss60_ethnic, ss60_ethnicAll) %>% 
  dplyr::rename(ss60_black = "Black",
                ss60_white = "White",
                ss60_asian = "Asian",
                ss60_other = "Other",
                ss60_na = "NA",
                total_ss60 = "sum(count)")

```

Count all S&S (section 60) for each outcome types that fall within MSOAs.

```{r message=FALSE}
ss60_joined2 <- st_join(ss60_london, msoaMap) %>% 
  group_by(MSOA11CD, outcome) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry()

ss60_outcome <- ss60_joined2 %>% 
  pivot_wider(names_from = outcome, values_from = count)

ss60_outcomeAll <- ss60_joined2 %>% 
  group_by(MSOA11CD) %>% 
  summarise(., sum(count),) %>%
  arrange(desc(MSOA11CD)) 

ss60_outcomeFinal <- left_join(ss60_outcome,ss60_outcomeAll) %>% 
  dplyr::rename(total_ss60_outcome = "sum(count)")
```

Count all violent crimes that fall within MSOAs.

```{r message=FALSE}
vio_count <- st_join(violence_london, lsoaMap) %>% 
  group_by(MSOA11CD) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry() %>% 
  na.omit(.) %>% 
  dplyr::rename(total_violent_crime = "count")

merged <- left_join(msoaDataMerged, vio_count) %>% 
  left_join(., ss60_ethicFinal) %>% 
  left_join(., ss60_outcomeFinal)
```

Check if the sum of S&S by ethical group is equal to the sum of outcome

```{r}
merged$total_ss60[!(merged$total_ss60 %in% merged$total_ss60_outcome)]
```

"ineger(0)" means they are equal so we can remove one and then we can calculate the rates.

```{r message=FALSE}
all_data <- subset(merged, select = -c(total_ss60_outcome,msoa_name)) %>% 
  clean_names() %>%   
  mutate(avg_household_vio_rate = total_violent_crime/
           households_2011_all_households*100) %>% 
  mutate(avg_household_ss60_rate = total_ss60 / 
           households_2011_all_households*100) %>% 
  mutate(ss60_black_rate = ss60_black/
           ethnic_group_2011_census_black_african_caribbean_black_british*100) %>% 
  mutate(ss60_white_rate = ss60_white/
           ethnic_group_2011_census_white*100) %>% 
  mutate(ss60_asian_rate = ss60_asian/
           ethnic_group_2011_census_asian_asian_british*100) %>% 
  mutate(ss60_other_ethnicity_rate = ss60_other/
           ethnic_group_2011_census_other_ethnic_group*100) %>% 
  mutate(ss60_arrest_rate = arrest/total_ss60*100) %>% 
  mutate_if(is.numeric, ~round(., 2))

```

Create a subset data of all rates for later parts.

```{r}
all_rates <- all_data %>% 
  dplyr::select(ends_with("rate") | msoa11cd) 
```

## Spatial autocorrelation

Create a neighborhoods list and spatial weight matrix.

[**Q: when creating a neighborhoods list using poly2nb, before setting 'snap' gives me error and after setting it to 0.01 it runs successfully, but how to determine if 0.01 is a reasonable value since the out map cannot really tell anything.**]{.ul}

```{r message=FALSE, warning=FALSE}
library(spdep)
# calculate centroids of all msoas in London
coordsM<- all_rates%>%
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
LMsoa_nb <- all_rates %>%
  poly2nb(., queen=T, snap=0.01)  
plot(LMsoa_nb, st_geometry(coordsM), col="red")

# create a spatial weight object
LMsoa_lw <- LMsoa_nb %>%
  nb2listw(., style="C")
```

### Global Moran's I

[**Q: Should I change all NAs in all_rates to 0? Some of the MSOAs have S&S but with a very low rate like 0.002%. Turning NAs to 0 will give places with no S&S the same color/interpretation as places with low rate, whereas keeping NAs will just show gray on the map. However, keeping NAs seems to create troubles in later statistical tests. For now, I chose to ignore NAs.**]{.ul}

```{r}
all_rates %>%
  pull(avg_household_ss60_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>%   # ignore the NA
  moran.test(., LMsoa_lw, zero.policy = TRUE)
```

Make a table of all statical results for all my variables 
[**Q: How to turn the results into a dataframe with common column names (i.e. Moran I statistic, Expectation, Variance) and each variable's results fill in new rows? **]{.ul}

```{r}
# variables used for the test
variable_names <- c( "avg_household_vio_rate", "avg_household_ss60_rate",
        "ss60_black_rate","ss60_white_rate",
        "ss60_asian_rate","ss60_arrest_rate")

# function to compute coefficient
cal_glb_moran <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    moran.test(., LMsoa_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_glb_moran(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist

```

Global Moran's I tells us almost all my variables have some clustering except for arrest rate and ss60 for white people, showing police tend to randomly stop and search white people but intentionly or on purpose to search black and Asian people. 

### Geary's C

```{r}
all_rates %>%
  na.omit(all_rates) %>%
  pull(avg_household_ss60_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>% 
  geary.test(., LMsoa_lw, zero.policy = TRUE)
```


### General G

```{r eval=FALSE, include=FALSE}
all_rates %>%
  pull(ss60_arrest_rate) %>%
  as.vector()%>%
  globalG.test(., LMsoa_lw, zero.policy = TRUE)

all_rates %>%
  pull(avg_household_vio_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>% 
  geary.test(., LMsoa_lw, zero.policy = TRUE)
```

### Local Moran's I

```{r eval=FALSE, include=FALSE}
cal_loc_moran <- all_rates %>%
  pull(avg_household_ss60_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>% 
  localmoran(., LMsoa_lw, zero.policy = TRUE)%>%
  as_tibble()

```

## Agglomerative Hierarchical Clustering

Detailed tutorial found at <https://uc-r.github.io/hc_clustering>

```{r message=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization

df <- subset(all_rates,select=-msoa11cd) %>% 
  st_drop_geometry() %>% 
  scale(.)

```

To determine which clustering method I choose use, agnes function can calculate the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).
```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac) 
```

Clearly, Ward method can identify the strongest clustering structures.

```{r}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
hc <- hclust(d, method = "ward.D2")

# Plot the obtained dendrogram
plot(hc, hang=-1, labels=FALSE)
```

To determine the optimal cluster number:
1. Elbow Method
2. Average Silhouette Method (haven't explored yet)

```{r}
# use Elbow Method first  
library(factoextra)
fviz_nbclust(df, FUN = hcut, method = "wss")

# k=5
plot(hc, cex = 0.6, hang=-1, labels=FALSE)
rect.hclust(hc, k = 5, border = 2:5)
```

```{r}
# Cut tree into 5 groups
sub_grp <- cutree(hc, k = 5)

# Number of members in each cluster
table(sub_grp)
```

We can see cluster 4 and 5 has very few obervations so the density of clusters is very different.

### Explore cluster characteristics

[**Q: Do I need to scale df_cluster before taking the median? **]

```{r message=False, warning=False, results='asis'}
# assign clusters back to rate df
df_cluster <- subset(all_rates) %>% 
  mutate(cluster=sub_grp)

df_cluster%>% 
  st_drop_geometry() %>% 
  group_by(cluster) %>% 
  na.omit(.) %>% 
  summarise(across(economic_activity_2011_census_unemployment_rate:ss60_other_ethnicity_rate, median)) %>% 
  dplyr::rename(unemploymentR = "economic_activity_2011_census_unemployment_rate",
                violeceR = "avg_household_vio_rate",
                total_ss60R = "avg_household_ss60_rate",
                black_ss60R = "ss60_black_rate",
                white_ss60R = "ss60_white_rate",
                asian_ss60R = "ss60_asian_rate",
                other_ss60R = "ss60_other_ethnicity_rate") %>% 
  kable(., caption = 'Cluster Statistics')
```
Make Facet Grid (Looks like this which is made by seaborn in python)
```{r echo = FALSE}
knitr::include_graphics("/Users/yingchen/Documents2/CASA/spatial_capture/final project/Output/sample_in_python.jpg")
```

Plot the cluster map 

```{r}
tm_shape(df_cluster) + 
  tm_fill("cluster", palette = "viridis")
```


