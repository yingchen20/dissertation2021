---
title: "dissertation analysis"
author: "Ying Chen"
date: "19/06/2021"
output: html_document
---

```{r message=FALSE}
library(sf)
library(tidyverse)
library(dplyr)
library(janitor)
library(RColorBrewer)
library(classInt)
library(sp)
library(rgeos)
library(rgdal)
library(spatstat)
library(here)
library(maptools)
library(GISTools)
library(tmap)
library(geojson)
library(geojsonio)
library(tmaptools)
library(hexbin)
library(ggspatial)
library(ggsn)
library(raster)
library(fpc)
library(dbscan)
library(plotrix)
library(spdep)
library(ggplot2)
library(ggpubr)
library(factoextra) 
library(knitr)
```

## Understand London Crime in general

-   spatial data: downloaded from London Datastore at <https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london> and stored in the local folder "data".
-   crime data: downloaded from <https://data.police.uk/data/> by selecting only Metropolitan Police Service for data from April 2019 to April 2021, renamed the folder as "crime_data" stored in the local folder "data".

```{r message=FALSE}
boroMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

wardMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Ward_CityMerged.shp")) %>% 
  st_transform(., 27700)

msoaMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "MSOA_2011_London_gen_MHW.shp")) %>% 
  st_transform(., 27700)
```

The following function used to search files that follow a specific pattern is obtained from <https://github.com/sjaraha/clustering-spatiotemporal-data>.

```{r}
list_data_paths <- function(pattern, rec){
  # searches working directory for files that match the specified pattern
  # on match, adds file path to a list
  # returns list the list of matching file paths
  ## pattern (str): regex pattern to match
  ## rec (boolean): recurse into directories (True) or don't (False)
  
  # initialize list
  data_path_list <- c()
  # loop through directories
  for (pd in list.dirs(recursive = rec)){
    # loop through files in directories
    for (f in list.files(pd)){
      # find files that match the pattern
      if (grepl(pattern, f, ignore.case = FALSE)==TRUE){
        # construct path to matching file
        data_path <- paste(pd,f, sep="/")
        # add path to list
        data_path_list <- c(data_path_list,data_path)
      }}}
  # return list of paths to matching files
  return(data_path_list)
}
```

Use the function above and search for all crime data (ended with metropolitan-street.csv) in my local folder.

```{r message = FALSE, warning=FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
crime_list <- list_data_paths("\\-metropolitan-street.csv$",FALSE) 

crime <- crime_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

Remove duplicated rows, clean names and select points within London.

```{r message = FALSE}
crime<- crime %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

crime_london <- crime[boroMap,]
```

Create a table counting total crime numbers for each crime type.

```{r message=False, warning=False}
library(knitr)
crime_london %>% 
  group_by(crime_type) %>% 
  summarise(., count=n(),) %>%
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(.)
```

I want to count the number of crimes by boroughs but they record crimes by LSOA, so I need to extract borough names from LSOA names. (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
crime_london$boro_name = substr(crime_london$lsoa_name,1,nchar(crime_london$lsoa_name)-5)

crime_london %>% 
  group_by(boro_name) %>% 
  summarise(., count=n(),) %>% 
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(prop)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London crimes by boroughs from April 2019 to April 2021')
```

## Understand stop and search (S&S) data

Search for stop and search data (ends with -metropolitan-stop-and-search.csv).

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
ss_list <- list_data_paths("\\-metropolitan-stop-and-search.csv$",FALSE) 

ss <- ss_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

clean names, remove duplicated records and select points within London boroughs.

```{r message=FALSE, warning=False}
ss <- ss %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

ss_london <- ss[wardMap,]
```

Check how police distributes their power on stop and search -- Searches for what?

```{r message=False, warning=False}
ssTab <- ss_london %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., searches=n(),) %>%
  st_drop_geometry()

arrestTab <- ss_london %>% 
  filter(outcome=="Arrest") %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., arrests=n(),) %>%
  st_drop_geometry()

left_join(ssTab, arrestTab, by = c("legislation", "object_of_search")) %>% 
  as.data.frame() %>% 
  mutate(search_rate = searches/sum(searches) *100,
         hit_rate = arrests/searches *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(hit_rate)) %>% 
  kable()
```

```{r results='asis'}
ssTab <- ss_london %>% 
  group_by(legislation) %>% 
  summarise(., searches=n(),) %>%
  st_drop_geometry()

arrestTab <- ss_london %>% 
  filter(outcome=="Arrest") %>% 
  group_by(legislation) %>% 
  summarise(., arrests=n(),) %>%
  st_drop_geometry()

left_join(ssTab, arrestTab, by = "legislation") %>% 
  as.data.frame() %>% 
  mutate(search_prop = searches/sum(searches) *100,
         hit_rate = arrests/searches *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(hit_rate)) %>% 
  kable()
```

Who are searched and arrested?

```{r warning=FALSE, message=FALSE}
arrestRaceTab <- ss_london %>%  
  filter(outcome == "Arrest") %>% 
  group_by(legislation, officer_defined_ethnicity) %>% 
  summarise(., arrests=n()) %>%
  st_drop_geometry()

ssRaceTab <- ss_london %>%  
  group_by(legislation, officer_defined_ethnicity) %>% 
  summarise(., searches=n()) %>%
  st_drop_geometry()

left_join(ssRaceTab, arrestRaceTab, by=c("legislation", "officer_defined_ethnicity")) %>% 
  as.data.frame() %>% 
  mutate(search_rate = searches/sum(searches) *100,
         hit_rate = arrests/searches *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(legislation)) %>% 
  kable()
```

### Map: Trend

```{r}
ss_london %>%
  count(year = year(date), officer_defined_ethnicity) %>%
  ggplot(aes(x = year, y = n, color = officer_defined_ethnicity)) +
  geom_point() +
  geom_line()
```

## dbscan

DBSCAN requires two parameters: 1. Epsilon - the radius within which the algorithm with search for clusters 2. MinPts - the minimum number of points that should be considered a cluster.

Two ways to determine Epsilon: 1. Ripley's K - where K values above the Poisson distribution indicates areas of clustering; needs to find the K values' cutoff between above and below the Poisson distribution. 2. KNNDistance - it is the distance of each point to its k-th nearest neighbor; needs to find a knee in the plot. "The idea behind this heuristic is that points located inside of clusters will have a small k-nearest neighbor distance, because they are close to other points in the same cluster, while noise points are more isolated and will have a rather large kNN distance (Hahsler et al., 2019).

Potential way to determine MinPts: The rule of thumb for setting minPts is to use at least the number of dimensions of the dataset plus one (Hahsler et al., 2019). However, by setting MinPts = 3 (which is what Hahsler et al suggest) it results in 229 unique cluster for all S&S (s60) points, which is too many.

### Black

Observations: 1. if setting the eps as Ripley's K test suggests, the number is small and resulting in some small clusters with very few points within. 2. if setting as kNNdistplot suggests, the "knee" is usually quite large and it will give us a large cluster + some small clusters + isolated noises.

```{r message=FALSE, warning=FALSE}
window <- as.owin(msoaMap)

ss_black <- ss_london %>% 
  filter(officer_defined_ethnicity == "Black") 

ss_blackSP <- ss_black %>%  
  as(., 'Spatial') 
 
ss_blackPPP <- ppp(x=ss_blackSP@coords[,1],
                     y=ss_blackSP@coords[,2],
                     window=window)

ss_blackPoints<- ss_blackSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Choose Ripley's K method to select suitable parameters:

```{r message=FALSE, warning=FALSE}
ss_blackPPP %>%
  Kest(., correction="border") %>%
  plot()
  #abline(v = 450, col = "blue", lty = 2)
```

Plot:

```{r}
library(factoextra)

db_b <- ss_blackPoints %>%
  fpc::dbscan(.,eps = 1000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_b, ss_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=1000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  Choose KNN distance method to select suitable parameters:

```{r}
ss_blackPoints %>%
  dbscan::kNNdistplot(.,k=30) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=10, knee=1000)") %>% 
  abline(h = 1000, col = "red", lty = 2)
```

plot:

```{r message=FALSE, warning=FALSE}
db_b_KNN <- ss_blackPoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 30)

theme_set(theme_minimal())

fviz_cluster(db_b_KNN, ss_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=1000, MinPts=10)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

### White

```{r message=FALSE, warning=FALSE}
ss_white<- ss_london %>% 
  filter(officer_defined_ethnicity == "White") 

ss_whiteSP <- ss_white %>%  
  as(., 'Spatial') 

ss_whitePPP <- ppp(x=ss_whiteSP@coords[,1],
                     y=ss_whiteSP@coords[,2],
                     window=window)

ss_whitePoints<- ss_whiteSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K method:

```{r message=FALSE, warning=FALSE}
ss_whitePPP %>%
  Kest(., correction="border") %>% 
  plot()
  #abline(v = 750, col = "blue", lty = 2)
```

```{r}
db_w <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 750, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_w, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=750, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  KNNDistance method:

```{r}
ss60_whitePoints %>%
  dbscan::kNNdistplot(.,k=30) %>% 
  title(main="10-nearst Neighbor Distance Plot \n (MinPts=10, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```

```{r}
db_w_KNN <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 30)

theme_set(theme_minimal())

fviz_cluster(db_w_KNN, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=1500, MinPts=10)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

### Asian

```{r message=FALSE, warning=FALSE}
ss60_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") 

ss60_asianSP <- ss60_asian %>% 
  as(., 'Spatial') 

ss60_asianPPP <- ppp(x=ss60_asianSP@coords[,1],
                     y=ss60_asianSP@coords[,2],
                     window=window)

ss60_asianPoints<- ss60_asianSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K:

```{r message=FALSE, warning=FALSE}
ss60_asianPPP %>%
  Kest(., correction="border") %>%
  plot() 
  #abline(v = 500, col = "blue", lty = 2)
```

```{r}
db_a <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 500, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_a, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=500, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  KNNDistance

```{r}
ss60_asianPoints %>%
  dbscan::kNNdistplot(.,k=30) %>% 
  title(main="10-nearst Neighbor Distance Plot \n (MinPts=10, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```

```{r}
db_a_KNN <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 30)

theme_set(theme_minimal())

fviz_cluster(db_a_KNN, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=2000, MinPts=10)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()
```

### With an openstreet basemap

```{r message=FALSE}
library(OpenStreetMap)

# before plotting, we need to transform geometry to (x,y) coordinates and the function was found at <https://maczokni.github.io/crimemapping_textbook_bookdown/more-on-thematic-maps.html>
sfc_as_cols <- function(x, names = c("x","y")) {
  stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
  ret <- sf::st_coordinates(x)
  ret <- tibble::as_tibble(ret)
  stopifnot(length(names) == ncol(ret))
  x <- x[ , !names(x) %in% names]
  ret <- setNames(ret,names)
  dplyr::bind_cols(x,ret)
}

# add (x,y) coordinate and clusters to new columns  
ss60_asian <- sfc_as_cols(ss60_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_a$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_a <- ss60_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# create a bounding box of London
LondonBB <- boroMap %>%
  st_transform(., 4326)%>%
  st_bbox()

# set the basemap showing London 
basemap <- OpenStreetMap::openmap(c(51.2867602,-0.5103751),c(51.6918741,0.3340156), 
                                  zoom=NULL,
                                  "stamen-toner")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

# plot
dbPlotAsian <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsian
```

How about for the black people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_black <- sfc_as_cols(ss60_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_b_KNN$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_b <- ss60_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotBlack <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=hulls_b, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 0.5,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlack
```

For white people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_white <- sfc_as_cols(ss60_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_w_KNN$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_w <- ss60_white %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotWhite <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_white, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 0.5,
               fill = "seagreen") +
  theme(legend.position = "right") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to White People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotWhite
```

### Overlapping map

```{r}
dbplotALL <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 0.5,
               fill="blueviolet") +
  geom_point(data=ss60_black, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 0.5,
               fill = "tomato") +
  geom_point(data=ss60_white, 
             aes(longitude,latitude), 
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 0.5,
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("Total ss60 towards Black people (red) +  White people (green) + \nAsian people (purple) from April 2019 to Aril 2021 ")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALL
```

## dbscan - only arrest

### Asian

```{r message=FALSE,warning=FALSE}
ss60Arr_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_asianSP <- ss60Arr_asian %>% 
  as(., 'Spatial') 

ss60Arr_asianPPP <- ppp(x=ss60Arr_asianSP@coords[,1],
                     y=ss60Arr_asianSP@coords[,2],
                     window=window)

ss60Arr_asianPoints<- ss60Arr_asianSP %>%
  geometry(.)%>%
  as.data.frame()

ss60Arr_asianPPP %>%
  Kest(., correction="border") %>%
  plot() %>% 
  abline(v = 285, col = "blue", lty = 2)
```

```{r}
ss60Arr_asianPoints %>%
  dbscan::kNNdistplot(.,k=4) %>% 
  title(main="10-nearst Neighbor Distance Plot \n (MinPts=10, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```

```{r message=FALSE, warning=FALSE}
# add (x,y) coordinate and clusters to new columns  
db_a_KNN_arr <- ss60Arr_asianPoints %>%
  fpc::dbscan(.,eps = 1000, MinPts = 3)

ss60Arr_asian <- sfc_as_cols(ss60Arr_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_a_KNN_arr$cluster) 

# create convex hull polygons to wrap around the points in our clusters
hulls_aArr <- ss60Arr_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotAsianArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=1)+
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsianArr
```

### Black

```{r message=False, warning=False}
ss60Arr_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_blackSP <- ss60Arr_black %>% 
  as(., 'Spatial') 

ss60Arr_blackPoints<- ss60Arr_blackSP %>%
  geometry(.)%>%
  as.data.frame()

ss60Arr_blackPPP <- ppp(x=ss60Arr_blackSP@coords[,1],
                     y=ss60Arr_blackSP@coords[,2],
                     window=window)

ss60Arr_blackPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1000)) %>% 
  abline(v = 400, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}

# add (x,y) coordinate and clusters to new columns  
db_b_KNN_arr <- ss60Arr_blackPoints %>%
  fpc::dbscan(.,eps = 1000, MinPts = 3)

ss60Arr_black <- sfc_as_cols(ss60Arr_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_b_KNN_arr$cluster) 

# create convex hull polygons to wrap around the points in our clusters
hulls_bArr <- ss60Arr_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotBlackArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=1)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlackArr
```

### White

```{r message=False, warning=False}
ss60Arr_white <- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_whiteSP <- ss60Arr_white %>% 
  as(., 'Spatial') 

ss60Arr_whitePoints<- ss60Arr_whiteSP %>%
  geometry(.)%>%
  as.data.frame()

ss60Arr_whitePPP <- ppp(x=ss60Arr_whiteSP@coords[,1],
                     y=ss60Arr_whiteSP@coords[,2],
                     window=window)

ss60Arr_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,600)) %>% 
  abline(v = 330, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}
# add (x,y) coordinate and clusters to new columns  
db_w_KNN_arr <- ss60Arr_whitePoints %>%
  fpc::dbscan(.,eps = 1000, MinPts = 3)

ss60Arr_white <- sfc_as_cols(ss60Arr_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_w_KNN_arr$cluster) 

# create convex hull polygons to wrap around the points in our clusters
hulls_wArr <- ss60Arr_white %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotwhiteArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=1)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to White People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotwhiteArr
```

### Overlapping map

```{r}
dbplotALLArr <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.1) +
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.1)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude), 
             size=0.1)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("Arrests for ss60 towards Black people (red) + White people (green) + \n Asian people (purple) from April 2019 to Aril 2021")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALLArr
```

## st-dbscan

The 'stdbscanr' package and related functions was obtained from Dr.Gordon McDonald's github at <https://github.com/gdmcdonald/stdbscanr>

```{r message=FALSE}
# install.packages("devtools")
# devtools::install_github("gdmcdonald/stdbscanr")
# install.packages("data.table")          

library("data.table")
library(stdbscanr)

ss60_londonTEST <- ss60_london[sample(nrow(ss60_london), 100), ] %>%  # random select 100 rows
  st_transform(., 4326) %>% 
  sfc_as_cols(., c("longitude", "latitude")) %>%    # convert geometry to coordinates
  setDT(.) %>%     # convert to data.table
  setkey(., date)  # sort by data 

# add time intervals in minues between points
ss60_londonTEST[,time_inc := as.numeric(date - shift(date), units = "mins")]

# run st-dbscan
location_with_visits <- 
  get_clusters_from_data(df = ss60_londonTEST,
                         x = "longitude", 
                         y = "latitude", 
                         t = "date",
                         eps = 0.005,  # 0.005 latitude/longitude ~ 500m either way in London
                         eps_t = 1440, # 1440 minutes = 1day
                         minpts = 2)
```

```{r results='asis'}
#Define a mode function to get the most common label
mode <- function(x) { names(which.max(table(x))) }

#data.table summary table of visits
clusters <-                                   
  location_with_visits[     
    !is.na(cluster),                         
    .(n = .N,                               
      latitude = mean(latitude),            
      longitude = mean(longitude), 
      time_spent = sum(time_inc,na.rm = T),
      ethnicity_label = mode(officer_defined_ethnicity)            
    ),    
    by=cluster] 

kable(clusters, caption = "ST-DBSCAN Cluster Labels")
```

```{r message=FALSE}
# order by time
setkey(location_with_visits, date)

library(leaflet)
# plot on leaflet map
leaflet(data = clusters) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addMarkers(popup = ~paste0("Time spent: ",round(time_spent/60, 1), " hours.<br>",
                             "S&S Ethnicity Cluster ",cluster,": ",ethnicity_label,"<br>",
                             "Counts: ", n)) %>% 
  addPolylines(data = location_with_visits, 
               lat = ~latitude, 
               lng = ~longitude)
```

## Borough Clustering

Detailed tutorial found at <https://uc-r.github.io/hc_clustering>

Aggregate data to boroughs

```{r message=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization

ss_count_boro <- st_join(ss_london, boroMap) %>% 
  group_by(GSS_CODE, NAME) %>% 
  summarise(., total_ss_count=n(),) %>% 
  st_drop_geometry() %>% 
  na.omit()

arrest_count_boro <- st_join(ss_london, boroMap) %>% 
  filter(outcome=="Arrest") %>% 
  group_by(GSS_CODE, NAME) %>% 
  summarise(., total_arrest_count=n(),) %>% 
  st_drop_geometry() %>% 
  na.omit()

object_search_boro <- st_join(ss_london, boroMap) %>% 
  group_by(GSS_CODE, object_of_search) %>% 
  summarise(., object_searched_count=n(),) %>% 
  group_by(GSS_CODE) %>% 
  top_n(1, object_searched_count) %>% 
  st_drop_geometry() %>% 
  na.omit()

crime_count_boro <- st_join(crime_london, boroMap) %>% 
  group_by(GSS_CODE, NAME) %>% 
  summarise(., total_crime_count=n(),) %>%
  st_drop_geometry() %>% 
  na.omit()

# Select the row with the maximum value in each group
crime_max_boro <- st_join(crime_london, boroMap) %>% 
  group_by(GSS_CODE, crime_type) %>% 
  summarise(., major_crime_count=n(),) %>% 
  group_by(GSS_CODE) %>% 
  top_n(1, major_crime_count) %>% 
  st_drop_geometry()
```

```{r}
# add population data and inner and outer london classification at borough level
boroData <- read_csv("https://data.london.gov.uk/download/london-borough-profiles/c1693b82-68b1-44ee-beb2-3decf17dc1f8/london-borough-profiles.csv", na = c("NA", "n/a")) %>% 
  clean_names()

# trim unuseful rows 
boroData <- boroData[1:33,]
```

```{r}
# merge all df together 
merged_boro <- left_join(ss_count_boro, arrest_count_boro) %>% 
  left_join(., object_search_boro) %>% 
  left_join(., crime_count_boro) %>% 
  left_join(., crime_max_boro) %>% 
  left_join(., boroData %>% dplyr::select(code, inner_outer_london,
                                          gla_population_estimate_2017),
            by = c("GSS_CODE" = "code")) %>% 
  rename(pop = "gla_population_estimate_2017",
         boro_name = "NAME",
         inner_outer = "inner_outer_london") %>% 
  mutate(ss_rate = total_ss_count/pop *100,
         arrest_rate = total_arrest_count/pop *100,
         crime_rate = total_crime_count/pop *100,
         hit_rate = total_arrest_count/total_ss_count *100,
         object_search_prop = object_searched_count/total_ss_count *100,
         major_crime_prop =  major_crime_count/total_crime_count *100) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  as.data.frame()
```

Show the stats table what objects are commonly searched in each borough

```{r results='asis'}
merged_boro_all %>% 
  dplyr::select(boro_name,
                inner_outer,
                crime_rate,
               major_crime_type,
               major_crime_prop) %>% 
  arrange(desc(crime_rate)) %>% 
  kable()
```

Show the stats table what objects are commonly searched in each borough

```{r}

```

The hit rate is very inconsistent across boroughs

```{r}
summary(merged_boro_all$hit_rate)
summary(merged_boro_all$ss_rate)
```

```{r results='asis'}
merged_boro_all %>% 
  group_by(inner_outer) %>% 
  summarise_at(vars(ss_rate, arrest_rate, crime_rate, hit_rate),list(~mean(., trim = .2, na.rm=T))) %>% 
  kable()

```

```{r}
# select columns to clustering
df <- data.frame(merged_boro_all) %>% 
  dplyr::select(boro_name, GSS_CODE, ss_rate, arrest_rate, crime_rate)

# scale the ss_rate, arrest_rate and crime_rate
df[c(3:5)] <- scale(df[c(3:5)])

# set borough name as rownames
rownames(df) <- df$boro_name 

# remove the original NAME column
df <- df %>% 
  dplyr::select(-boro_name)
```

### Agglomerative clustering

To determine which clustering method I choose use, agnes function can calculate the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac)
```

Clearly, Ward method with highest coefficient helps identify the strongest clustering structures

```{r warning=FALSE}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
hc <- hclust(d, method = "ward.D2")

# Plot the obtained dendrogram
plot(hc, cex = 0.6, hang = -1)
```

To determine the optimal cluster number using the Elbow Method

```{r warning=FALSE}
# use Elbow Method first  
library(factoextra)
fviz_nbclust(df, FUN = hcut, method = "wss")
```

Elbow method suggests 4 group is the optimal cluster size

```{r}
# Cut tree into 4 groups
sub_grp <- cutree(hc, k = 4)

# Number of members in each cluster
table(sub_grp)
```

See how the boroughs are cut by 4 groups

```{r}
plot(hc, cex = 0.6)
rect.hclust(hc, k = 4, border = 2:5)
```

### Cluster characteristics

```{r message=False, warning=False, results='asis'}
# assign clusters back to rate df
merged_boro_all <- data.frame(merged_boro_all) %>% 
  mutate(cluster= as.character(sub_grp))

merged_boro_all %>% 
  group_by(cluster) %>% 
  summarise(across(ss_rate:crime_rate, median)) %>% 
  kable(., caption = 'Cluster Statistics')
```

```{r results='asis'}
merged_boro_all %>% arrange(desc(cluster)) %>% 
  kable()
```

Plot the cluster map

```{r}
# according to the cluster stats, rename clusters 
merged_boro_all$cluster[merged_boro_all$cluster == "1"] <- 
  "Low Search, Low Arrest, Low Crime"
merged_boro_all$cluster[merged_boro_all$cluster == "2"] <- 
  "Medium Search, Medium Arrest, Medium Crime"
merged_boro_all$cluster[merged_boro_all$cluster == "3"] <- 
  "High Search, High Arrest, High Crime"
merged_boro_all$cluster[merged_boro_all$cluster == "4"] <- 
  "Very High Search, Very High Arrest, Very High Crime"

# add geometry and plot
merged_boro_all <- merged_boro_all %>% 
  left_join(., boroMap %>% dplyr::select(GSS_CODE, geometry),
            by = "GSS_CODE") %>% 
  st_as_sf() 
```

### Maps

```{r warning=F, message=F}
# add inner and outer london london outline
innerOuterLondon <- st_read(here::here("data","lp-falp-2006-inner-outer-london-shp", 
                              "lp-falp-2006-inner-outer-london.shp")) %>% 
  st_transform(., 27700)

cluster_map <- tm_shape(merged_boro_all) + 
  tm_fill("cluster", 
          palette="viridis",
          title = "Crime Clusters in London",
          legend.hist=FALSE,
          title.fontface = "bold") +  
  tm_shape(innerOuterLondon) +
  tm_polygons(col = NA,
              alpha = 0.01,
              border.col = "red",
              lwd=1.5) +
  tm_borders(col = NA, alpha = 0.1) +
  tm_layout(legend.title.size = 0.6,
            legend.text.size = 0.5,
            legend.outside = FALSE,
            frame = FALSE,
            legend.position = c(.8,0),
            legend.title.fontface = "bold") +
  tm_compass(type = "4star", size = 3, fontsize = 0.5,
             position = c("left", "bottom")) +
  tm_scale_bar(position = c("left", "bottom")) 

tmap_save(cluster_map, "output/cluster_map.png",outer.margins = 0.2,dpi=500)
```

```{r}
tm_shape(merged_boro_all) + 
  tm_fill("cluster", 
          palette="viridis",
          title = "Crime Clusters in London",
          legend.hist=FALSE,
          title.fontface = "bold") +  
  tm_shape(innerOuterLondon) +
  tm_polygons(col = NA,
              alpha = 0.01,
              border.col = "red",
              lwd=1.5) +
  tm_borders(col = NA, alpha = 0.1) +
  tm_layout(legend.title.size = 0.7,
            legend.text.size = 0.4,
            legend.outside = FALSE,
            frame = FALSE,
            legend.position = c(.8,0),
            legend.title.fontface = "bold") +
  tm_compass(type = "4star", size = 3, fontsize = 0.5,
             position = c("left", "bottom")) +
  tm_scale_bar(position = c("left", "bottom")) 
```

## MSOA Spatial Autocorrelation

We have explored the point pattern analysis for London S&S and what about the MSOA patterns? Where are the hotspots and how to explain them? To do that, we need to count the number of crimes and S&S for each geographic unit. Here, I chose to group by msoa.

The demographic data was obtained from <https://data.london.gov.uk/dataset/msoa-atlas>.

```{r message=FALSE, warning=FALSE}
msoaData <- read_csv("https://data.london.gov.uk/download/msoa-atlas/20264159-36cb-4aa2-8371-ae884ae83e88/msoa-data.csv",na = c("NA", "n/a")) %>% 
  clean_names() 
```

Notice msoaData has 984 rows while the msoaMap has 983 rows. After inspection, the msoaData has an extra row showing the average statistics for all London msoa, which needs to be removed.

```{r}
msoaData <- msoaData[1:983,]
```

Select columns we need

```{r message=FALSE, warning=FALSE}
cols <- c(
"middle_super_output_area",
"msoa_name",
"age_structure_2011_census_all_ages",
"ethnic_group_2011_census_white",                                               
"ethnic_group_2011_census_asian_asian_british",
"ethnic_group_2011_census_black_african_caribbean_black_british",
"ethnic_group_2011_census_other_ethnic_group", 
"ethnic_group_2011_census_white_percent",
"ethnic_group_2011_census_asian_asian_british_percent",
"ethnic_group_2011_census_black_african_caribbean_black_british_percent",
"ethnic_group_2011_census_other_ethnic_group_percent",
"economic_activity_2011_census_unemployment_rate",
"household_income_estimates_2011_12_total_mean_annual_household_income",
"household_income_estimates_2011_12_total_median_annual_household_income",
"income_deprivation_2010_percent_living_in_income_deprived_households_reliant_on_means_tested_benefit",
"income_deprivation_2010_percent_of_people_aged_over_60_who_live_in_pension_credit_households",
"health_2011_census_bad_health_percent",
"car_or_van_availability_2011_census_no_cars_or_vans_in_household_percent")

# rename variables
msoaDataMerged <-
  left_join(msoaMap %>% dplyr::select(MSOA11CD,MSOA11NM,geometry),
            msoaData %>% dplyr::select(all_of(cols)),
            by = c("MSOA11CD" = "middle_super_output_area")) %>% 
  rename(all_population = "age_structure_2011_census_all_ages",
         white_pop = "ethnic_group_2011_census_white",
         asian_pop = "ethnic_group_2011_census_asian_asian_british",
         black_pop = "ethnic_group_2011_census_black_african_caribbean_black_british",
         otherEth_pop = "ethnic_group_2011_census_other_ethnic_group",
         white_pop_rate = "ethnic_group_2011_census_white_percent",
         asian_pop_rate = "ethnic_group_2011_census_asian_asian_british_percent",
         black_pop_rate = "ethnic_group_2011_census_black_african_caribbean_black_british_percent",
         otherEth_pop_rate = "ethnic_group_2011_census_other_ethnic_group_percent",
         unemp_rate = "economic_activity_2011_census_unemployment_rate",
         mean_income_rate = "household_income_estimates_2011_12_total_mean_annual_household_income",
         median_income_rate = "household_income_estimates_2011_12_total_median_annual_household_income",
         income_deprivation_rate = "income_deprivation_2010_percent_living_in_income_deprived_households_reliant_on_means_tested_benefit",
         income_old_deprivation_rate = "income_deprivation_2010_percent_of_people_aged_over_60_who_live_in_pension_credit_households",
         bad_health_rate = "health_2011_census_bad_health_percent",
         cars_rate = "car_or_van_availability_2011_census_no_cars_or_vans_in_household_percent")

```

Count all S&S for each ethnic group that fall within MSOAs.

```{r message=FALSE}
ss_count <- st_join(ss_london, msoaMap) %>% 
  group_by(MSOA11CD, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_ss = sum(c_across(Asian:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(ss_black = "Black",
                ss_white = "White",
                ss_asian = "Asian",
                ss_other = "Other",
                ss_na = "NA_eth") 
```

Count all violent crimes that fall within wards.

```{r message=FALSE}
crime_count <- st_join(crime_london, msoaMap) %>% 
  group_by(MSOA11CD) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry() %>% 
  na.omit(.) %>% 
  dplyr::rename(total_crime_count = "count")
```

Count successful S&S that leading to an arrest.

```{r message=FALSE, warning=FALSE}
arrest_count <- st_join(ss_london, msoaMap) %>% 
  filter(outcome=="Arrest") %>% 
  group_by(MSOA11CD, officer_defined_ethnicity) %>% 
  summarise(., arrest_count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = arrest_count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_arrest = sum(c_across(Asian:Other), na.rm = TRUE)) %>% 
  dplyr::rename(arrest_black = "Black",
                arrest_white = "White",
                arrest_asian = "Asian",
                arrest_other = "Other",
                arrest_na = "NA_eth") 
```

Join demographic data with crime data.

```{r}
merged <- left_join(msoaDataMerged, crime_count, by = c("MSOA11CD" = "MSOA11CD")) %>% 
  left_join(., ss_count, by = c("MSOA11CD" = "MSOA11CD")) %>% 
  left_join(., arrest_count, by = "MSOA11CD")
```

Calculate rates.

```{r message=FALSE}
all_data <- merged %>% 
  clean_names() %>%   
  # total crime rate 
  mutate(total_crime_rate = total_crime_count/
           all_population *100) %>% 
  # total crime proportion
  mutate(total_crime_prop = total_crime_count/
           sum(total_crime_count) *100) %>% 
  # total search rate
  mutate(total_search_rate = total_ss / 
           all_population *100) %>%
  # total arrest rate
  mutate(total_arrest_rate = total_arrest/
           all_population *100) %>% 
  # total hit rate 
  mutate(total_hit_rate = total_arrest/ 
           total_ss *100) %>% 
  # search rate for ethical groups 
  mutate(black_search_rate = ss_black/ black_pop *100) %>% 
  mutate(white_search_rate = ss_white/ white_pop *100) %>% 
  mutate(asian_search_rate = ss_asian/ asian_pop *100) %>% 
  mutate(other_search_rate = ss_other/ other_eth_pop *100) %>% 
  # hit rates for ethical groups 
  mutate(black_hit_rate = arrest_black/ ss_black *100) %>% 
  mutate(white_hit_rate = arrest_white/ ss_white *100) %>% 
  mutate(asian_hit_rate = arrest_asian/ ss_asian *100) %>% 
  mutate(other_hit_rate = arrest_other/ ss_other *100) %>% 
  # arrest rates for ethical groups
  mutate(black_arrest_rate = arrest_black/ black_pop *100) %>% 
  mutate(white_arrest_rate = arrest_white/ white_pop *100) %>% 
  mutate(asian_arrest_rate = arrest_asian/ asian_pop *100) %>% 
  mutate(other_arrest_rate = arrest_other/ other_eth_pop *100) %>% 
  mutate_if(is.numeric, ~round(., 2))

```

### Overall trend

Hit rate is, what proportion of searches, by race, were successful? If racial groups have different hit rates, it can imply that racial groups are being subjected to different standards.

```{r message=False, warning=False, results='asis'}
# count the number of searches by race
HR1 <- ss_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., searches=n()) %>% 
  st_drop_geometry()
  
# count the number of successful searches leading to arrest 
HR2 <- ss_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  filter(outcome=="Arrest") %>% 
  summarise(., arrests=n()) %>% 
  st_drop_geometry()

# calculate total population for each race based on 2011 census 
aP <- sum(as.numeric(all_data$asian_pop), na.rm = TRUE)
bP <- sum(as.numeric(all_data$black_pop), na.rm = TRUE)
wP <- sum(as.numeric(all_data$white_pop), na.rm = TRUE)
oP <- sum(as.numeric(all_data$other_eth_pop), na.rm = TRUE)

left_join(HR1, HR2) %>% 
  mutate(population = c(aP, bP, oP, wP, 0),
         search_prop = searches/sum(searches) *100,
         arrest_prop = arrests/sum(arrests) *100,
         population_prop = population/sum(population) *100,
         search_rate = searches/population *100,
         arrest_rate = arrests/population *100,
         hit_rate = arrests/searches *100) %>% 
  dplyr::rename(race = "officer_defined_ethnicity") %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  kable(.,)

```

### Below/Above S&S Avg.

```{r}
summary(all_data$black_search_rate)
summary(all_data$white_search_rate)
summary(all_data$asian_search_rate)
summary(all_data$other_search_rate)
```

```{r}
all_data_plot <- all_data %>% 
  dplyr::select(black_search_rate,
                white_search_rate,
                asian_search_rate,
                other_search_rate,
                black_hit_rate,
                white_hit_rate,
                asian_hit_rate,
                other_hit_rate,
                black_arrest_rate,
                white_arrest_rate,
                asian_arrest_rate,
                other_arrest_rate) %>% 
  mutate(black_search_compare = case_when(
    (black_search_rate >= 1.34 & black_search_rate < 8.4) ~ "Very Low",
    (black_search_rate >= 8.4 & black_search_rate < 23.6) ~ "Low",
    (black_search_rate >= 23.6 & black_search_rate < 26.22) ~ "High", 
    (black_search_rate >= 26.22 ~ "Very High")))
```

```{r}
tm_shape(all_data_plot) + 
  tm_fill("black_search_compare", 
          palette="viridis",
          title="Black Search Rate Comparison")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(a)", position=c(0,0.8), size=1.5)

```

### Search rate & Hit rate comparison

```{r message=FALSE, warning=FALSE}
# select columns we need but first change the msoa code name 
all_data <- all_data %>% 
  rename(msoa_code = "msoa11cd")

cols_s <- c("msoa_code",
          "black_search_rate", 
          "white_search_rate", 
          "asian_search_rate", 
          "other_search_rate")
cols_h <- c("black_hit_rate",
            "white_hit_rate",
            "asian_hit_rate",
            "other_hit_rate",
            "msoa_code")
cols_a <- c("black_arrest_rate",
            "white_arrest_rate",
            "asian_arrest_rate",
            "other_arrest_rate",
            "msoa_code")

search_data <- all_data[cols_s]
hit_data <- all_data[cols_h]
arrest_data <- all_data[cols_a]

# pivot longer 
search_df <- search_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("search_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_') 

hit_df <- hit_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("hit_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_')

arrest_df <- arrest_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("arrest_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_')

# join df
main_data <- left_join(search_df, hit_df) %>% 
  left_join(., arrest_df) %>% 
  rename(search_rate = "search",
         hit_rate = "hit",
         arrest_rate = "arrest") 
  #mutate_at(c(3,4), funs(c(scale(.))))

# convert NAs to zero
main_data[is.na(main_data)] <- 0
```

Compare search rate vs. hit rate for each race

```{r message=FALSE, warning=FALSE}
# We'll use this just to make our axes' limits nice and even
max_hit_rate <-
  main_data %>% 
  dplyr::select("hit_rate") %>% 
  max()

max_search_rate <-
  main_data %>% 
  dplyr::select("search_rate") %>% 
  max()

main_data %>% 
  ggplot(aes(
    x = search_rate,
    y = hit_rate
  )) +
  geom_point(size=0.5) +
  # This sets a diagonal reference line (line of equal search and hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("Search rate (%)", 
    limits = c(0, max_search_rate + 0.1)
  ) +
  scale_y_continuous("Hit rate (%)", 
    limits = c(0, max_hit_rate + 0.1)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  facet_wrap(. ~ race, nrow = 2)
```

Compare white hit rate with minorities' hit rate

```{r}
# Reshape table to show hit rates of minorities vs white 
main_data_white <- main_data %>% 
  dplyr::select(-search_rate) %>% 
  spread(race, hit_rate, fill=0) %>% 
  rename(white_hit_rate = white) %>% 
  gather(minority_race, minority_hit_rate, c(black, asian, other)) %>%
  arrange(msoa_code)
```

Plot

```{r}
max_hit_rate <-
  main_data_white %>% 
  dplyr::select(ends_with("hit_rate")) %>% 
  max()

main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point() +
  # This sets a diagonal reference line (line of equal hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  # This allows us to compare black v. white and Hispanic v. white side by
  # side, in panels
  facet_grid(. ~ minority_race)
```

Add the number of searches to the hit rate df

```{r}
search_count <- all_data %>% 
  dplyr::select("msoa_code", "ss_asian", "ss_black", "ss_white", "ss_other") %>% 
  st_drop_geometry(.) %>% 
  pivot_longer(cols = ! (msoa_code | ss_white),
               names_to = "minority_race",
               names_prefix = "ss_",
               values_to = "minority_search_counts") %>% 
  rename(white_search_counts = ss_white) %>% 
  arrange(msoa_code)

# convert NAs to zero
search_count[is.na(search_count)] <- 0

# join df
main_data_white <- left_join(main_data_white, 
                             search_count, 
                             by = c("msoa_code", "minority_race")) %>% 
  mutate(all_search_count = minority_search_counts + white_search_counts)
```

plot

```{r}
main_data_white_nonzero <- main_data_white %>% 
  filter(all_search_count != 0 )

main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point(aes(color=all_search_count, size=all_search_count), pch = 21) +
  scale_fill_viridis_c(option = "viridis",
                       aesthetics = c("colour", "fill")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  coord_fixed() +
  facet_grid(. ~ minority_race)
```

### Investigate anomalies

Examine the outliers of black/white/Asian search rate, arrest rate and hit rate

```{r}
ggplot(main_data, aes(x='', y=search_rate, color=race)) +
  geom_boxplot()

```

## Spatial autocorrelation

Create a subset data of all rates for later parts.

```{r}
all_rates <- all_data %>% 
  dplyr::select(ends_with("rate") | msoa_code)
```

Create a neighborhoods list and spatial weight matrix.

```{r message=FALSE, warning=FALSE}
library(spdep)
# calculate centroids 
coordsM<- all_rates%>%
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
Lward_nb <- all_rates %>%
  poly2nb(., queen=T, snap=0.00001)  
plot(Lward_nb, st_geometry(coordsM), col="red")

# create a spatial weight object
Lward_lw <- Lward_nb %>%
  nb2listw(., style="C")
```

### Global Moran's I

variables to test for spatial autocorrelation:

```{r}
# convert NA to zeros
all_rates[is.na(all_rates)] <- 0
colnames(all_rates)
```

Moran's I test tells us whether we have clustered values (close to 1) or dispersed values (close to -1).

```{r}
# variables used for the test
variable_names <- c( "black_search_rate",
                     "white_search_rate",
                     "asian_search_rate",
                     "black_hit_rate",
                     "white_hit_rate",
                     "asian_hit_rate",
                     "black_arrest_rate",
                     "white_arrest_rate",
                     "asian_arrest_rate",
                     "black_pop_rate",
                     "white_pop_rate",
                     "asian_pop_rate")

# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    moran.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist

```

### Geary's C

It tells us whether similar values or dissimilar values are clusering. Geary's C falls between 0 and 2; 1 means no spatial autocorrelation, \<1 - positive spatial autocorrelation or similar values clustering, \>1 - negative spatial autocorreation or dissimilar values clustering.

```{r}
# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    geary.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist
```

### General G

It tells us whether high or low values are clustering. If G \> Expected = High values clustering; if G \< expected = low values clustering.

```{r message=FALSE}
# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    globalG.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist
```

### Local Moran's I

Calculate local Moran's I for several important variables for inspecting racial discrimination of search rate and hit rate.

```{r}
moranI_black_search <- all_rates %>%
  pull(black_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_search <- all_rates %>% 
  pull(white_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_search <- all_rates %>% 
  pull(asian_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_black_hit <- all_rates %>% 
  pull(black_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_hit <- all_rates %>% 
  pull(white_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_hit <- all_rates %>% 
  pull(asian_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_arrest <- all_rates %>% 
  pull(asian_arrest_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_black_arrest <- all_rates %>% 
  pull(black_arrest_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_arrest <- all_rates %>% 
  pull(white_arrest_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()
```

copy the Moran's I score and the z-score standard deviation back to all_rates dataframe

```{r}
all_rates <- all_rates %>% 
  mutate(black_search_Iz = as.numeric(moranI_black_search$Z.Ii)) %>% 
  mutate(white_search_Iz = as.numeric(moranI_white_search$Z.Ii)) %>% 
  mutate(asian_search_Iz = as.numeric(moranI_asian_search$Z.Ii)) %>% 
  mutate(black_hit_Iz = as.numeric(moranI_black_hit$Z.Ii)) %>% 
  mutate(white_hit_Iz = as.numeric(moranI_white_hit$Z.Ii)) %>% 
  mutate(asian_hit_Iz = as.numeric(moranI_asian_hit$Z.Ii)) %>% 
  mutate(black_arrest_Iz = as.numeric(moranI_black_arrest$Z.Ii)) %>% 
  mutate(white_arrest_Iz = as.numeric(moranI_white_arrest$Z.Ii)) %>% 
  mutate(asian_arrest_Iz = as.numeric(moranI_asian_arrest$Z.Ii)) 
```

We'll set the breaks manually based on the rule that data points \>2.58 or \<-2.58 standard deviations away from the mean are significant at the 99% level (\<1% chance that autocorrelation not present); \>1.96 - \<2.58 or \<-1.96 to \>-2.58 standard deviations are significant at the 95% level (\<5% change that autocorrelation not present). \>1.65 = 90% etc.

#### Maps

```{r}
breaks1<-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)
MoranColours<- rev(brewer.pal(8, "RdGy"))


t1 <- tm_shape(all_rates) + 
  tm_polygons("black_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Black Search Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,            
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(a)", position=c(0,0.8), size=1)


t2 <- tm_shape(all_rates) + 
  tm_polygons("white_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, White Search Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.8), size=1)

t3 <- tm_shape(all_rates) + 
  tm_polygons("asian_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Asian Search Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.8), size=1)

t4 <- tm_shape(all_rates) + 
  tm_polygons("black_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Black Hit Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.8), size=1)

t5 <- tm_shape(all_rates) + 
  tm_polygons("white_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, White Hit Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.8), size=1)

t6 <- tm_shape(all_rates) + 
  tm_polygons("asian_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Asian Hit Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.8), size=1)

t7 <- tm_shape(all_rates) + 
  tm_polygons("black_arrest_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Black Arrest Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(g)", position=c(0,0.8), size=1)


t8 <- tm_shape(all_rates) + 
  tm_polygons("white_arrest_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, White Arrest Rate ") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(h)", position=c(0,0.8), size=1)

t9 <- tm_shape(all_rates) + 
  tm_polygons("asian_arrest_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Asian Arrest Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(i)", position=c(0,0.8), size=1)

t=tmap_arrange(t1, t2, t3, t7, t8, t9, t4, t5, t6,
               nrow = 3,
               ncol = 3)

t
```

## Regression Models

The local moran's I test suggests different ethic groups have different hot-spots of frequently searched places and arrested places. Therefore, it is reasonable to investigate each of the ethnic group in terms of their search rate and arrest rate more thoroughly.

### Maps

```{r}
tm2 <- tm_shape(all_rates) + 
  tm_fill("econ_inac_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="economic inactivity rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.6), size=1)


tm3 <- tm_shape(all_rates) + 
  tm_fill("unemployed_lone_parent_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="lone parent not in employment rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.6), size=1)

tm4 <- tm_shape(all_rates) + 
  tm_fill("no_qualification_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="no qualification rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.6), size=1)

tm5 <- tm_shape(all_rates) + 
  tm_fill("total_vio_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards violent crime rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.6), size=1)

tm6 <- tm_shape(all_rates) + 
  tm_fill("total_search_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards S&S (section 60) search rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.6), size=1)

tm7 <- tm_shape(all_rates) + 
  tm_fill("total_hit_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards S&S (section 60) hit rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(g)", position=c(0,0.6), size=1)

tm_shape(all_rates) + 
  tm_fill("idaci_index", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="idaci index", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(h)", position=c(0,0.6), size=1)

tm =tmap_arrange(tm2, tm3, tm4, tm5, tm6, tm7)

tm
```

### Research Hypothesis

1.  Is there a relationship between S&S (s60) search rate and arrest rate?

    -   Is increasing S&S (s60) potentially lead to more/less arrests across wards (i.e. are arrests distributed randomly or related to searches) ?

    -   What about for different minority groups, will the outcome be different?

2.  What are the factors that might lead to variation in search rate across the city?

    -   Is there a relationship between search rate and violent crime rate? How about factors such as lone parent unemployment rate, economic inactivity rate?

```{r}
colnames(all_data)
```

### Arrests

#### Linear regression

Assumption 1: there is a linear relationship between my variables --

-- yes, the linear pattern is very obvious.

```{r message=FALSE, warning=FALSE}
library(broom)
library(ggpubr)
library(car)

# to make results easy to interpret, I divide population by 1000, meaning the number of arrest/search per 1,000 population in London wards, rather than per population.
regression_data <- all_data %>% 
  mutate(total_search_1000 = total_ss60/all_population*1000,
         total_arrest_1000 = total_arrest/all_population*1000,
         black_search_1000 = ss60_black/black_pop*1000,
         black_arrest_1000 = arrest_black/black_pop*1000,
         white_search_1000 = ss60_white/white_pop*1000,
         white_arrest_1000 = arrest_white/white_pop*1000,
         asian_search_1000 = ss60_asian/asian_pop*1000,
         asian_arrest_1000 = arrest_asian/asian_pop*1000,
         population1000 = all_population/1000) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  st_drop_geometry() 

# convert NAs to zero
regression_data[is.na(regression_data)] <- 0

# check variables' histogram 
ggplot(regression_data, aes(x=total_search_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=total_arrest_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = (total_search_1000), 
      y = (total_arrest_1000),
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

```

Assumption 2: the residuals in my model should be normally distributed

-- yes!

```{r message=FALSE, warning=FALSE}
# compute linear regression model
model_lr <- regression_data %>%
  lm(total_arrest_1000 ~ total_search_1000, data=.)

# summary stats
# tidy(model1)
# glance(model1)
summary(model_lr)

# append residuals to regression_data 
regression_data <- model_lr %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_lr)
d <- deviance(model_lr)
par(mfrow=c(2,2))
plot(regression_data$total_search_1000, rsid)
boxplot(rsid~total_search_1000,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

```

The predicted function is:

y = 0.0079 + 0.0424 ss60, meaning for 1 increase in stop and search for every thousand of population, the model suggests the arrest count is expected to increase 0.0424. In order words, on average, for every 1k stop and search in London, we are expected to have 42 arrests.

Assumption 3: errors/residuals in the model exhibit constant / homogeneous variance

-- The first and third plot shows some clustering pattern and the red line is not straight suggesting some hetroscedasticity, perhaps because there are many wards with zero arrest rate or search rate.

```{r}
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_lr)
```

Assumption 4: independent of errors, look for spatial autocorrelation

```{r message=F, warning=F}
# transform regression data to sf object
regression_data <- regression_data %>% 
  left_join(.,wardMap %>% dplyr::select(GSS_CODE,geometry),
            by = c("ward_code" = "GSS_CODE"))
regression_data <- data.frame(regression_data) %>% 
  st_as_sf()

# calculate centroids 
coordsM<- regression_data %>% 
  st_centroid()%>%
  st_geometry()

# create a neighborhoods list
Lward_nb <- regression_data %>%
  poly2nb(., queen=T, snap=0.00001)  

# create a spatial weight object
Lward_lw <- Lward_nb %>%
  nb2listw(., style="C")

regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)

```

Moran's I statistic is 0.01, showing no spatial autocorrelation in my residuals. Linear regression is suitable for my rate data is okay but what if using count data in a Poisson regression model...

Plot equation back to the scatter plot

```{r}
Lfun <- function(xvar) {
 (0.0079 + 0.0424*xvar)
}

# plot
ggplot(regression_data,
       aes(y=total_arrest_1000,
           x=total_search_1000)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = 0.0079 + 0.0424*x \n R-sqr = 0.798", vjust=3, hjust=1, col="red")
```

#### Poisson regression

Assumptions (<https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html>)

1.  **Poisson Response** The response variable is a count per unit of time or space, described by a Poisson distribution.

2.  **Independence** The observations must be independent of one another.

3.  **Mean=Variance** By definition, the mean of a Poisson random variable must be equal to its variance.

4.  **Linearity** The log of the mean rate, log(), must be a linear function of x.

First checking the histogram and scatter plot of the original data

```{r}
ggplot(regression_data, aes(x=total_ss60))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=total_arrest))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

qplot(x = total_ss60, 
      y = total_arrest,
      data = regression_data) +
stat_smooth(method="glm", se=FALSE, size=1) + 
  geom_jitter()
```

Both histogram reveal the pattern often found with distributions of counts of rare events. Many wards have few or none ss60 and arrests and a few wards have a large number of ss60 or arrests making for a distribution that appears to be far from normal. Therefore, Poisson regression should be used to model our data.

Next, checking mean and variance, they should be equal if they follow Poisson distribution.

```{r}
mean(regression_data$total_arrest)
var(regression_data$total_arrest)
```

The variance is slightly larger than the mean so should not be a big problem.

While a Poisson regression model is a good first choice because the responses are counts per period of time (from April 2019 to April 2021), it is important to note that the counts are not directly comparable because they come from different size wards. This issue sometimes is referred to as the need to account for *sampling effort*; in other words, we expect wards with more population to have more reports of arrests since there are more people who could be affected. We can take the differences in ward population into account by including an **offset** in our model.

```{r}
library(stats)
model_pr <- glm(total_arrest ~ total_ss60, 
              family = "poisson", 
              offset = log(population1000),
              data = regression_data)
summary(model_pr)
```

The standard Poisson regression model gives us,

log() = -2.89 + 0.008 ss60,

by exponentiating the coefficient of ss60 we obtain the multiplicative factor by which the mean count changes. In this case, the mean number of S&S (s60) arrests changes by a factor of e\^0.008=1.01 or increases by 0.01 % (since 1.01-1=0.01) with each additional stop and search for violating section 60. 1.01 is referred to as a **rate ratio** or **relative risk**.

We can use standard error to construct a confidence interval for 1. A 95% CI provides a range of plausible values for the ss60 coefficient and can be constructed by using "sandwich" library based on work done by Cameron and Trivedi (2009). <https://stats.idre.ucla.edu/r/dae/poisson-regression/>

```{r}
library(sandwich)
cov.mdl2 <- vcovHC(model_pr, type="HC0")
std.err <- sqrt(diag(cov.mdl2))
r.est <- cbind(Estimate= coef(model_pr), "Robust SE" = std.err,
"Pr(>|z|)" = 2 * pnorm(abs(coef(model_pr)/std.err), lower.tail=FALSE),
LL = coef(model_pr) - 1.96 * std.err,
UL = coef(model_pr) + 1.96 * std.err)
r.est
```

Exponentiating the endpoints yields a confidence interval for the relative risk; i.e., the percent change in arrest counts for each additional percent increase in ss60.

( e^0.006513633^ , e^0.009542442^ ) = (1.00653 , 1.00959) suggesting that we are 95% confident that the mean number in the arrest increase between 0.65% and 0.96% for each additional stop and search for section 60. The interval does not include 1 so we can conclude stop and search is significantly associated with arrests.

Deviance is a way in which to measure how the observed data deviates from the model predictions, similar to sum of squared errors in linear regression. Because we want models that minimize deviance, we calculate the drop-in-deviance when adding age to the model with no covariates (the null model).

The deviances for the null model and the model with age can be found in the model output. A residual deviance for the model with ss60 is reported as 1143.1 with 623 df. The output also includes the deviance and degrees of freedom for the null model (1879.7 with 624 df). The drop-in-deviance is 736.6 (1879.7-1143.1) with a difference of only 1 df, so that the addition of one extra term (ss60) reduced unexplained variability by 736.6.

If the null model were true, we would expect the drop-in-deviance to follow a 2 distribution with 1 df. Therefore, the p-value for comparing the null model to the model with ss60 is found by determining the probability that the value for a 2 random variable with one degree of freedom exceeds 736.6, which is essentially 0. Once again, we can conclude that we have statistically significant evidence (2 (df=1)=736.6, p\<.001) that average count of arrest increases as stop and search increases.

```{r}
# model0 is the null/reduced model
model0 <- glm(total_arrest ~ 1, family = poisson, offset = log(population1000), 
              data = regression_data)
anova(model0, model_pr, test = "Chisq")
```

Based on the above, we can see that the p-value \< 0.05, so we reject the null hypothesis that the regression coefficients are all zero. Hence our model is statistically significant.

**Drop-in-deviance test to compare models**

-   Compute the deviance for each model, then calculate: drop-in-deviance = residual deviance for reduced model -- residual deviance for the larger model.

-   When the reduced model is true, the drop-in-deviance d2 where d= the difference in the degrees of freedom associated with the two models (that is, the difference in the number of terms/coefficients).

-   A large drop-in-deviance favors the larger model.

```{r}
rsid <- residuals(model_pr)
d <- deviance(model_pr)
par(mfrow=c(2,2))
plot(regression_data$total_ss60, rsid)
boxplot(rsid~total_ss60,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)
```

We saw residuals are not normally distributed, probably because we have many observations that have certain amount of searches but no arrests, leading to a bad model prediction.

**Goodness of fit:**

In order to assess goodness of fit, we will look at the deviance residuals, and apply a df2 test based on our n-p-1 degrees of freedom. In this case, our p-value is zero, and so we fail to reject the null hypothesis that the model is not a good fit.

```{r}
c(deviance(model_pr), 1-pchisq(model_pr$deviance, model_pr$df.residual))  # GOF test
```

The model residual deviance can be used to assess the degree to which the predicted values differ from the observed. When a model is true, we can expect the residual deviance to be distributed as a 2 random variable with degrees of freedom equal to the model's residual degrees of freedom. Our model has a residual deviance of 1143.1 with 623 df. The probability of observing a deviance this large if the model fits is esentially 0, saying that there is significant evidence of lack-of-fit.

There are several reasons why **lack-of-fit** may be observed. (1) We may be missing important covariates or interactions; a more comprehensive data set may be needed. (2) There may be extreme observations that may cause the deviance to be larger than expected; indeed, there are very few places with very high S&S counts and the majority of places with very little S&S. (3) Lastly, there may be a problem with the Poisson model. In particular, the Poisson model has only a single parameter, , for each combination of the levels of the predictors which must describe both the mean and the variance. This limitation can become manifest when the variance appears to be larger than the corresponding means. In that case, the response is more variable than the Poisson model would imply, and the response is considered to be **overdispersed**.

#### Negative binomial regression

```{r message=F, warning=F}
library(MASS)
model_nb <- glm.nb(total_arrest ~ total_ss60 +log(total_violent_crime),
                   offset(log(population1000)),
                   data = regression_data)
summary(model_nb)
```

```{r}
model0 <- glm.nb(total_arrest ~ 1, data = regression_data)
anova(model0, model_nb, test = "Chisq")
```

P-value =1 meaning the model does not predict values well, so should stick with Poisson regression.

```{r}
c(deviance(model_nb), 1-pchisq(model_nb$deviance, model_nb$df.residual)) 
```

Plot the predicted equation from Poisson regression back to our scatter plot

log(y) = -2.89 + 0.008 ss60 (Poisson)

```{r}
Pfun <- function(xvar) {
 (exp(-2.89) + exp(0.008*xvar))
}

# plot
ggplot(regression_data,
       aes(y=total_arrest,x=total_ss60)) +
  geom_point() +
  stat_function(fun = Pfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "log(y) = -2.89 + 0.008*x \n Residual Deviance = 1143.1", vjust=3, hjust=1, col="red")
```

#### Black

```{r warning=FALSE, message=FALSE}
###### assumption1 #####
ggplot(regression_data, aes(x=black_search_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 10) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=black_arrest_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = black_search_1000, 
      y = black_arrest_1000,
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

###### assumption2 #####
# compute linear regression model
model_b <- regression_data %>% 
  lm(black_arrest_1000 ~ black_search_1000, data=.)

# summary stats
summary(model_b)

# append residuals data to regression_data 
regression_data <- model_b %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_b)
d <- deviance(model_b)
par(mfrow=c(2,2))
plot(regression_data$black_search_1000, rsid)
boxplot(rsid~black_search_1000,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

###### assumption3 #####
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_b)


###### assumption4 #####
regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)
```

```{r}
library(ggplot2)
Lfun <- function(xvar) {
 (0.0689 + 0.0395*xvar)
}

# plot
ggplot(regression_data,
       aes(y=black_arrest_1000,x=black_search_1000)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = 0.0689 + 0.0395*x \n R-sqr = 0.715", vjust=3, hjust=1, col="red")
```

#### White

```{r warning=FALSE, message=FALSE}
###### assumption1 #####
ggplot(regression_data, aes(x=white_search_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=white_arrest_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = white_search_1000, 
      y = white_arrest_1000,
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

###### assumption2 #####
# compute linear regression model
model_w <- regression_data %>% 
  lm(white_arrest_1000 ~ white_search_1000, data=.)

# summary stats
summary(model_w)

# append residuals data to regression_data 
regression_data <- model_w %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_w)
d <- deviance(model_w)
par(mfrow=c(2,2))
plot(regression_data$white_search_1000, rsid)
boxplot(rsid~white_search_1000,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

###### assumption3 #####
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_w)


###### assumption4 #####
regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)
```

```{r}
library(ggplot2)
Lfun <- function(xvar) {
 (0.00594 + 0.0433*xvar)
}

# plot
ggplot(regression_data,
       aes(y=white_arrest_1000,x=white_search_1000)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = 0.00594 + 0.0433*x \n R-sqr = 0.4917",vjust=3, hjust=1, col="red")
```

#### Asian

```{r warning=FALSE, message=FALSE}
###### assumption1 #####
ggplot(regression_data, aes(x=asian_search_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=asian_arrest_1000))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = asian_search_1000, 
      y = asian_arrest_1000,
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

###### assumption2 #####
# compute linear regression model
model_a <- regression_data %>% 
  lm(asian_arrest_1000 ~ asian_search_1000, data=.)

# summary stats
summary(model_a)

# append residuals data to regression_data 
regression_data <- model_a %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_a)
d <- deviance(model_a)
par(mfrow=c(2,2))
plot(regression_data$asian_search_1000, rsid)
boxplot(rsid~asian_search_1000,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

###### assumption3 #####
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_a)


###### assumption4 #####
regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)
```

```{r}
library(ggplot2)
Lfun <- function(xvar) {
 (-0.00199 + 0.0344*xvar)
}

# plot
ggplot(regression_data,
       aes(y=asian_arrest_1000,x=asian_search_1000)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = -0.00199 + 0.0344*x \n R-sqr = 0.322", vjust=3, hjust=1, col="red")
```

### Searches

```{r}
# check variables' histogram 
ggplot(regression_data, aes(x=log(total_search_rate)))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=log(total_vio_rate)))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=unemployed_lone_parent_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=no_qualification_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=econ_inac_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=white_pop))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 100) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=log(black_pop)))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=log(asian_pop)))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=idaci_index))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=idaopi_index))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.05) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)
```

Because violent crime rate and stop and search rate are left-skewed, I take log-transformation to them but search rate have 0 which cannot be logged so I need to convert inf to really small number

```{r}
regression_data2 <- all_data %>% 
  dplyr::select(ward_code,
                econ_inac_rate,
                unemployed_lone_parent_rate,
                no_qualification_rate,
                total_vio_rate,
                total_search_rate,
                idaci_index,
                idaopi_index,
                white_pop,
                black_pop,
                asian_pop,
                all_population) %>% 
  mutate(vio_log_rate = log(total_vio_rate),
         search_log_rate = log(total_search_rate))%>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  st_drop_geometry() %>% 
  na.omit(.)

# build model 
model_mlr <- lm(search_log_rate ~ vio_log_rate  +  econ_inac_rate + 
                    unemployed_lone_parent_rate + no_qualification_rate + idaci_index +
                    idaopi_index + white_pop + log(black_pop) + log(asian_pop), 
                  data = regression_data2)
summary(model_mlr)
```

VIF

```{r}
vif(model_mlr)
```

Remove idaci_index to avoid Multicolinearity

```{r}
# build model 
model_final <- lm(search_log_rate ~ vio_log_rate + econ_inac_rate + 
                    unemployed_lone_parent_rate + no_qualification_rate + idaopi_index +
                    log(white_pop) + black_pop + log(asian_pop),
                  data = regression_data2)
summary(model_final)
```

Check VIF again

```{r}
vif(model_final)
```

Check residual's distribution and variance

```{r}
# append residuals data to regression_data 
regression_data2 <- model_final %>%
  augment(., regression_data2)

# plot residuals
regression_data2 %>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

# check Homoscedasticity
par(mfrow=c(2,2))    
plot(model_final)

```

Check spatial autocorrelation for the residuals

```{r message=FALSE, warning=FALSE}
# join geometry data to regression_data2
regression_data2 <- regression_data2 %>% 
  left_join(.,
            wardMap %>% dplyr::select(GSS_CODE, geometry),
            by = c("ward_code" = "GSS_CODE")) %>% 
  st_as_sf()

# create spatial weight matrix 
coordsW <- regression_data2%>%
  st_centroid()%>%
  st_geometry()

knn_wards <-coordsW %>%
  knearneigh(., k=4)

LWard_knn <- knn_wards %>%
  knn2nb()

plot(LWard_knn, st_geometry(coordsW), col="red")

Lward.knn_4_weight <- LWard_knn %>%
  nb2listw(., style="C")

# run moran's I test on the residuals
Nearest_neighbour <- regression_data2 %>%
  st_drop_geometry()%>%
  dplyr::select(.resid)%>%
  pull()%>%
  moran.test(., Lward.knn_4_weight)

Nearest_neighbour
```

Moran's I index is 0.4, showing a moderate spatial autocorrelation on my multiple linear regression residuals. Check the quick plot

```{r}
tm_shape(regression_data2) +
  tm_polygons(".resid", 
              palette = "RdYlBu")
```

#### Spatially Lagged models

```{r message=FALSE, warning=FALSE}
#run a spatially-lagged regression model
model_gwr <- lagsarlm(search_log_rate ~ vio_log_rate + econ_inac_rate + 
                    unemployed_lone_parent_rate + no_qualification_rate + idaopi_index +
                    log(white_pop) + black_pop + log(asian_pop),
                  data = regression_data2,
                  nb2listw(LWard_knn, style="C"), 
                  method = "eigen")

summary(model_gwr)
```

```{r message=FALSE, warning=FALSE}
glance(model_gwr)
```

#### Spatial Error Model

```{r warning=FALSE}
model_sem <- errorsarlm(search_log_rate ~ vio_log_rate + econ_inac_rate + 
                    unemployed_lone_parent_rate + no_qualification_rate + idaopi_index +
                    log(white_pop) + black_pop + log(asian_pop),
                  data = regression_data2,
                  nb2listw(LWard_knn, style="C"), 
                  method = "eigen")

summary(model_sem)
```
