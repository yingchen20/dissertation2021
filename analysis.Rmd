---
title: "dissertation analysis"
author: "Ying Chen"
date: "19/06/2021"
output: html_document
---

```{r message=FALSE}
library(sf)
library(tidyverse)
library(dplyr)
library(janitor)
library(RColorBrewer)
library(classInt)
library(sp)
library(rgeos)
library(rgdal)
library(spatstat)
library(here)
library(maptools)
library(GISTools)
library(tmap)
library(geojson)
library(geojsonio)
library(tmaptools)
library(hexbin)
library(ggspatial)
library(ggsn)
library(raster)
library(fpc)
library(dbscan)
library(plotrix)
library(spdep)
library(ggplot2)
library(ggpubr)
library(factoextra) 
library(knitr)
```

## Understand London Crime in general

-   spatial data: downloaded from London Datastore at <https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london> and stored in the local folder "data".
-   crime data: downloaded from <https://data.police.uk/data/> by selecting only Metropolitan Police Service for data from April 2019 to April 2021, renamed the folder as "crime_data" stored in the local folder "data".

```{r message=FALSE}
boroMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

wardMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Ward_CityMerged.shp")) %>% 
  st_transform(., 27700)

msoaMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "MSOA_2011_London_gen_MHW.shp")) %>% 
  st_transform(., 27700)
```

The following function used to search files that follow a specific pattern is obtained from <https://github.com/sjaraha/clustering-spatiotemporal-data>.

```{r}
list_data_paths <- function(pattern, rec){
  # searches working directory for files that match the specified pattern
  # on match, adds file path to a list
  # returns list the list of matching file paths
  ## pattern (str): regex pattern to match
  ## rec (boolean): recurse into directories (True) or don't (False)
  
  # initialize list
  data_path_list <- c()
  # loop through directories
  for (pd in list.dirs(recursive = rec)){
    # loop through files in directories
    for (f in list.files(pd)){
      # find files that match the pattern
      if (grepl(pattern, f, ignore.case = FALSE)==TRUE){
        # construct path to matching file
        data_path <- paste(pd,f, sep="/")
        # add path to list
        data_path_list <- c(data_path_list,data_path)
      }}}
  # return list of paths to matching files
  return(data_path_list)
}
```

Use the function above and search for all crime data (ended with metropolitan-street.csv) in my local folder.

```{r message = FALSE, warning=FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
crime_list <- list_data_paths("\\-metropolitan-street.csv$",FALSE) 

crime <- crime_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

Remove duplicated rows, clean names and select points within London.

```{r message = FALSE}
crime<- crime %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

crime_london <- crime[boroMap,]
```

Create a table counting total crime numbers for each crime type.

```{r message=False, warning=False}
library(knitr)
crime_london %>% 
  group_by(crime_type) %>% 
  summarise(., count=n(),) %>%
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(.)
```

I want to count the number of crimes by boroughs but they record crimes by LSOA, so I need to extract borough names from LSOA names. (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
crime_london$boro_name = substr(crime_london$lsoa_name,1,nchar(crime_london$lsoa_name)-5)

crime_london %>% 
  group_by(boro_name) %>% 
  summarise(., count=n(),) %>% 
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(prop)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London crimes by boroughs from April 2019 to April 2021')
```

## Understand stop and search (S&S) data

Search for stop and search data (ends with -metropolitan-stop-and-search.csv).

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
ss_list <- list_data_paths("\\-metropolitan-stop-and-search.csv$",FALSE) 

ss <- ss_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

clean names, remove duplicated records and select points within London boroughs.

```{r message=FALSE, warning=False}
ss <- ss %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

ss_london <- ss[boroMap,]
```

Check how police distributes their power on stop and search -- Searches for what?

```{r message=False, warning=False}
ssTab <- ss_london %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., searches=n(),) %>%
  st_drop_geometry()

arrestTab <- ss_london %>% 
  filter(outcome=="Arrest") %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., arrests=n(),) %>%
  st_drop_geometry()

left_join(ssTab, arrestTab, by = c("legislation", "object_of_search")) %>% 
  as.data.frame() %>% 
  mutate(search_rate = searches/sum(searches) *100,
         hit_rate = arrests/searches *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(hit_rate)) %>% 
  kable()
```

```{r results='asis'}
ssTab <- ss_london %>% 
  group_by(legislation) %>% 
  summarise(., searches=n(),) %>%
  st_drop_geometry()

arrestTab <- ss_london %>% 
  filter(outcome=="Arrest") %>% 
  group_by(legislation) %>% 
  summarise(., arrests=n(),) %>%
  st_drop_geometry()

left_join(ssTab, arrestTab, by = "legislation") %>% 
  as.data.frame() %>% 
  mutate(search_prop = searches/sum(searches) *100,
         hit_rate = arrests/searches *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(hit_rate)) %>% 
  kable()
```

Who are searched and arrested?

```{r warning=FALSE, message=FALSE}
arrestRaceTab <- ss_london %>%  
  filter(outcome == "Arrest") %>% 
  group_by(legislation, officer_defined_ethnicity) %>% 
  summarise(., arrests=n()) %>%
  st_drop_geometry()

ssRaceTab <- ss_london %>%  
  group_by(legislation, officer_defined_ethnicity) %>% 
  summarise(., searches=n()) %>%
  st_drop_geometry()

left_join(ssRaceTab, arrestRaceTab, by=c("legislation", "officer_defined_ethnicity")) %>% 
  as.data.frame() %>% 
  mutate(search_rate = searches/sum(searches) *100,
         hit_rate = arrests/searches *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(legislation)) %>% 
  kable()
```

### Trend?

```{r}
# ss_london %>%
#   count(year = year(date), officer_defined_ethnicity) %>%
#   ggplot(aes(x = year, y = n, color = officer_defined_ethnicity)) +
#   geom_point() +
#   geom_line()
```

## Borough Clustering

Detailed tutorial found at <https://uc-r.github.io/hc_clustering>

Aggregate data to boroughs

```{r message=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization

ss_count_boro <- st_join(ss_london, boroMap) %>% 
  group_by(GSS_CODE, NAME) %>% 
  summarise(., total_ss_count=n(),) %>% 
  st_drop_geometry() %>% 
  na.omit()

arrest_count_boro <- st_join(ss_london, boroMap) %>% 
  filter(outcome=="Arrest") %>% 
  group_by(GSS_CODE, NAME) %>% 
  summarise(., total_arrest_count=n(),) %>% 
  st_drop_geometry() %>% 
  na.omit()

object_search_boro <- st_join(ss_london, boroMap) %>% 
  group_by(GSS_CODE, object_of_search) %>% 
  summarise(., object_searched_count=n(),) %>% 
  group_by(GSS_CODE) %>% 
  top_n(1, object_searched_count) %>% 
  st_drop_geometry() %>% 
  na.omit()

crime_count_boro <- st_join(crime_london, boroMap) %>% 
  group_by(GSS_CODE, NAME) %>% 
  summarise(., total_crime_count=n(),) %>%
  st_drop_geometry() %>% 
  na.omit()

# Select the row with the maximum value in each group
crime_max_boro <- st_join(crime_london, boroMap) %>% 
  group_by(GSS_CODE, crime_type) %>% 
  summarise(., major_crime_count=n(),) %>% 
  group_by(GSS_CODE) %>% 
  top_n(1, major_crime_count) %>% 
  st_drop_geometry()
```

```{r message=F, warning=FALSE}
# add population data and inner and outer london classification at borough level
boroData <- read_csv("https://data.london.gov.uk/download/london-borough-profiles/c1693b82-68b1-44ee-beb2-3decf17dc1f8/london-borough-profiles.csv", na = c("NA", "n/a")) %>% 
  clean_names()

# trim unuseful rows 
boroData <- boroData[1:33,]
```

```{r warning=FALSE, message=FALSE}
# merge all df together 
merged_boro <- left_join(ss_count_boro, arrest_count_boro) %>% 
  left_join(., object_search_boro) %>% 
  left_join(., crime_count_boro) %>% 
  left_join(., crime_max_boro) %>% 
  left_join(., boroData %>% dplyr::select(code, inner_outer_london,
                                          gla_population_estimate_2017),
            by = c("GSS_CODE" = "code")) %>% 
  rename(pop = "gla_population_estimate_2017",
         boro_name = "NAME",
         inner_outer = "inner_outer_london",
         major_crime_type = "crime_type") %>% 
  mutate(ss_rate = total_ss_count/pop *100,
         arrest_rate = total_arrest_count/pop *100,
         crime_rate = total_crime_count/pop *100,
         hit_rate = total_arrest_count/total_ss_count *100,
         object_search_prop = object_searched_count/total_ss_count *100,
         major_crime_prop =  major_crime_count/total_crime_count *100) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  as.data.frame()
```

Show the stats table what objects are commonly searched in each borough

```{r results='asis'}
merged_boro %>% 
  dplyr::select(boro_name,
                inner_outer,
                crime_rate,
               major_crime_type,
               major_crime_prop) %>% 
  arrange(desc(crime_rate)) %>% 
  kable()
```

Show the stats table what objects are commonly searched in each borough

```{r results='asis'}
merged_boro %>% 
  dplyr::select(boro_name,
                inner_outer,
                ss_rate,
                arrest_rate,
                hit_rate,
               object_of_search,
               object_search_prop) %>% 
  arrange(desc(object_search_prop)) %>% 
  kable()

```

The hit rate is very inconsistent across boroughs

```{r}
summary(merged_boro$hit_rate)
summary(merged_boro$ss_rate)
summary(merged_boro$arrest_rate)
```

```{r results='asis'}
merged_boro %>% 
  group_by(inner_outer) %>% 
  summarise_at(vars(ss_rate, arrest_rate, crime_rate, hit_rate),list(~mean(., trim = .2, na.rm=T))) %>% 
  kable()

```

Prepare data for clustering

```{r}
# select columns to clustering
df <- data.frame(merged_boro) %>% 
  dplyr::select(boro_name, GSS_CODE, inner_outer, ss_rate, arrest_rate, crime_rate, hit_rate)

# scale the ss_rate, arrest_rate and crime_rate
df[c(4:7)] <- scale(df[c(4:7)])

# set borough name as rownames
rownames(df) <- df$boro_name 

# remove the original NAME column
df <- df %>% 
  dplyr::select(-boro_name)
```

### Agglomerative clustering

To determine which clustering method I choose use, agnes function can calculate the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac)
```

Clearly, Ward method with highest coefficient helps identify the strongest clustering structures

```{r warning=FALSE}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
hc <- hclust(d, method = "ward.D2")

# Plot the obtained dendrogram
plot(hc, cex = 0.6, hang = -1)
```

To determine the optimal cluster number using the Elbow Method

```{r warning=FALSE}
# use Elbow Method first  
library(factoextra)
fviz_nbclust(df, FUN = hcut, method = "wss")
```

Elbow method suggests 4 group is the optimal cluster size

```{r}
# Cut tree into 4 groups
sub_grp <- cutree(hc, k = 4)

# Number of members in each cluster
table(sub_grp)
```

See how the boroughs are cut by 4 groups

```{r}
plot(hc, cex = 0.6)
rect.hclust(hc, k = 4, border = 2:5)
```

### Cluster characteristics

```{r message=False, warning=False, results='asis'}
# assign clusters back to rate df
merged_boro <- data.frame(merged_boro) %>% 
  mutate(cluster= as.character(sub_grp))

merged_boro %>% 
  group_by(cluster) %>% 
  summarise(across(ss_rate:hit_rate, median)) %>% 
  kable(., caption = 'Cluster Statistics')
```

Rename the cluster label

```{r}
# according to the cluster stats, rename clusters
merged_boro$cluster[merged_boro$cluster == "1"] <-
  "low crimes, low S&S and outcomes  "
merged_boro$cluster[merged_boro$cluster == "2"] <-
  "medium crimes, medium S&S and outcomes"
merged_boro$cluster[merged_boro$cluster == "3"] <-
  "high crimes, high S&S and outcomes"
merged_boro$cluster[merged_boro$cluster == "4"] <-
  "very high crimes, very high S&S and outcomes"

# add geometry and plot
merged_boro <- merged_boro %>% 
  left_join(., boroMap %>% dplyr::select(GSS_CODE, geometry),
            by = "GSS_CODE") %>% 
  st_as_sf() 
```

### Maps

```{r warning=F, message=F}
# add inner and outer london london outline
innerOuterLondon <- st_read(here::here("data","lp-falp-2006-inner-outer-london-shp", 
                              "lp-falp-2006-inner-outer-london.shp")) %>% 
  st_transform(., 27700)

cluster_map <- tm_shape(merged_boro) + 
  tm_fill("cluster", 
          palette="viridis",
          title = "Crime Clusters in London",
          legend.hist=FALSE,
          title.fontface = "bold") +  
  tm_shape(innerOuterLondon) +
  tm_polygons(col = NA,
              alpha = 0.01,
              border.col = "red",
              lwd=1.5) +
  tm_borders(col = NA, alpha = 0.1) +
  tm_layout(legend.title.size = 0.6,
            legend.text.size = 0.5,
            legend.outside = FALSE,
            frame = FALSE,
            legend.position = c(.8,0),
            legend.title.fontface = "bold") +
  tm_compass(type = "4star", size = 3, fontsize = 0.5,
             position = c("left", "bottom")) +
  tm_scale_bar(position = c("left", "bottom")) 

tmap_save(cluster_map, "output/cluster_map.png",outer.margins = 0.2,dpi=500)
```

## MSOA Spatial Autocorrelation

We have explored the point pattern analysis for London S&S and what about the MSOA patterns? Where are the hotspots and how to explain them? To do that, we need to count the number of crimes and S&S for each geographic unit. Here, I chose to group by msoa.

The demographic data was obtained from <https://data.london.gov.uk/dataset/msoa-atlas>.

```{r message=FALSE, warning=FALSE}
msoaData <- read_csv("https://data.london.gov.uk/download/msoa-atlas/20264159-36cb-4aa2-8371-ae884ae83e88/msoa-data.csv",na = c("NA", "n/a")) %>% 
  clean_names() 
```

Notice msoaData has 984 rows while the msoaMap has 983 rows. After inspection, the msoaData has an extra row showing the average statistics for all London msoa, which needs to be removed.

```{r}
msoaData <- msoaData[1:983,]
```

Select columns we need

```{r message=FALSE, warning=FALSE}
cols <- c(
"middle_super_output_area",
"msoa_name",
"age_structure_2011_census_all_ages",
"ethnic_group_2011_census_white",                                               
"ethnic_group_2011_census_asian_asian_british",
"ethnic_group_2011_census_black_african_caribbean_black_british",
"ethnic_group_2011_census_other_ethnic_group", 
"ethnic_group_2011_census_white_percent",
"ethnic_group_2011_census_asian_asian_british_percent",
"ethnic_group_2011_census_black_african_caribbean_black_british_percent",
"ethnic_group_2011_census_other_ethnic_group_percent",
"economic_activity_2011_census_unemployment_rate",
"household_income_estimates_2011_12_total_mean_annual_household_income",
"household_income_estimates_2011_12_total_median_annual_household_income",
"income_deprivation_2010_percent_living_in_income_deprived_households_reliant_on_means_tested_benefit",
"income_deprivation_2010_percent_of_people_aged_over_60_who_live_in_pension_credit_households",
"health_2011_census_bad_health_percent",
"car_or_van_availability_2011_census_no_cars_or_vans_in_household_percent")

# rename variables
msoaDataMerged <-
  left_join(msoaMap %>% dplyr::select(MSOA11CD,MSOA11NM,geometry),
            msoaData %>% dplyr::select(all_of(cols)),
            by = c("MSOA11CD" = "middle_super_output_area")) %>% 
  rename(all_population = "age_structure_2011_census_all_ages",
         white_pop = "ethnic_group_2011_census_white",
         asian_pop = "ethnic_group_2011_census_asian_asian_british",
         black_pop = "ethnic_group_2011_census_black_african_caribbean_black_british",
         otherEth_pop = "ethnic_group_2011_census_other_ethnic_group",
         white_pop_rate = "ethnic_group_2011_census_white_percent",
         asian_pop_rate = "ethnic_group_2011_census_asian_asian_british_percent",
         black_pop_rate = "ethnic_group_2011_census_black_african_caribbean_black_british_percent",
         otherEth_pop_rate = "ethnic_group_2011_census_other_ethnic_group_percent",
         unemp_rate = "economic_activity_2011_census_unemployment_rate",
         mean_income_rate = "household_income_estimates_2011_12_total_mean_annual_household_income",
         median_income_rate = "household_income_estimates_2011_12_total_median_annual_household_income",
         income_deprivation_rate = "income_deprivation_2010_percent_living_in_income_deprived_households_reliant_on_means_tested_benefit",
         income_old_deprivation_rate = "income_deprivation_2010_percent_of_people_aged_over_60_who_live_in_pension_credit_households",
         bad_health_rate = "health_2011_census_bad_health_percent",
         cars_rate = "car_or_van_availability_2011_census_no_cars_or_vans_in_household_percent")

```

Count all S&S for each ethnic group that fall within MSOAs.

```{r message=FALSE}
ss_count <- st_join(ss_london, msoaMap) %>% 
  group_by(MSOA11CD, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_ss = sum(c_across(Asian:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(ss_black = "Black",
                ss_white = "White",
                ss_asian = "Asian",
                ss_other = "Other",
                ss_na = "NA_eth") 
```

Count all violent crimes that fall within wards.

```{r message=FALSE}
crime_count <- st_join(crime_london, msoaMap) %>% 
  group_by(MSOA11CD) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry() %>% 
  na.omit(.) %>% 
  dplyr::rename(total_crime_count = "count")
```

Count successful S&S that leading to an arrest.

```{r message=FALSE, warning=FALSE}
arrest_count <- st_join(ss_london, msoaMap) %>% 
  filter(outcome=="Arrest") %>% 
  group_by(MSOA11CD, officer_defined_ethnicity) %>% 
  summarise(., arrest_count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = arrest_count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_arrest = sum(c_across(Asian:Other), na.rm = TRUE)) %>% 
  dplyr::rename(arrest_black = "Black",
                arrest_white = "White",
                arrest_asian = "Asian",
                arrest_other = "Other",
                arrest_na = "NA_eth") 
```

Join demographic data with crime data.

```{r}
merged <- left_join(msoaDataMerged, crime_count, by = c("MSOA11CD" = "MSOA11CD")) %>% 
  left_join(., ss_count, by = c("MSOA11CD" = "MSOA11CD")) %>% 
  left_join(., arrest_count, by = "MSOA11CD")
```

Calculate rates.

```{r message=FALSE}
all_data <- merged %>% 
  clean_names() %>%   
  # total crime rate 
  mutate(total_crime_rate = total_crime_count/
           all_population *100) %>% 
  # total crime proportion
  mutate(total_crime_prop = total_crime_count/
           sum(total_crime_count) *100) %>% 
  # total search rate
  mutate(total_search_rate = total_ss / 
           all_population *100) %>%
  # total arrest rate
  mutate(total_arrest_rate = total_arrest/
           all_population *100) %>% 
  # total hit rate 
  mutate(total_hit_rate = total_arrest/ 
           total_ss *100) %>% 
  # search rate for ethical groups 
  mutate(black_search_rate = ss_black/ black_pop *100) %>% 
  mutate(white_search_rate = ss_white/ white_pop *100) %>% 
  mutate(asian_search_rate = ss_asian/ asian_pop *100) %>% 
  mutate(other_search_rate = ss_other/ other_eth_pop *100) %>% 
  # hit rates for ethical groups 
  mutate(black_hit_rate = arrest_black/ ss_black *100) %>% 
  mutate(white_hit_rate = arrest_white/ ss_white *100) %>% 
  mutate(asian_hit_rate = arrest_asian/ ss_asian *100) %>% 
  mutate(other_hit_rate = arrest_other/ ss_other *100) %>% 
  # arrest rates for ethical groups
  mutate(black_arrest_rate = arrest_black/ black_pop *100) %>% 
  mutate(white_arrest_rate = arrest_white/ white_pop *100) %>% 
  mutate(asian_arrest_rate = arrest_asian/ asian_pop *100) %>% 
  mutate(other_arrest_rate = arrest_other/ other_eth_pop *100) %>% 
  mutate_if(is.numeric, ~round(., 2))

```

### Overall trend

Hit rate is, what proportion of searches, by race, were successful? If racial groups have different hit rates, it can imply that racial groups are being subjected to different standards.

```{r message=False, warning=False, results='asis'}
# count the number of searches by race
HR1 <- ss_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., searches=n()) %>% 
  st_drop_geometry()
  
# count the number of successful searches leading to arrest 
HR2 <- ss_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  filter(outcome=="Arrest") %>% 
  summarise(., arrests=n()) %>% 
  st_drop_geometry()

# calculate total population for each race based on 2011 census 
aP <- sum(as.numeric(all_data$asian_pop), na.rm = TRUE)
bP <- sum(as.numeric(all_data$black_pop), na.rm = TRUE)
wP <- sum(as.numeric(all_data$white_pop), na.rm = TRUE)
oP <- sum(as.numeric(all_data$other_eth_pop), na.rm = TRUE)

left_join(HR1, HR2) %>% 
  mutate(population = c(aP, bP, oP, wP, 0),
         search_prop = searches/sum(searches) *100,
         arrest_prop = arrests/sum(arrests) *100,
         population_prop = population/sum(population) *100,
         search_rate = searches/population *100,
         arrest_rate = arrests/population *100,
         hit_rate = arrests/searches *100) %>% 
  dplyr::rename(race = "officer_defined_ethnicity") %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  kable(.,)

```

### Hit rate

```{r}
summary(all_data$black_hit_rate)
sd(all_data$black_hit_rate, na.rm=TRUE)
summary(all_data$white_hit_rate)
summary(all_data$asian_hit_rate)
summary(all_data$other_hit_rate)
```

```{r}
all_data_plot <- all_data %>% 
  dplyr::select(black_search_rate,
                white_search_rate,
                asian_search_rate,
                other_search_rate,
                black_hit_rate,
                white_hit_rate,
                asian_hit_rate,
                other_hit_rate,
                black_arrest_rate,
                white_arrest_rate,
                asian_arrest_rate,
                other_arrest_rate) %>% 
  mutate(black_hit_compare = case_when(
    (black_hit_rate < 10.09) ~ "Very Low",
    (black_hit_rate >= 10.09 & black_hit_rate < 12.4) ~ "Low",
    (black_hit_rate >= 12.4 & black_hit_rate < 16.13) ~ "High", 
    (black_hit_rate >= 16.13 ~ "Very High")),
    white_hit_compare = case_when(
    (white_hit_rate < 10.14) ~ "Very Low",
    (white_hit_rate >= 10.14 & white_hit_rate < 12.6) ~ "Low",
    (white_hit_rate >= 12.6 & white_hit_rate < 16.56) ~ "High", 
    (white_hit_rate >= 16.56 ~ "Very High")),
    asian_hit_compare = case_when(
    (asian_hit_rate < 7.275) ~ "Very Low",
    (asian_hit_rate >= 7.275 & asian_hit_rate < 9.4) ~ "Low",
    (asian_hit_rate >= 9.4 & asian_hit_rate < 13.64) ~ "High", 
    (asian_hit_rate >= 13.64 ~ "Very High")),
    other_hit_compare = case_when(
    (other_hit_rate < 9.09) ~ "Very Low",
    (other_hit_rate >= 9.09 & other_hit_rate < 10.7) ~ "Low",
    (other_hit_rate >= 10.7 & other_hit_rate < 20) ~ "High", 
    (other_hit_rate >= 20 ~ "Very High")))
```

Plot

```{r}
p1 <- tm_shape(all_data_plot) + 
  tm_fill("black_hit_compare", 
          palette="viridis",
          title="Black Hit Rate Comparison")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(a)", position=c(0,0.8), size=1)

p2 <- tm_shape(all_data_plot) + 
  tm_fill("white_hit_compare", 
          palette="viridis",
          title="White Hit Rate Comparison")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(b)", position=c(0,0.8), size=1)

p3 <- tm_shape(all_data_plot) + 
  tm_fill("asian_hit_compare", 
          palette="viridis",
          title="Asian Hit Rate Comparison")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(c)", position=c(0,0.8), size=1)

p4 <- tm_shape(all_data_plot) + 
  tm_fill("other_hit_compare", 
          palette="viridis",
          title="Other Ethnic Group Hit Rate Comparison")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(d)", position=c(0,0.8), size=1)


p <- tmap_arrange(p1, p2, p3, p4)
p


```

```{r}
p1 <- tm_shape(all_data_plot) + 
  tm_fill("black_hit_rate", 
          style="jenks",
          palette="plasma",
          title="Black Hit Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(a)", position=c(0,0.8), size=1)

p2 <- tm_shape(all_data_plot) + 
  tm_fill("white_hit_rate", 
          style="jenks",
          palette="Set2",
          title="White Hit Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(b)", position=c(0,0.8), size=1)

p3 <- tm_shape(all_data_plot) + 
  tm_fill("asian_hit_rate", 
          style="jenks",
          palette="Set1",
          title="Asian Hit Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(c)", position=c(0,0.8), size=1)

p4 <- tm_shape(all_data_plot) + 
  tm_fill("other_hit_rate", 
          style="jenks",
          palette="-RdGy",
          title="Other Ethnic Group Hit Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(d)", position=c(0,0.8), size=1)


p <- tmap_arrange(p1, p2, p3, p4)
p


```

Standard Deviation classification

```{r}
p1 <- tm_shape(all_data_plot) + 
  tm_fill("black_hit_rate", 
          style="sd",
          palette="viridis",
          title="Black Hit Rate (Std Dev)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(a)", position=c(0,0.8), size=1)

p2 <- tm_shape(all_data_plot) + 
  tm_fill("white_hit_rate", 
          style="sd",
          palette="viridis",
          title="White Hit Rate (Std Dev)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(b)", position=c(0,0.8), size=1)

p3 <- tm_shape(all_data_plot) + 
  tm_fill("asian_hit_rate", 
          style="sd",
          palette="viridis",
          title="Asian Hit Rate (Std Dev)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(c)", position=c(0,0.8), size=1)

p4 <- tm_shape(all_data_plot) + 
  tm_fill("other_hit_rate", 
          style="sd",
          palette="viridis",
          title="Other Ethnic Group Hit Rate (Std Dev)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(d)", position=c(0,0.8), size=1)


p <- tmap_arrange(p1, p2, p3, p4)
p


```

### Search rate

```{r}
p1 <- tm_shape(all_data) + 
  tm_fill("total_search_rate", 
          style="jenks",
          palette="viridis",
          title="Total Search Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(a)", position=c(0,0.8), size=1)

p2 <- tm_shape(all_data) + 
  tm_fill("total_arrest_rate", 
          style="jenks",
          palette="viridis",
          title="Total Arrest Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(b)", position=c(0,0.8), size=1)

p3 <- tm_shape(all_data) + 
  tm_fill("total_hit_rate", 
          style="jenks",
          palette="viridis",
          title="total Hit Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(c)", position=c(0,0.8), size=1)

p4 <- tm_shape(all_data) + 
  tm_fill("total_crime_rate", 
          style="jenks",
          palette="viridis",
          title="total Crime Rate (Jenks)")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(d)", position=c(0,0.8), size=1)

p <- tmap_arrange(p1, p2, p3, p4)
p
```

### Search rate & Hit rate comparison

```{r message=FALSE, warning=FALSE}
# select columns we need but first change the msoa code name 
all_data <- all_data %>% 
  rename(msoa_code = "msoa11cd")

cols_s <- c("msoa_code",
          "black_search_rate", 
          "white_search_rate", 
          "asian_search_rate", 
          "other_search_rate")
cols_h <- c("black_hit_rate",
            "white_hit_rate",
            "asian_hit_rate",
            "other_hit_rate",
            "msoa_code")
cols_a <- c("black_arrest_rate",
            "white_arrest_rate",
            "asian_arrest_rate",
            "other_arrest_rate",
            "msoa_code")

search_data <- all_data[cols_s]
hit_data <- all_data[cols_h]
arrest_data <- all_data[cols_a]

# pivot longer 
search_df <- search_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("search_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_') 

hit_df <- hit_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("hit_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_')

# join df
main_data <- left_join(search_df, hit_df) %>% 
  rename(search_rate = "search",
         hit_rate = "hit") 
  #mutate_at(c(3,4), funs(c(scale(.))))

# convert NAs to zero
main_data[is.na(main_data)] <- 0
```

Compare search rate vs. hit rate for each race

```{r message=FALSE, warning=FALSE}
# We'll use this just to make our axes' limits nice and even
max_hit_rate <-
  main_data %>% 
  dplyr::select("hit_rate") %>% 
  max()

max_search_rate <-
  main_data %>% 
  dplyr::select("search_rate") %>% 
  max()

main_data %>% 
  ggplot(aes(
    x = search_rate,
    y = hit_rate
  )) +
  geom_point(size=0.5) +
  # This sets a diagonal reference line (line of equal search and hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("Search rate (%)", 
    limits = c(0, max_search_rate + 0.1)
  ) +
  scale_y_continuous("Hit rate (%)", 
    limits = c(0, max_hit_rate + 0.1)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  facet_wrap(. ~ race, nrow = 2)
```

Compare white hit rate with minorities' hit rate

```{r}
# Reshape table to show hit rates of minorities vs white 
main_data_white <- main_data %>% 
  dplyr::select(-search_rate) %>% 
  spread(race, hit_rate, fill=0) %>% 
  rename(white_hit_rate = white) %>% 
  gather(minority_race, minority_hit_rate, c(black, asian, other)) %>%
  arrange(msoa_code)
```

Plot

```{r}
max_hit_rate <-
  main_data_white %>% 
  dplyr::select(ends_with("hit_rate")) %>% 
  max()

main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point() +
  # This sets a diagonal reference line (line of equal hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  # This allows us to compare black v. white and Hispanic v. white side by
  # side, in panels
  facet_grid(. ~ minority_race)
```

Add the number of searches to the hit rate df

```{r}
search_count <- all_data %>% 
  dplyr::select("msoa_code", "ss_asian", "ss_black", "ss_white", "ss_other") %>% 
  st_drop_geometry(.) %>% 
  pivot_longer(cols = ! (msoa_code | ss_white),
               names_to = "minority_race",
               names_prefix = "ss_",
               values_to = "minority_search_counts") %>% 
  rename(white_search_counts = ss_white) %>% 
  arrange(msoa_code)

# convert NAs to zero
search_count[is.na(search_count)] <- 0

# join df
main_data_white <- left_join(main_data_white, 
                             search_count, 
                             by = c("msoa_code", "minority_race")) %>% 
  mutate(all_search_count = minority_search_counts + white_search_counts)
```

plot

```{r}
hit_rate_plot <- main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point(aes(color=all_search_count, size=all_search_count), pch = 21) +
  scale_fill_viridis_c(option = "viridis",
                       aesthetics = c("colour", "fill")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  coord_fixed() +
  facet_grid(. ~ minority_race)

ggsave("output/hit_rate_comparion_plot.png")

```

### 0 Hit rate

```{r}
# Examine the zeros in asian hit rate 
test <- all_data %>% 
  dplyr::select(msoa_code, black_hit_rate, white_hit_rate, asian_hit_rate, other_hit_rate) %>% 
  st_drop_geometry()

# convert NAs to zero
test[is.na(test)] <- 0

test_0 <- test %>% 
  filter(asian_hit_rate==0) %>% 
  dplyr::select(msoa_code) %>% 
  left_join(.,
            all_data %>% dplyr::select(msoa_code, msoa_name,
                                       asian_pop_rate,
                                       asian_search_rate, 
                                       arrest_asian),
            by="msoa_code")

# add borough name to test_0 df
test_0$boro_name = substr(test_0$msoa_name,1,nchar(test_0$msoa_name)-4)

# examine asian's population in those areas
summary(test_0$asian_pop_rate)
# compared with all areas's asian population avg.
summary(all_data$asian_pop_rate)

# compare list0's asian ssrate
summary(test_0$asian_search_rate)
# with the national average
summary(all_data$asian_search_rate)
```

Thus, counting all London's MSOA's, on average a MSOA is made of 18% of Asian people, for those MSOA having empty Asian hit rate but non-empty White hit rate, it might because of the population.

```{r warning=F, message=F, results='asis'}
test_0 <- test_0 %>% 
  mutate(asian_pop_compare = case_when(
    (asian_pop_rate < 18) ~ "Below London Average",
    TRUE ~ "Above London Average"),
    asian_ss_compare = case_when(
      (asian_search_rate < 5.93) ~ "Below London Average",
      TRUE ~ "Above London Average"
    )) 

test_0 %>% 
  group_by(asian_pop_compare)%>%
  summarise(., count=n()) %>% 
  kable()

test_0 %>% 
  group_by(asian_ss_compare)%>%
  summarise(., count=n()) %>% 
  kable()

test_0 %>% 
  filter((asian_pop_compare=="Below London Average") & (asian_ss_compare=="Above London Average")) %>% 
  kable()
```

```{r}
asia0 <- test_0 %>% 
  filter((asian_pop_compare=="Below London Average") & (asian_ss_compare=="Above London Average")) %>% 
  filter(msoa_code != "E02000077") %>% 
  st_as_sf() 

tm_shape(boroMap) +
  tm_polygons() +
tm_shape(asia0) + 
  tm_polygons("msoa_name", palette="red")
```

## Spatial autocorrelation

Create a subset data of all rates for later parts.

```{r}
all_rates <- all_data %>% 
  dplyr::select(ends_with("rate") | msoa_code)
```

Create a neighborhoods list and spatial weight matrix.

```{r message=FALSE, warning=FALSE}
library(spdep)
# calculate centroids 
coordsM<- all_rates%>%
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
Lward_nb <- all_rates %>%
  poly2nb(., queen=T, snap=0.00001)  
plot(Lward_nb, st_geometry(coordsM), col="red")
# neighborhood summary
summary(Lward_nb)

# create a spatial weight LIST for moran's I 
Lward_lw <- Lward_nb %>%
  nb2listw(., style="W")
```

### Global Moran's I

variables to test for spatial autocorrelation:

```{r}
# convert NA to zeros
all_rates[is.na(all_rates)] <- 0
colnames(all_rates)
```

Moran's I test tells us whether we have clustered values (close to 1) or dispersed values (close to -1).

```{r}
# variables used for the test
variable_names <- c( "black_search_rate",
                     "white_search_rate",
                     "asian_search_rate",
                     "black_hit_rate",
                     "white_hit_rate",
                     "asian_hit_rate",
                     "black_arrest_rate",
                     "white_arrest_rate",
                     "asian_arrest_rate",
                     "total_crime_rate")

# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    moran.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar))
  datalist[[aVar]] <- dat
}
datalist

```

### Geary's C

It tells us whether similar values or dissimilar values are clusering. Geary's C falls between 0 and 2; 1 means no spatial autocorrelation, \<1 - positive spatial autocorrelation or similar values clustering, \>1 - negative spatial autocorreation or dissimilar values clustering.

```{r}
# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    geary.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar))
  datalist[[aVar]] <- dat
}
datalist
```

### Local Moran's I

Calculate local Moran's I for several important variables for inspecting racial discrimination of search rate and hit rate.

```{r}
moranI_black_search <- all_rates %>%
  pull(black_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_search <- all_rates %>% 
  pull(white_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_search <- all_rates %>% 
  pull(asian_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_black_hit <- all_rates %>% 
  pull(black_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_hit <- all_rates %>% 
  pull(white_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_hit <- all_rates %>% 
  pull(asian_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_black_arrest <- all_rates %>% 
  pull(black_arrest_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_arrest <- all_rates %>% 
  pull(white_arrest_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_arrest <- all_rates %>% 
  pull(asian_arrest_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_total_crime <- all_rates %>% 
  pull(total_crime_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()
```

copy the Moran's I score and the z-score standard deviation back to all_rates dataframe

```{r}
all_rates <- all_rates %>% 
  mutate(black_search_Iz = as.numeric(moranI_black_search$Z.Ii)) %>% 
  mutate(white_search_Iz = as.numeric(moranI_white_search$Z.Ii)) %>% 
  mutate(asian_search_Iz = as.numeric(moranI_asian_search$Z.Ii)) %>% 
  mutate(black_hit_Iz = as.numeric(moranI_black_hit$Z.Ii)) %>% 
  mutate(white_hit_Iz = as.numeric(moranI_white_hit$Z.Ii)) %>% 
  mutate(asian_hit_Iz = as.numeric(moranI_asian_hit$Z.Ii)) %>% 
  mutate(black_arrest_Iz = as.numeric(moranI_black_arrest$Z.Ii)) %>% 
  mutate(white_arrest_Iz = as.numeric(moranI_white_arrest$Z.Ii)) %>% 
  mutate(asian_arrest_Iz = as.numeric(moranI_asian_arrest$Z.Ii)) %>% 
  mutate(total_crime_Iz = as.numeric(moranI_total_crime$Z.Ii)) 
```

We'll set the breaks manually based on the rule that data points \>2.58 or \<-2.58 standard deviations away from the mean are significant at the 99% level (\<1% chance that autocorrelation not present); \>1.96 - \<2.58 or \<-1.96 to \>-2.58 standard deviations are significant at the 95% level (\<5% change that autocorrelation not present). \>1.65 = 90% etc.

### LISA

Local Indicators of Spatial association (LISA), refer to [https://maczokni.github.io/crime_mapping_textbook/global-and-local-spatial-autocorrelation.html\#putting-neighbourness-in-our-analysis---constructing-a-spatial-weight-matrix](https://maczokni.github.io/crime_mapping_textbook/global-and-local-spatial-autocorrelation.html#putting-neighbourness-in-our-analysis—constructing-a-spatial-weight-matrix) as written guide.

```{r}
moran.plot(all_rates$black_search_rate, Lward_lw)
```

Scale the variables we want

```{r}
all_rates$s_black_search_rate <- scale(all_rates$black_search_rate) %>% as.vector()
all_rates$s_white_search_rate <- scale(all_rates$white_search_rate) %>% as.vector()
all_rates$s_asian_search_rate <- scale(all_rates$asian_search_rate) %>% as.vector()
# arrest rate
all_rates$s_black_arrest_rate <- scale(all_rates$black_arrest_rate) %>% as.vector()
all_rates$s_white_arrest_rate <- scale(all_rates$white_arrest_rate) %>% as.vector()
all_rates$s_asian_arrest_rate <- scale(all_rates$asian_arrest_rate) %>% as.vector()
# crime rate
all_rates$s_total_crime_rate <- scale(all_rates$total_crime_rate) %>% as.vector()
```

Now we also want to account for the spatial dependence of our values. So what do we mean by this spatial dependence? When a value observed in one location depends on the values observed at neighbouring locations, there is a **spatial dependence**. And spatial data may show spatial dependence in the variables and error terms. *Why should spatial dependence occur?* There are two reasons commonly given. First, data collection of observations associated with spatial units may reflect measurement error. This happens when the boundaries for which information is collected do not accurately reflect the nature of the underlying process generating the sample data. A second reason for spatial dependence is that the spatial dimension of a social or economic characteristic may be an important aspect of the phenomenon. For example, based on the premise that location and distance are important forces at work, regional science theory relies on notions of spatial interaction and diffusion effects, hierarchies of place and spatial spillovers.

There are two types of dependence, spatial error and spatial lag. Here we focus on spatial lag.

**Spatial lag** is when the dependent variable y in place i is affected by the independent variables in both place i and j. This will be important to keep in mind when considering spatial regression. With spatial lag in ordinary least square regression, the assumption of uncorrelated error terms is violated, because near things will have associated error terms. Similarly, the assumption of independent observations is also violated, as the observations are influenced by the other observations near them. As a result, the estimates are biased and inefficient. Spatial lag is suggestive of a possible diffusion process -- events in one place predict an increased likelihood of similar events in neighboring places.

So, create a variable accounting for spatial lag.

```{r}
#create a spatial lag variable and save it to a new column
all_rates$lag_s_black_search <- lag.listw(Lward_lw, all_rates$s_black_search_rate)
all_rates$lag_s_white_search <- lag.listw(Lward_lw, all_rates$s_white_search_rate)
all_rates$lag_s_asian_search <- lag.listw(Lward_lw, all_rates$s_asian_search_rate)
# arrest 
all_rates$lag_s_black_arrest <- lag.listw(Lward_lw, all_rates$s_black_arrest_rate)
all_rates$lag_s_white_arrest <- lag.listw(Lward_lw, all_rates$s_white_arrest_rate)
all_rates$lag_s_asian_arrest <- lag.listw(Lward_lw, all_rates$s_asian_arrest_rate)
all_rates$lag_s_total_crime <- lag.listw(Lward_lw, all_rates$s_total_crime_rate)
```

```{r}
# check
summary(all_rates$s_black_search_rate)
summary(all_rates$lag_s_black_search)
```

We are now going to create a new variable to identify the quadrant in which each observation falls within the Moran Scatter plot, so that we can tell apart the high-high, low-low, high-low, and low-high areas. We will only identify those that are significant according to the p value that was provided by the local moran function.

Essentially all we'll be doing, is assigning a variable values based on where in the plot it is. So for example, if it's in the upper right, it is high-high, and has values larger than 0 for both the burglary and the spatial lag values. If it's in the upper left, it's low-high, and has a value larger than 0 for the spatial lag value, but lower than 0 on the burglary value.

```{r}
all_rates <- all_rates %>% 
  mutate(quad_sig_black_search = ifelse(all_rates$s_black_search_rate > 0 & 
                              all_rates$lag_s_black_search > 0 & 
                              moranI_black_search[,5] <= 0.05, 
                     "high-high",
                     ifelse(all_rates$s_black_search_rate <= 0 & 
                              all_rates$lag_s_black_search <= 0 & 
                              moranI_black_search[,5] <= 0.05, 
                     "low-low", 
                     ifelse(all_rates$s_black_search_rate > 0 & 
                              all_rates$lag_s_black_search <= 0 & 
                              moranI_black_search[,5] <= 0.05, 
                     "high-low",
                     ifelse(all_rates$s_black_search_rate <= 0 & 
                              all_rates$lag_s_black_search > 0 & 
                              moranI_black_search[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))),
         quad_sig_white_search = ifelse(all_rates$s_white_search_rate > 0 & 
                              all_rates$lag_s_white_search > 0 & 
                              moranI_white_search[,5] <= 0.05, 
                     "high-high",
                     ifelse(all_rates$s_white_search_rate <= 0 & 
                              all_rates$lag_s_white_search <= 0 & 
                              moranI_white_search[,5] <= 0.05, 
                     "low-low", 
                     ifelse(all_rates$s_white_search_rate > 0 & 
                              all_rates$lag_s_white_search <= 0 & 
                              moranI_white_search[,5] <= 0.05, 
                     "high-low",
                     ifelse(all_rates$s_white_search_rate <= 0 & 
                              all_rates$lag_s_white_search > 0 & 
                              moranI_white_search[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))))
```

```{r}
all_rates <- all_rates %>% 
  mutate(quad_sig_asian_search = ifelse(all_rates$s_asian_search_rate > 0 & 
                              all_rates$lag_s_asian_search > 0 & 
                              moranI_asian_search[,5] <= 0.05, 
                     "high-high",
                     ifelse(all_rates$s_asian_search_rate <= 0 & 
                              all_rates$lag_s_asian_search <= 0 & 
                              moranI_asian_search[,5] <= 0.05, 
                     "low-low", 
                     ifelse(all_rates$s_asian_search_rate > 0 & 
                              all_rates$lag_s_asian_search <= 0 & 
                              moranI_asian_search[,5] <= 0.05, 
                     "high-low",
                     ifelse(all_rates$s_asian_search_rate <= 0 & 
                              all_rates$lag_s_asian_search > 0 & 
                              moranI_asian_search[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))),
           quad_sig_black_arrest = ifelse(all_rates$s_black_arrest_rate > 0 & 
                              all_rates$lag_s_black_arrest > 0 & 
                              moranI_black_arrest[,5] <= 0.05, 
                     "high-high",
                     ifelse(all_rates$s_black_arrest_rate <= 0 & 
                              all_rates$lag_s_black_arrest <= 0 & 
                              moranI_black_arrest[,5] <= 0.05, 
                     "low-low", 
                     ifelse(all_rates$s_black_arrest_rate > 0 & 
                              all_rates$lag_s_black_arrest <= 0 & 
                              moranI_black_arrest[,5] <= 0.05, 
                     "high-low",
                     ifelse(all_rates$s_black_arrest_rate <= 0 & 
                              all_rates$lag_s_black_arrest > 0 & 
                              moranI_black_arrest[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))))
```

```{r}
all_rates <- all_rates %>% 
  mutate(quad_sig_white_arrest = ifelse(all_rates$s_white_arrest_rate > 0 & 
                              all_rates$lag_s_white_arrest > 0 & 
                              moranI_white_arrest[,5] <= 0.05, 
                     "high-high",
                     ifelse(all_rates$s_white_arrest_rate <= 0 & 
                              all_rates$lag_s_white_arrest <= 0 & 
                              moranI_white_arrest[,5] <= 0.05, 
                     "low-low", 
                     ifelse(all_rates$s_white_arrest_rate > 0 & 
                              all_rates$lag_s_white_arrest <= 0 & 
                              moranI_white_arrest[,5] <= 0.05, 
                     "high-low",
                     ifelse(all_rates$s_white_arrest_rate <= 0 & 
                              all_rates$lag_s_white_arrest > 0 & 
                              moranI_white_arrest[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))),
             quad_sig_asian_arrest = ifelse(all_rates$s_asian_arrest_rate > 0 & 
                              all_rates$lag_s_asian_arrest > 0 & 
                              moranI_asian_arrest[,5] <= 0.05, 
                     "high-high",
                     ifelse(all_rates$s_asian_arrest_rate <= 0 & 
                              all_rates$lag_s_asian_arrest <= 0 & 
                              moranI_asian_arrest[,5] <= 0.05, 
                     "low-low", 
                     ifelse(all_rates$s_asian_arrest_rate > 0 & 
                              all_rates$lag_s_asian_arrest <= 0 & 
                              moranI_asian_arrest[,5] <= 0.05, 
                     "high-low",
                     ifelse(all_rates$s_asian_arrest_rate <= 0 & 
                              all_rates$lag_s_asian_arrest > 0 & 
                              moranI_asian_arrest[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))),
              quad_sig_total_crime = ifelse(all_rates$s_total_crime_rate > 0 & 
                              all_rates$lag_s_total_crime > 0 & 
                              moranI_total_crime[,5] <= 0.05, 
                     "high-high",
                     ifelse(all_rates$s_total_crime_rate <= 0 & 
                              all_rates$lag_s_total_crime <= 0 & 
                              moranI_total_crime[,5] <= 0.05, 
                     "low-low", 
                     ifelse(all_rates$s_total_crime_rate > 0 & 
                              all_rates$lag_s_total_crime <= 0 & 
                              moranI_total_crime[,5] <= 0.05, 
                     "high-low",
                     ifelse(all_rates$s_total_crime_rate <= 0 & 
                              all_rates$lag_s_total_crime > 0 & 
                              moranI_total_crime[,5] <= 0.05,
                     "low-high", 
                     "non-significant")))))
```

```{r}
table(all_rates$quad_sig_black_search)
table(all_rates$quad_sig_white_search)
table(all_rates$quad_sig_asian_search)
# arrest
table(all_rates$quad_sig_black_arrest)
table(all_rates$quad_sig_white_arrest)
table(all_rates$quad_sig_asian_arrest)
# crime
table(all_rates$quad_sig_total_crime)
```

### Maps

```{r}
MoranColours<- rev(brewer.pal(3, "BrBG"))

t1 <- tm_shape(all_rates) + 
  tm_fill("quad_sig_black_search",
        palette= MoranColours,
        midpoint=NA,
        title="LISA, Black Searches") +
  tm_borders(NA, alpha = 0.3) +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,            
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(a)", position=c(0,0.8), size=1)


t2 <- tm_shape(all_rates) + 
  tm_polygons("quad_sig_black_arrest",
        palette= MoranColours,
        midpoint=NA,
        title="LISA, Black Arrests") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.8), size=1)

t3 <- tm_shape(all_rates) + 
  tm_polygons("quad_sig_white_search",
        palette= MoranColours,
        midpoint=NA,
        title="LISA, White Searches") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.8), size=1)

t4 <- tm_shape(all_rates) + 
  tm_polygons("quad_sig_white_arrest",
        palette= MoranColours,
        midpoint=NA,
        title="LISA, White Arrests") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.8), size=1)

t5 <- tm_shape(all_rates) + 
  tm_polygons("quad_sig_asian_search",
        palette= MoranColours,
        midpoint=NA,
        title="LISA, Asian Searches") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.8), size=1)

t6 <- tm_shape(all_rates) + 
  tm_polygons("quad_sig_asian_arrest",
        palette= MoranColours,
        midpoint=NA,
        title="LISA, Asian Arrests") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.8), size=1)

t7 <- tm_shape(all_rates) + 
  tm_polygons("quad_sig_total_crime",
        palette= MoranColours,
        midpoint=NA,
        title="LISA, Total Crimes") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(g)", position=c(0,0.8), size=1)



t=tmap_arrange(t1, t3, t5, t2, t4, t6, t7,
               nrow = 3, ncol = 3)
t
```

So how do we interpret these results? Well keep in mind:

-   The LISA value for each location is determined from its individual contribution to the global Moran's I calculation.

-   Whether or not this value is statistically significant is assessed by comparing the actual value to the value calculated for the same location by randomly reassigning the data among all the areal units and recalculating the values each time (the Monte Carlo simulation approach discussed earlier).

So essentially this map now tells us that there was statistically significant moderate clustering in burglaries in Manchester. When reporting your results, report at least the Moran's I test value and the p value. So, for this test, you should report Moran's I = 0.32, p \< .001. Including the LISA cluster map is also a great way of showing how the attribute is actually clustering.

```{r}
breaks1<-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)
MoranColours<- rev(brewer.pal(8, "RdGy"))


t1 <- tm_shape(all_rates) + 
  tm_polygons("black_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Black Search Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,            
            legend.outside = FALSE,
            legend.position = c(.8,0)) +
  tm_credits("(a)", position=c(0,0.8), size=1)


t2 <- tm_shape(all_rates) + 
  tm_polygons("white_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, White Search Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.8), size=1)

t3 <- tm_shape(all_rates) + 
  tm_polygons("asian_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Asian Search Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.8), size=1)

t4 <- tm_shape(all_rates) + 
  tm_polygons("black_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Black Hit Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.8), size=1)

t5 <- tm_shape(all_rates) + 
  tm_polygons("white_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, White Hit Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.8), size=1)

t6 <- tm_shape(all_rates) + 
  tm_polygons("asian_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Asian Hit Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.8), size=1)

t7 <- tm_shape(all_rates) + 
  tm_polygons("black_arrest_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Black Arrest Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(g)", position=c(0,0.8), size=1)


t8 <- tm_shape(all_rates) + 
  tm_polygons("white_arrest_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, White Arrest Rate ") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(h)", position=c(0,0.8), size=1)

t9 <- tm_shape(all_rates) + 
  tm_polygons("asian_arrest_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I, Asian Arrest Rate") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,            
            legend.outside.size = 1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(i)", position=c(0,0.8), size=1)

t=tmap_arrange(t1, t2, t3, t7, t8, t9, t4, t5, t6,
               nrow = 3,
               ncol = 3)

t
```

## Regression Models

The local moran's I test suggests different ethic groups have different hot-spots of frequently searched places and arrested places. Therefore, it is reasonable to investigate each of the ethnic group in terms of their search rate and arrest rate more thoroughly.

```{r}
tm_shape(all_rates) + 
  tm_fill("mean_income_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="mean_income_rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))

tm_shape(all_rates) + 
  tm_fill("median_income_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="median_income_rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))

tm_shape(all_rates) + 
  tm_fill("unemp_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="unemployment rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))

tm_shape(all_rates) + 
  tm_fill("income_deprivation_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="income_deprivation_rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))

tm_shape(all_rates) + 
  tm_fill("income_old_deprivation_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="income_old_deprivation_rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))

tm_shape(all_rates) + 
  tm_fill("bad_health_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="bad_health_rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))

tm_shape(all_rates) + 
  tm_fill("cars_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="cars_rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))

tm_shape(all_rates) + 
  tm_fill("total_crime_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="total_crime_rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))
```

### Research Hypothesis

-   Is there a relationship between S&S (s60) search rate and arrest rate?

    -   Is increasing S&S (s60) potentially lead to more/less arrests across wards (i.e. are arrests distributed randomly or related to searches) ?

    -   What about for different minority groups, will the outcome be different?

-   What are the factors that might lead to variation in arrest rate across the city?

    -   Is there a relationship between arrest rate and violent crime rate? How about factors such as lone parent unemployment rate, economic inactivity rate, deprivation rate?

-   Does the model explains without local geographical variation?

```{r}
colnames(all_data)
```

### Arrests

#### Linear regression

Assumption 1: there is a linear relationship between my variables --

-- yes, the linear pattern is very obvious.

```{r message=FALSE, warning=FALSE}
library(broom)
library(ggpubr)
library(car)

regression_data <- all_data %>% 
  dplyr::select(msoa_code,
                total_search_rate,
                total_arrest_rate,
                black_search_rate,
                white_search_rate,
                asian_search_rate,
                black_arrest_rate,
                white_arrest_rate,
                asian_arrest_rate,
                total_ss,
                total_arrest,
                all_population) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  st_drop_geometry() 

# convert NAs to zero
regression_data[is.na(regression_data)] <- 0

# check variables' histogram 
ggplot(regression_data, aes(x=total_search_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=total_arrest_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = (total_search_rate), 
      y = (total_arrest_rate),
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

```

Assumption 2: the residuals in my model should be normally distributed

-- yes!

```{r message=FALSE, warning=FALSE}
# compute linear regression model
model_lr <- regression_data %>%
  lm(total_arrest_rate ~ total_search_rate, data=.)

# summary stats
# tidy(model1)
# glance(model1)
summary(model_lr)

# append residuals to regression_data 
regression_data <- model_lr %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_lr)
d <- deviance(model_lr)
par(mfrow=c(2,2))
plot(regression_data$total_search_rate, rsid)
boxplot(rsid~total_search_rate,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

```

The predicted function is:

y = 0.069 + 0.109 ss60 (R 0.935) meaning for 1 increase in stop and search for every thousand of population, the model suggests the arrest count is expected to increase 0.0424. In order words, on average, for every 1k stop and search in London, we are expected to have 42 arrests.

Assumption 3: errors/residuals in the model exhibit constant / homogeneous variance

-- The first and third plot shows some clustering pattern and the red line is not straight suggesting some hetroscedasticity, perhaps because there are many wards with zero arrest rate or search rate.

```{r}
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_lr)
```

Assumption 4: independent of errors, look for spatial autocorrelation

```{r message=F, warning=F}
# transform regression data to sf object
regression_data <- regression_data %>% 
  left_join(.,msoaMap %>% dplyr::select(MSOA11CD,geometry),
            by = c("msoa_code" = "MSOA11CD"))
regression_data <- data.frame(regression_data) %>% 
  st_as_sf()

# calculate centroids 
coordsM<- regression_data %>% 
  st_centroid()%>%
  st_geometry()

# create a neighborhoods list
Lward_nb <- regression_data %>%
  poly2nb(., queen=T, snap=0.00001)  

# create a spatial weight object
Lward_lw <- Lward_nb %>%
  nb2listw(., style="C")

regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)

```

Moderate spatial autocorrelation.

y = 0.069 + 0.109 ss60 (R 0.935)

Plot equation back to the scatter plot

```{r}
Lfun <- function(xvar) {
 (0.069 + 0.109*xvar)
}

# plot
ggplot(regression_data,
       aes(y=total_arrest_rate,
           x=total_search_rate)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = 0.069 + 0.109*x \n R-sqr = 0.935", vjust=3, hjust=1, col="red")
```

#### Poisson regression

Assumptions (<https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html>)

1.  **Poisson Response** The response variable is a count per unit of time or space, described by a Poisson distribution.

2.  **Independence** The observations must be independent of one another.

3.  **Mean=Variance** By definition, the mean of a Poisson random variable must be equal to its variance.

4.  **Linearity** The log of the mean rate, log(λ), must be a linear function of x.

First checking the histogram and scatter plot of the original data

```{r}
ggplot(regression_data, aes(x=total_ss))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=total_arrest))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

qplot(x = total_ss, 
      y = total_arrest,
      data = regression_data) +
stat_smooth(method="glm", se=FALSE, size=1) + 
  geom_jitter()
```

Both histogram reveal the pattern often found with distributions of counts of rare events. Many wards have few or none ss60 and arrests and a few wards have a large number of ss60 or arrests making for a distribution that appears to be far from normal. Therefore, Poisson regression should be used to model our data.

Next, checking mean and variance, they should be equal if they follow Poisson distribution.

```{r}
mean(regression_data$total_arrest)
var(regression_data$total_arrest)
```

The variance is slightly larger than the mean so should not be a big problem.

While a Poisson regression model is a good first choice because the responses are counts per period of time (from April 2019 to April 2021), it is important to note that the counts are not directly comparable because they come from different size wards. This issue sometimes is referred to as the need to account for *sampling effort*; in other words, we expect wards with more population to have more reports of arrests since there are more people who could be affected. We can take the differences in ward population into account by including an **offset** in our model.

```{r}
library(stats)
model_pr <- glm(total_arrest ~ total_ss, 
              family = "poisson", 
              offset = log(all_population),
              data = regression_data)
summary(model_pr)
```

The standard Poisson regression model gives us,

log(λ) = -2.89 + 0.008 ss60,

by exponentiating the coefficient of ss60 we obtain the multiplicative factor by which the mean count changes. In this case, the mean number of S&S (s60) arrests changes by a factor of e\^0.008=1.01 or increases by 0.01 % (since 1.01-1=0.01) with each additional stop and search for violating section 60. 1.01 is referred to as a **rate ratio** or **relative risk**.

We can use standard error to construct a confidence interval for β1. A 95% CI provides a range of plausible values for the ss60 coefficient and can be constructed by using "sandwich" library based on work done by Cameron and Trivedi (2009). <https://stats.idre.ucla.edu/r/dae/poisson-regression/>

```{r}
library(sandwich)
cov.mdl2 <- vcovHC(model_pr, type="HC0")
std.err <- sqrt(diag(cov.mdl2))
r.est <- cbind(Estimate= coef(model_pr), "Robust SE" = std.err,
"Pr(>|z|)" = 2 * pnorm(abs(coef(model_pr)/std.err), lower.tail=FALSE),
LL = coef(model_pr) - 1.96 * std.err,
UL = coef(model_pr) + 1.96 * std.err)
r.est
```

Exponentiating the endpoints yields a confidence interval for the relative risk; i.e., the percent change in arrest counts for each additional percent increase in ss60.

( e^0.006513633^ , e^0.009542442^ ) = (1.00653 , 1.00959) suggesting that we are 95% confident that the mean number in the arrest increase between 0.65% and 0.96% for each additional stop and search for section 60. The interval does not include 1 so we can conclude stop and search is significantly associated with arrests.

Deviance is a way in which to measure how the observed data deviates from the model predictions, similar to sum of squared errors in linear regression. Because we want models that minimize deviance, we calculate the drop-in-deviance when adding age to the model with no covariates (the null model).

The deviances for the null model and the model with age can be found in the model output. A residual deviance for the model with ss60 is reported as 1143.1 with 623 df. The output also includes the deviance and degrees of freedom for the null model (1879.7 with 624 df). The drop-in-deviance is 736.6 (1879.7-1143.1) with a difference of only 1 df, so that the addition of one extra term (ss60) reduced unexplained variability by 736.6.

If the null model were true, we would expect the drop-in-deviance to follow a χ2 distribution with 1 df. Therefore, the p-value for comparing the null model to the model with ss60 is found by determining the probability that the value for a χ2 random variable with one degree of freedom exceeds 736.6, which is essentially 0. Once again, we can conclude that we have statistically significant evidence (χ2 (df=1)=736.6, p\<.001) that average count of arrest increases as stop and search increases.

```{r}
# model0 is the null/reduced model
model0 <- glm(total_arrest ~ 1, family = poisson, offset = log(all_population), 
              data = regression_data)
anova(model0, model_pr, test = "Chisq")
```

Based on the above, we can see that the p-value \< 0.05, so we reject the null hypothesis that the regression coefficients are all zero. Hence our model is statistically significant.

**Drop-in-deviance test to compare models**

-   Compute the deviance for each model, then calculate: drop-in-deviance = residual deviance for reduced model -- residual deviance for the larger model.

-   When the reduced model is true, the drop-in-deviance ∼χd2 where d= the difference in the degrees of freedom associated with the two models (that is, the difference in the number of terms/coefficients).

-   A large drop-in-deviance favors the larger model.

```{r}
rsid <- residuals(model_pr)
d <- deviance(model_pr)
par(mfrow=c(2,2))
plot(regression_data$total_ss, rsid)
boxplot(rsid~total_ss,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)
```

We saw residuals are not normally distributed, probably because we have many observations that have certain amount of searches but no arrests, leading to a bad model prediction.

**Goodness of fit:**

In order to assess goodness of fit, we will look at the deviance residuals, and apply a χdf2 test based on our n-p-1 degrees of freedom. In this case, our p-value is zero, and so we fail to reject the null hypothesis that the model is not a good fit.

```{r}
c(deviance(model_pr), 1-pchisq(model_pr$deviance, model_pr$df.residual))  # GOF test
```

The model residual deviance can be used to assess the degree to which the predicted values differ from the observed. When a model is true, we can expect the residual deviance to be distributed as a χ2 random variable with degrees of freedom equal to the model's residual degrees of freedom. Our model has a residual deviance of 1143.1 with 623 df. The probability of observing a deviance this large if the model fits is esentially 0, saying that there is significant evidence of lack-of-fit.

There are several reasons why **lack-of-fit** may be observed. (1) We may be missing important covariates or interactions; a more comprehensive data set may be needed. (2) There may be extreme observations that may cause the deviance to be larger than expected; indeed, there are very few places with very high S&S counts and the majority of places with very little S&S. (3) Lastly, there may be a problem with the Poisson model. In particular, the Poisson model has only a single parameter, λ, for each combination of the levels of the predictors which must describe both the mean and the variance. This limitation can become manifest when the variance appears to be larger than the corresponding means. In that case, the response is more variable than the Poisson model would imply, and the response is considered to be **overdispersed**.

#### Black

```{r warning=FALSE, message=FALSE}
###### assumption1 #####
ggplot(regression_data, aes(x=black_search_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 10) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=black_arrest_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = black_search_rate, 
      y = black_arrest_rate,
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

###### assumption2 #####
# compute linear regression model
model_b <- regression_data %>% 
  lm(black_arrest_rate ~ black_search_rate, data=.)

# summary stats
summary(model_b)

# append residuals data to regression_data 
regression_data <- model_b %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_b)
d <- deviance(model_b)
par(mfrow=c(2,2))
plot(regression_data$black_search_rate, rsid)
boxplot(rsid~black_search_rate,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

###### assumption3 #####
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_b)


###### assumption4 #####
regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)
```

y = 0.258 +0.115 black ss

```{r}
library(ggplot2)
Lfun <- function(xvar) {
 (0.258 + 0.115*xvar)
}

# plot
ggplot(regression_data,
       aes(y=black_arrest_rate,x=black_search_rate)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = 0.258 + 0.115*x \n R-sqr = 0.928", vjust=3, hjust=1, col="red")
```

#### White

```{r warning=FALSE, message=FALSE}
###### assumption1 #####
ggplot(regression_data, aes(x=white_search_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=white_arrest_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = white_search_rate, 
      y = white_arrest_rate,
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

###### assumption2 #####
# compute linear regression model
model_w <- regression_data %>% 
  lm(white_arrest_rate ~ white_search_rate, data=.)

# summary stats
summary(model_w)

# append residuals data to regression_data 
regression_data <- model_w %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_w)
d <- deviance(model_w)
par(mfrow=c(2,2))
plot(regression_data$white_search_rate, rsid)
boxplot(rsid~white_search_rate,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

###### assumption3 #####
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_w)


###### assumption4 #####
regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)
```

y = 0.106 + 0.102 white ss

```{r}
library(ggplot2)
Lfun <- function(xvar) {
 (0.106 + 0.102*xvar)
}

# plot
ggplot(regression_data,
       aes(y=white_arrest_rate,x=white_search_rate)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = 0.106 + 0.102*x \n R-sqr = 0.915",vjust=3, hjust=1, col="red")
```

#### Asian

```{r warning=FALSE, message=FALSE}
###### assumption1 #####
ggplot(regression_data, aes(x=asian_search_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=asian_arrest_rate))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = asian_search_rate, 
      y = asian_arrest_rate,
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

###### assumption2 #####
# compute linear regression model
model_a <- regression_data %>% 
  lm(asian_arrest_rate ~ asian_search_rate, data=.)

# summary stats
summary(model_a)

# append residuals data to regression_data 
regression_data <- model_a %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

rsid <- residuals(model_a)
d <- deviance(model_a)
par(mfrow=c(2,2))
plot(regression_data$asian_search_rate, rsid)
boxplot(rsid~asian_search_rate,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)

###### assumption3 #####
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model_a)


###### assumption4 #####
regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)
```

y = 0.0126 + 0.0968 asian ss

```{r}
library(ggplot2)
Lfun <- function(xvar) {
 (-0.0126 + 0.0968*xvar)
}

# plot
ggplot(regression_data,
       aes(y=asian_arrest_rate,x=asian_search_rate)) +
  geom_point() +
  stat_function(fun = Lfun, geom = "line", col="red") +
  geom_label(x=Inf, y = Inf, 
           label = "y = 0.0126 + 0.0968*x \n R-sqr = 0.741", vjust=3, hjust=1, col="red")
```

### Searches

```{r}
colnames(all_data)
```

```{r}
symbox(~income_deprivation_rate, 
       all_data, 
       na.rm=T,
       powers=seq(-3,3,by=.5))

ggplot(all_data, aes(x=(cars_rate))) + 
  geom_histogram()
```

```{r}
regression_data2 <- all_data %>% 
  dplyr::select(msoa_code,
                all_population,
                mean_income_rate,
                median_income_rate,
                income_deprivation_rate,
                income_old_deprivation_rate,
                bad_health_rate,
                cars_rate,
                total_search_rate,
                total_arrest_rate,
                total_crime_rate) %>% 
  mutate(search_log = log(total_search_rate),
         crime_log = log(total_crime_rate)) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  st_drop_geometry() %>% 
  na.omit(.)

# add inner and outer london
innerOuter <- left_join(msoaMap %>% dplyr::select(MSOA11CD, LAD11CD),
                        boroData %>% dplyr::select(code,inner_outer_london),
                        by = c("LAD11CD" = "code"))

regression_data2 <- left_join(regression_data2,innerOuter,
                              by = c("msoa_code" = "MSOA11CD"))

# change to a factor
regression_data2<- regression_data2 %>%
  mutate(inner_outer=as.factor(inner_outer_london))
```

#### Simple: S&S \~ crime+inner/outer  (R2 = 0.565)

```{r}
model_l2 <- lm(search_log ~ crime_log + inner_outer,
                  data = regression_data2)
summary(model_l2) 
```

Change the reference group to outer london

```{r}
contrasts(regression_data2$inner_outer)
```

```{r}
regression_data2 <- regression_data2 %>%
  mutate(inner_outer = relevel(inner_outer, 
                               ref="Outer London"))

model_l3 <- lm(search_log ~ crime_log + inner_outer,
                  data = regression_data2)
summary(model_l3) 
```

#### Multiple: all included 

```{r}
# build model 
model_mlr <- lm(search_log ~ crime_log + mean_income_rate + median_income_rate
                + income_deprivation_rate + income_old_deprivation_rate
                + bad_health_rate + cars_rate +inner_outer,
                  data = regression_data2)
summary(model_mlr)
```

VIF

```{r}
vif(model_mlr)
```

#### Multiple: vif proofed

```{r}
# build model 
model_final <- lm(search_log ~ crime_log  + median_income_rate
                + income_old_deprivation_rate +  cars_rate + inner_outer,
                  data = regression_data2)
summary(model_final)
```

Check VIF again and R-sq is improved to 0.615

```{r}
vif(model_final)
```

Check residual's distribution and variance

```{r}
# append residuals data to regression_data 
regression_data2 <- model_final %>%
  augment(., regression_data2)

# plot residuals
regression_data2 %>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()

# check Homoscedasticity
par(mfrow=c(2,2))    
plot(model_final)

```

Check spatial autocorrelation for the residuals

```{r message=FALSE, warning=FALSE}
# join geometry data to regression_data2
regression_data2 <- regression_data2 %>% 
  left_join(.,
            msoaMap %>% dplyr::select(MSOA11CD, geometry),
            by = c("msoa_code" = "MSOA11CD")) %>% 
  st_as_sf()

# create spatial weight matrix 
coordsW <- regression_data2%>%
  st_centroid()%>%
  st_geometry()

# KNN method
knn_wards <-coordsW %>%
  knearneigh(., k=4)

LWard_knn <- knn_wards %>%
  knn2nb()

Lward.knn_4_weight <- LWard_knn %>%
  nb2listw(., style="W")

# run moran's I test on the residuals
Nearest_neighbour <- regression_data2 %>%
  st_drop_geometry()%>%
  dplyr::select(.resid)%>%
  pull()%>%
  moran.test(., Lward.knn_4_weight)

Nearest_neighbour
```

Moran's I index is 0.25, showing a moderate spatial autocorrelation on my multiple linear regression residuals.

#### Lagged models

```{r message=FALSE, warning=FALSE, results='asis'}
#run a spatially-lagged regression model
model_slag_knn <- lagsarlm(search_log ~ crime_log  + median_income_rate
                + income_old_deprivation_rate + cars_rate + inner_outer,
                  data = regression_data2,
                  nb2listw(LWard_knn, style="W"), 
                  method = "eigen")

summary(model_slag_knn)
tidy(model_slag_knn) %>% kable()
```

Check the residuals, should not have spatial autocorrelation

```{r warning=FALSE}
regression_data2 <- regression_data2 %>%
  mutate(model_slag_knn_resids = residuals(model_slag_knn))

regression_data2 %>%
  st_drop_geometry()%>%
  dplyr::select(model_slag_knn_resids)%>%
  pull()%>%
  moran.test(., Lward.knn_4_weight)
```

#### Error Model

```{r warning=FALSE, results='asis'}
model_sem <- errorsarlm(search_log ~ crime_log  + median_income_rate
                + income_old_deprivation_rate +  cars_rate + inner_outer,
                  data = regression_data2,
                  nb2listw(LWard_knn, style="W"), 
                  method = "eigen")

summary(model_sem)
tidy(model_sem) %>% kable()
```

##### Lagrange multiple tests

LMerr: for spatially error model

LMlag: for spatially lagged model

```{r}
library(spdep)

lm.LMtests(model_final, Lward.knn_4_weight, test = c("LMerr","LMlag","RLMerr","RLMlag","SARMA"))
```

#### GWR

```{r}
regression_data2 <- regression_data2 %>%
  mutate(model_final_res = residuals(model_final))

par(mfrow=c(2,2))
plot(model_final)
```

```{r}
tm_shape(regression_data2) +
  tm_polygons("model_final_res", 
              palette = "-RdYlBu")
```

```{r}
regression_data2 %>%
  st_drop_geometry()%>%
  dplyr::select(model_final_res)%>%
  pull()%>%
  moran.test(., Lward.knn_4_weight)
```

Convert to sp object

```{r message=F, warning=F}
library(spgwr)

st_crs(regression_data2) = 27700

regression_data2_SP <- regression_data2 %>%
  as(., "Spatial")

st_crs(coordsW) = 27700
coordsWSP <- coordsW %>%
  as(., "Spatial")

coordsWSP
```

```{r}
#calculate kernel bandwidth
GWRbandwidth <- gwr.sel(search_log ~ crime_log  + median_income_rate
                + income_old_deprivation_rate +  cars_rate + inner_outer,
                  data = regression_data2_SP,
                        coords=coordsWSP,
                        adapt=T)
```

run the model

```{r}
#run the gwr model
model_gwr = gwr(search_log ~ crime_log  + median_income_rate
                + income_old_deprivation_rate +  cars_rate + inner_outer,
                  data = regression_data2_SP, 
                coords=coordsWSP, 
                adapt=GWRbandwidth, 
                hatmatrix=TRUE, 
                se.fit=TRUE)

#print the results of the model
model_gwr
```

Attach coefficients to our dataframe

```{r}
results <- as.data.frame(model_gwr$SDF)
names(results)
```

```{r}
regression_data2 <- regression_data2 %>%
  mutate(coef_CrimeLog = results$crime_log,
         coef_meadianIncome = results$median_income_rate,
         coef_oldDeprivation = results$income_old_deprivation_rate,
         coef_cars = results$cars_rate,
         coef_inner_outerInner = results$inner_outerInner.London)
```

#### se-crime 

```{r}
tm_shape(regression_data2) +
  tm_polygons(col = "coef_CrimeLog", 
              palette = "-RdBu", 
              alpha = 0.5)
```

From the above map, for 1% increase in crime rate, the resulting S&S rate can increase from 0.6% to 1.8%, showing some boroughs' stop and searches are more sensitive to increase in crime rate.

Of course, these results may not be statistically significant across the whole of London. Roughly speaking, if a coefficient estimate is more than 2 standard errors away from zero, then it is "statistically significant."

```{r}
#run the significance test
sigTest_crime = abs(model_gwr$SDF$"crime_log") - (2* model_gwr$SDF$"crime_log_se")

#store significance results
regression_data2 <- regression_data2 %>%
  mutate(se_crimeLog = sigTest_crime)

# map the standard errors
tm_shape(regression_data2) +
  tm_polygons(col = "se_crimeLog", 
              palette = "-RdYlBu")
```

#### se-median income 

```{r}
tm_shape(regression_data2) +
  tm_polygons(col = "coef_meadianIncome", 
              palette = "-RdBu", 
              alpha = 0.5)
```

```{r}
#run the significance test
sigTest_income = abs(model_gwr$SDF$"median_income_rate") - (2* model_gwr$SDF$"median_income_rate_se")

#store significance results
regression_data2 <- regression_data2 %>%
  mutate(se_medianIncome = sigTest_income)

# map the standard errors
tm_shape(regression_data2) +
  tm_polygons(col = "se_medianIncome", 
              palette = "-RdYlBu")
```

#### se-deprivation

```{r}
tm_shape(regression_data2) +
  tm_polygons(col = "coef_oldDeprivation", 
              palette = "-RdBu", 
              alpha = 0.5)
```

```{r}
#run the significance test
sigTest_deprivation = abs(model_gwr$SDF$"income_old_deprivation_rate") - (2* model_gwr$SDF$"income_old_deprivation_rate_se")

#store significance results
regression_data2 <- regression_data2 %>%
  mutate(se_deprivation = sigTest_deprivation)

# map the standard errors
tm_shape(regression_data2) +
  tm_polygons(col = "se_deprivation", 
              palette = "-RdYlBu")
```

#### cars

```{r}
tm_shape(regression_data2) +
  tm_polygons(col = "coef_cars", 
              palette = "-RdBu", 
              alpha = 0.5)
```

```{r}
#run the significance test
sigTest_cars = abs(model_gwr$SDF$"cars_rate") - (2* model_gwr$SDF$"cars_rate_se")

#store significance results
regression_data2 <- regression_data2 %>%
  mutate(se_cars = sigTest_cars)

# map the standard errors
tm_shape(regression_data2) +
  tm_polygons(col = "se_cars", 
              palette = "-RdYlBu")
```

     [9] "crime_log_se"                       "median_income_rate_se"             
    [11] "income_old_deprivation_rate_se"     "cars_rate_se"                      
    [13] "inner_outerInner.London_se"

#### Inner outer london

```{r}
tm_shape(regression_data2) +
  tm_polygons(col = "coef_inner_outerInner", 
              palette = "-RdBu", 
              alpha = 0.5)
```

```{r}
#run the significance test
sigTest_innerOuter = abs(model_gwr$SDF$"inner_outerInner.London") - (2* model_gwr$SDF$"inner_outerInner.London_se")

#store significance results
regression_data2 <- regression_data2 %>%
  mutate(se_innerOuter = sigTest_innerOuter)

# map the standard errors
tm_shape(regression_data2) +
  tm_polygons(col = "se_innerOuter", 
              palette = "-RdYlBu")
```
