---
title: "dissertation analysis"
author: "Ying Chen"
date: "19/06/2021"
output: html_document
---

```{r message=FALSE}
library(sf)
library(tidyverse)
library(dplyr)
library(janitor)
library(RColorBrewer)
library(classInt)
library(sp)
library(rgeos)
library(rgdal)
library(spatstat)
library(here)
library(maptools)
library(GISTools)
library(tmap)
library(geojson)
library(geojsonio)
library(tmaptools)
library(hexbin)
library(ggspatial)
library(ggsn)
library(raster)
library(fpc)
library(dbscan)
library(plotrix)
library(spdep)
library(ggplot2)
library(ggpubr)
library(factoextra) 
```

## Understand London Crime in general

-   spatial data: downloaded from London Datastore at <https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london> and stored in the local folder "data".
-   crime data: downloaded from <https://data.police.uk/data/> by selecting only Metropolitan Police Service for data from April 2019 to April 2021, renamed the folder as "crime_data" stored in the local folder "data".

```{r message=FALSE}
boroMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

wardMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Ward_CityMerged.shp")) %>% 
  st_transform(., 27700)
```

The following function used to search files that follow a specific pattern is obtained from <https://github.com/sjaraha/clustering-spatiotemporal-data>.

```{r}
list_data_paths <- function(pattern, rec){
  # searches working directory for files that match the specified pattern
  # on match, adds file path to a list
  # returns list the list of matching file paths
  ## pattern (str): regex pattern to match
  ## rec (boolean): recurse into directories (True) or don't (False)
  
  # initialize list
  data_path_list <- c()
  # loop through directories
  for (pd in list.dirs(recursive = rec)){
    # loop through files in directories
    for (f in list.files(pd)){
      # find files that match the pattern
      if (grepl(pattern, f, ignore.case = FALSE)==TRUE){
        # construct path to matching file
        data_path <- paste(pd,f, sep="/")
        # add path to list
        data_path_list <- c(data_path_list,data_path)
      }}}
  # return list of paths to matching files
  return(data_path_list)
}
```

Use the function above and search for all crime data (ended with metropolitan-street.csv) in my local folder.

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
crime_list <- list_data_paths("\\-metropolitan-street.csv$",FALSE) 

crime <- crime_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

Remove duplicated rows, clean names and select points within London.

```{r message = FALSE}
crime<- crime %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

crime_london <- crime[wardMap,]
```

Create a table counting crime numbers for each type.

```{r message=False, warning=False, results='asis'}
library(knitr)
crime_london %>% 
  group_by(crime_type) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London crimes by types from April 2019 to April 2021')
```

I am interested in comparing violent crimes with stop and searches under one particular legislation (section 60) because section 60 is specifically targeted for violent crimes.

```{r}
violence_london <- crime_london %>% 
  filter(., crime_type=='Violence and sexual offences') 
```

What are the most common outcome for violent crimes in London?

```{r message=False, warning=False, results='asis'}
violence_london %>% 
  group_by(last_outcome_category) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by outcomes from April 2019 to April 2021')
```

I want to count the number of violent crimes by boroughs but they record crimes by LSOA, so I need to extract borough names from LSOA names. (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
violence_london$boro_name = substr(violence_london$lsoa_name,1,nchar(violence_london$lsoa_name)-4)

violence_london %>% 
  group_by(boro_name) %>% 
  summarise(., count=n(),) %>% 
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by boroughs from April 2019 to April 2021')
```

## Understand stop and search (S&S) data

Search for stop and search data (ends with -metropolitan-stop-and-search.csv).

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
ss_list <- list_data_paths("\\-metropolitan-stop-and-search.csv$",FALSE) 

ss <- ss_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

clean names, remove duplicated records and select points within London boroughs.

```{r message=FALSE, warning=False}
ss <- ss %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

ss_london <- ss[wardMap,]
```

What are the common legislation and search object during S&S?

```{r message=False, warning=False, results='asis'}
ss_london %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London S&S by legislation and search objects from April 2019 to April 2021')
```

-   The Criminal Justice and Public Order 1994 (section 60) aims to reduce violence in London. From April 2014, the UK government implements the Best Use of Stop and Search Scheme (BUSSS) and it gives more police power to stop and search people without having reasonable grounds for suspicion if violence is anticipated by police. The main purpose of the scheme is to prevent violent crimes (such as knife crime) before it happens.
-   Therefore, comparing the police usage of section 60 in terms of their locations, ethnicity of the person being S&S, time of a day, season of a year, and actual violent crime locationns within MSOA, LSOA or ward can help understand whether S&S is an effective way of deterring violent crimes at local level.

```{r}
ss60_london <- ss_london %>% 
  filter(legislation == "Criminal Justice and Public Order Act 1994 (section 60)") 
```

What are the most common ethnicity of people being S&S for potential violence? (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
ss60_london %>% 
  group_by(outcome) %>% 
  summarise(., count=n(),) %>%
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(prop)) %>% 
  st_drop_geometry() %>% 
  kable(.)
```

```{r message=False, warning=False, results='asis'}
ss60_london %>%  
  filter(outcome == "Arrest") %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., count=n()) %>%
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(prop)) %>% 
  st_drop_geometry() %>% 
  kable(.)
```

```{r}
# ss60_london %>% 
#   count(year = year(date), officer_defined_ethnicity) %>% 
#   ggplot(aes(x = year, y = n, color = officer_defined_ethnicity)) +
#   geom_point() +
#   geom_line() 
```

## Simple point distribution

```{r}
tm_shape(wardMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(violence_london) +
  tm_dots(col = "blue") +
  tm_layout(main.title="Violent Crimes London April 2019 to April 2021",
            main.title.size=1)

tm_shape(wardMap) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss_london) +
  tm_dots(col = "yellow") +
  tm_layout(main.title="Stop and Search London April 2019 to April 2021", 
            main.title.size=1)
  
tm_shape(wardMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss60_london) +
  tm_dots(col = "red") +
  tm_layout(main.title= "S&S (section 60) London April 2019 to April 2021", 
            main.title.size=1)

```

## dbscan

DBSCAN requires two parameters: 1. Epsilon - the radius within which the algorithm with search for clusters 2. MinPts - the minimum number of points that should be considered a cluster.

Two ways to determine Epsilon: 1. Ripley's K - where K values above the Poisson distribution indicates areas of clustering; needs to find the K values' cutoff between above and below the Poisson distribution. 2. KNNDistance - it is the distance of each point to its k-th nearest neighbor; needs to find a knee in the plot. "The idea behind this heuristic is that points located inside of clusters will have a small k-nearest neighbor distance, because they are close to other points in the same cluster, while noise points are more isolated and will have a rather large kNN distance (Hahsler et al., 2019).

Potential way to determine MinPts: The rule of thumb for setting minPts is to use at least the number of dimensions of the dataset plus one (Hahsler et al., 2019). However, by setting MinPts = 3 (which is what Hahsler et al suggest) it results in 229 unique cluster for all S&S (s60) points, which is too many.

### Black

Observations: 1. if setting the eps as Ripley's K test suggests, the number is small and resulting in some small clusters with very few points within. 2. if setting as kNNdistplot suggests, the "knee" is usually quite large and it will give us a large cluster + some small clusters + isolated noises.

```{r message=FALSE, warning=FALSE}
window <- as.owin(wardMap)

ss60_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") 

ss60_blackSP <- ss60_black %>%  
  as(., 'Spatial') 

ss60_blackPPP <- ppp(x=ss60_blackSP@coords[,1],
                     y=ss60_blackSP@coords[,2],
                     window=window)

ss60_blackPoints<- ss60_blackSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Choose Ripley's K method to select suitable parameters:

```{r message=FALSE, warning=FALSE}
ss60_blackPPP %>%
  Kest(., correction="border") %>%
  plot()
  #abline(v = 450, col = "blue", lty = 2)
```

Plot:

```{r}
library(factoextra)

db_b <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 1000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_b, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=450, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  Choose KNN distance method to select suitable parameters:

```{r}
ss60_blackPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```

plot:

```{r}
db_b_KNN <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_b_KNN, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=2000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

It gives us 2 unique clusters.

### White

```{r message=FALSE, warning=FALSE}
ss60_white<- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") 

ss60_whiteSP <- ss60_white %>%  
  as(., 'Spatial') 

ss60_whitePPP <- ppp(x=ss60_whiteSP@coords[,1],
                     y=ss60_whiteSP@coords[,2],
                     window=window)

ss60_whitePoints<- ss60_whiteSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K method:

```{r message=FALSE, warning=FALSE}
ss60_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 750, col = "blue", lty = 2)
```

```{r}
db_w <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 750, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_w, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=750, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  KNNDistance method:

```{r}
ss60_whitePoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=3000)") %>% 
  abline(h = 3000, col = "red", lty = 2)
```

```{r}
db_w_KNN <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 3000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_w_KNN, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=3000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

### Asian

```{r message=FALSE, warning=FALSE}
ss60_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") 

ss60_asianSP <- ss60_asian %>% 
  as(., 'Spatial') 

ss60_asianPPP <- ppp(x=ss60_asianSP@coords[,1],
                     y=ss60_asianSP@coords[,2],
                     window=window)

ss60_asianPoints<- ss60_asianSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K:

```{r message=FALSE, warning=FALSE}
ss60_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 500, col = "blue", lty = 2)
```

```{r}
db_a <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 500, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_a, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=500, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
db_a2 <- ss60_asianPoints %>%
  dbscan::optics(.,eps = 500, minPts = 50)

plot(db_a2)

test <- extractDBSCAN(db_a2, eps_cl = 400)
plot(test)

hullplot(ss60_asianPoints, test,
         cex = TRUE,
         main="OPTICS Cluster Hulls") 
```

2.  KNNDistance

```{r}
ss60_asianPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=4000)") %>% 
  abline(h = 4000, col = "red", lty = 2)
```

```{r}
db_a_KNN <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 4000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_a_KNN, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=4000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()
```

### With an openstreet basemap

```{r message=FALSE}
library(OpenStreetMap)

# before plotting, we need to transform geometry to (x,y) coordinates and the function was found at <https://maczokni.github.io/crimemapping_textbook_bookdown/more-on-thematic-maps.html>
sfc_as_cols <- function(x, names = c("x","y")) {
  stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
  ret <- sf::st_coordinates(x)
  ret <- tibble::as_tibble(ret)
  stopifnot(length(names) == ncol(ret))
  x <- x[ , !names(x) %in% names]
  ret <- setNames(ret,names)
  dplyr::bind_cols(x,ret)
}

# add (x,y) coordinate and clusters to new columns  
ss60_asian <- sfc_as_cols(ss60_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_a$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_a <- ss60_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# create a bounding box of London
LondonBB <- boroMap %>%
  st_transform(., 4326)%>%
  st_bbox()

# set the basemap showing London 
basemap <- OpenStreetMap::openmap(c(51.2867602,-0.5103751),c(51.6918741,0.3340156), 
                                  zoom=NULL,
                                  "stamen-toner")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

# plot
dbPlotAsian <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsian
```

How about for the black people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_black <- sfc_as_cols(ss60_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_b$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_b <- ss60_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotBlack <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_black, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlack
```

For white people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_white <- sfc_as_cols(ss60_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_w$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_w <- ss60_white %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotWhite <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_white, 
             aes(longitude,latitude), 
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to White People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotWhite
```

### Overlapping map

```{r}
dbplotALL <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  geom_point(data=ss60_black, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  geom_point(data=ss60_white, 
             aes(longitude,latitude), 
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("ALL")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALL
```

## dbscan - only arrest

### Asian

```{r message=FALSE,warning=FALSE}
ss60Arr_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_asianSP <- ss60Arr_asian %>% 
  as(., 'Spatial') 

ss60Arr_asianPPP <- ppp(x=ss60Arr_asianSP@coords[,1],
                     y=ss60Arr_asianSP@coords[,2],
                     window=window)

ss60Arr_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,400)) %>% 
  abline(v = 285, col = "blue", lty = 2)
```

```{r}
ss60Arr_asianPoints<- ss60Arr_asianSP %>%
  geometry(.)%>%
  as.data.frame()

db_aArr <- ss60Arr_asianPoints %>%
  fpc::dbscan(.,eps = 285, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_aArr, ss60Arr_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()
```

```{r}
db_aArr2 <- ss60Arr_asianPoints %>%
  dbscan::optics(.,eps = 500, minPts = 3)

plot(db_aArr2)

test <- extractDBSCAN(db_aArr2, eps_cl = 400)
plot(test)

hullplot(ss60Arr_asianPoints, test,
         cex = TRUE,
         main="OPTICS Cluster Hulls")
```

```{r message=FALSE, warning=FALSE}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_asian <- sfc_as_cols(ss60Arr_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=test$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_aArr <- ss60Arr_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotAsianArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=1)+
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsianArr
```

### Black People

```{r message=False, warning=False}
ss60Arr_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_blackSP <- ss60Arr_black %>% 
  as(., 'Spatial') 

ss60Arr_blackPPP <- ppp(x=ss60Arr_blackSP@coords[,1],
                     y=ss60Arr_blackSP@coords[,2],
                     window=window)

ss60Arr_blackPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1000)) %>% 
  abline(v = 400, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}
ss60Arr_blackPoints<- ss60Arr_blackSP %>%
  geometry(.)%>%
  as.data.frame()

db_bArr <- ss60Arr_blackPoints %>%
  fpc::dbscan(.,eps = 400, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_bArr, ss60Arr_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_black <- sfc_as_cols(ss60Arr_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_bArr$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_bArr <- ss60Arr_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotBlackArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=0.3)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "left") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlackArr
```

### White

```{r message=False, warning=False}
ss60Arr_white <- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_whiteSP <- ss60Arr_white %>% 
  as(., 'Spatial') 

ss60Arr_whitePPP <- ppp(x=ss60Arr_whiteSP@coords[,1],
                     y=ss60Arr_whiteSP@coords[,2],
                     window=window)

ss60Arr_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,600)) %>% 
  abline(v = 330, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}
ss60Arr_whitePoints<- ss60Arr_whiteSP %>%
  geometry(.)%>%
  as.data.frame()

db_wArr <- ss60Arr_whitePoints %>%
  fpc::dbscan(.,eps = 330, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_wArr, ss60Arr_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_white <- sfc_as_cols(ss60Arr_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_wArr$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_wArr <- ss60Arr_white %>% 
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotWhiteArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=0.3)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "left") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotWhiteArr
```

### Overlapping map

```{r}
dbplotALLArr <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.1) +
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.1)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude), 
             size=0.1)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("ALL")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALLArr
```

## st-dbscan

The 'stdbscanr' package and related functions was obtained from Dr.Gordon McDonald's github at <https://github.com/gdmcdonald/stdbscanr>

```{r message=FALSE}
# install.packages("devtools")
# devtools::install_github("gdmcdonald/stdbscanr")
# install.packages("data.table")          

library("data.table")
library(stdbscanr)

ss60_londonTEST <- ss60_london[sample(nrow(ss60_london), 100), ] %>%  # random select 100 rows
  st_transform(., 4326) %>% 
  sfc_as_cols(., c("longitude", "latitude")) %>%    # convert geometry to coordinates
  setDT(.) %>%     # convert to data.table
  setkey(., date)  # sort by data 

# add time intervals in minues between points
ss60_londonTEST[,time_inc := as.numeric(date - shift(date), units = "mins")]

# run st-dbscan
location_with_visits <- 
  get_clusters_from_data(df = ss60_londonTEST,
                         x = "longitude", 
                         y = "latitude", 
                         t = "date",
                         eps = 0.005,  # 0.005 latitude/longitude ~ 500m either way in London
                         eps_t = 1440, # 1440 minutes = 1day
                         minpts = 2)
```

```{r results='asis'}
#Define a mode function to get the most common label
mode <- function(x) { names(which.max(table(x))) }

#data.table summary table of visits
clusters <-                                   
  location_with_visits[     
    !is.na(cluster),                         
    .(n = .N,                               
      latitude = mean(latitude),            
      longitude = mean(longitude), 
      time_spent = sum(time_inc,na.rm = T),
      ethnicity_label = mode(officer_defined_ethnicity)            
    ),    
    by=cluster] 

kable(clusters, caption = "ST-DBSCAN Cluster Labels")
```

```{r message=FALSE}
# order by time
setkey(location_with_visits, date)

library(leaflet)
# plot on leaflet map
leaflet(data = clusters) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addMarkers(popup = ~paste0("Time spent: ",round(time_spent/60, 1), " hours.<br>",
                             "S&S Ethnicity Cluster ",cluster,": ",ethnicity_label,"<br>",
                             "Counts: ", n)) %>% 
  addPolylines(data = location_with_visits, 
               lat = ~latitude, 
               lng = ~longitude)
```

## Add demographic data

We have explored the point pattern analysis for London S&S and what about the ward patterns? Where are the hotspots and how to explain them? To do that, we need to count the number of crimes and S&S for each geographic unit. Here, I chose to group by ward.

The demographic data was obtained from <https://data.london.gov.uk/dataset/ward-profiles-and-atlas>.

```{r message=FALSE, warning=FALSE}
wardData <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/150584ff-3509-4e17-91d1-315ed4557419/ward-atlas-data.csv",na = c("NA", "n/a")) %>% 
  clean_names() 
```

Clean up column names

```{r}
wardData <- wardData %>% 
  rename(old_code = "x1",
         new_code = "x2",
         borough_name = "x3",
         ward_name = "x4")
```

Notice wardData has 629 rows while the wardMap has 625 rows. After inspection, wardData's last three rows are national statistics and the first row is a part of column name, which need to be removed.

```{r message=FALSE}
wardData <- wardData[2:626,]
```

Columns we need:

> -   "population_and_age_all_ages_2011"
>
> *the above can calculate violent crime rate and overall S&S rate by dividing by the number of crimes/S&S in each geographic unit*
>
> -   "diversity_ethnic_group_5\_groups_2011_census_white",
> -   "diversity_ethnic_group_5\_groups_2011_census_asian_or_asian_british",
> -   "diversity_ethnic_group_5\_groups_2011_census_black_or_black_british",
> -   "diversity_ethnic_group_5\_groups_2011_census_other",
>
> *the above can calculate specific S&S rate for each ethnic group because the S&S data also has ethnic data*
>
> -   "household_income_mean_modelled_household_income_2012_13",
> -   "employment_economic_activity_percentages_2011_census_economically_inactive_percent",
> -   "employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
> -   "qualifications_qualifications_and_students_2011_census_percent_no_qualifications",
>
> *the above might be useful if to investigate spatial autocorrelation between S&S rate and income/economically inactive rate*

```{r message=FALSE, warning=FALSE}
cols <- c(
"new_code",
"population_and_age_all_ages_2011",
"diversity_ethnic_group_5_groups_2011_census_white",
"diversity_ethnic_group_5_groups_2011_census_asian_or_asian_british",
"diversity_ethnic_group_5_groups_2011_census_black_or_black_british",                         
"diversity_ethnic_group_5_groups_2011_census_other",
"household_income_mean_modelled_household_income_2012_13",
"employment_economic_activity_percentages_2011_census_economically_inactive_percent",
"employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
"qualifications_qualifications_and_students_2011_census_percent_no_qualifications")

wardDataMerged <-
  left_join(wardMap %>% dplyr::select(NAME,GSS_CODE,geometry),
            wardData %>% dplyr::select(all_of(cols)),
            by = c("GSS_CODE" = "new_code")) %>% 
  rename(all_population = "population_and_age_all_ages_2011",
         white_pop = "diversity_ethnic_group_5_groups_2011_census_white",
         asian_pop = "diversity_ethnic_group_5_groups_2011_census_asian_or_asian_british",
         black_pop = "diversity_ethnic_group_5_groups_2011_census_black_or_black_british",
         otherEth_pop = "diversity_ethnic_group_5_groups_2011_census_other",
         econ_inac_rate = "employment_economic_activity_percentages_2011_census_economically_inactive_percent",
         mean_income_rate = "household_income_mean_modelled_household_income_2012_13",
         unemployed_loneParent_rate = "employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
         no_qualification_rate = "qualifications_qualifications_and_students_2011_census_percent_no_qualifications",
         ward_name = "NAME",
         ward_code = "GSS_CODE") 

# convert column types to numeric
cols.num <- c("all_population",  "white_pop", "asian_pop",
              "black_pop", "otherEth_pop", "econ_inac_rate", "mean_income_rate",
              "unemployed_loneParent_rate", "no_qualification_rate")
wardDataMerged[cols.num] <- sapply(wardDataMerged[cols.num],as.numeric)


# calculate population percent
wardDataMerged <- wardDataMerged %>% 
  dplyr::mutate(white_pop_rate = white_pop / all_population*100,
         asian_pop_rate = asian_pop / all_population*100,
         black_pop_rate = black_pop / all_population*100,
         otherEth_pop_rate = otherEth_pop / all_population*100) %>% 
  mutate_if(is.numeric, ~round(., 2))
         
```

Count all S&S (section 60) for each ethnic group that fall within wards.

```{r message=FALSE}
ss60_count <- st_join(ss60_london, wardMap) %>% 
  group_by(GSS_CODE, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_ss60 = sum(c_across(Asian:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(ss60_black = "Black",
                ss60_white = "White",
                ss60_asian = "Asian",
                ss60_other = "Other",
                ss60_na = "NA_eth") 
```

Count all violent crimes that fall within wards.

```{r message=FALSE}
vio_count <- st_join(violence_london, wardMap) %>% 
  group_by(GSS_CODE) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(GSS_CODE)) %>% 
  st_drop_geometry() %>% 
  na.omit(.) %>% 
  dplyr::rename(total_violent_crime = "count")
```

Count successful S&S (s60) that leading to an arrest.

```{r message=FALSE, warning=FALSE}
arrest_count <- st_join(ss60_london, wardMap) %>% 
  filter(outcome=="Arrest") %>% 
  group_by(GSS_CODE, officer_defined_ethnicity) %>% 
  summarise(., arrest_count=n(),) %>%
  arrange(desc(GSS_CODE)) %>% 
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = arrest_count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_arrest = sum(c_across(Black:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(arrest_black = "Black",
                arrest_white = "White",
                arrest_asian = "Asian",
                arrest_other = "Other",
                arrest_na = "NA_eth") 

merged <- left_join(wardDataMerged, vio_count, by = c("ward_code" = "GSS_CODE")) %>% 
  left_join(., ss60_count, by = c("ward_code" = "GSS_CODE")) %>% 
  left_join(., arrest_count, by = c("ward_code" = "GSS_CODE"))
  
```

Calculate rates.

```{r message=FALSE}
all_data <- merged %>% 
  clean_names() %>%   
  # total violent crime rate 
  mutate(total_vio_rate = total_violent_crime/
           all_population *100) %>% 
  mutate(total_vio_prop = total_violent_crime/
           sum(total_violent_crime) *100) %>% 
  # total S&S rate
  mutate(total_search_rate = total_ss60 / 
           all_population *100) %>%
  # total hit rate 
  mutate(total_hit_rate = total_arrest /
           total_ss60 *100) %>% 
  # search rate for ethical groups 
  mutate(black_search_rate = ss60_black/ black_pop *100) %>% 
  mutate(white_search_rate = ss60_white/ white_pop *100) %>% 
  mutate(asian_search_rate = ss60_asian/ asian_pop *100) %>% 
  mutate(other_search_rate = ss60_other/ other_eth_pop *100) %>% 
  # hit rates for ethical groups 
  mutate(black_hit_rate = arrest_black/ ss60_black *100) %>% 
  mutate(white_hit_rate = arrest_white/ ss60_white *100) %>% 
  mutate(asian_hit_rate = arrest_asian/ ss60_asian *100) %>% 
  mutate(other_hit_rate = arrest_other/ ss60_other *100) %>% 
  mutate(na_hit_rate = arrest_na/ ss60_na *100) %>% 
  mutate_if(is.numeric, ~round(., 2))

```

### Overall trend

Hit rate is, what proportion of searches, by race, were successful? If racial groups have different hit rates, it can imply that racial groups are being subjected to different standards.

```{r message=False, warning=False, results='asis'}
# count the number of searches by race
HR1 <- ss60_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., searches=n()) %>% 
  st_drop_geometry()
  
# count the number of successful searches leading to arrest 
HR2 <- ss60_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  filter(outcome=="Arrest") %>% 
  summarise(., arrests=n()) %>% 
  st_drop_geometry()

# calculate total population for each race based on 2011 census 
aP <- sum(as.numeric(all_data$asian_pop), na.rm = TRUE)
bP <- sum(as.numeric(all_data$black_pop), na.rm = TRUE)
wP <- sum(as.numeric(all_data$white_pop), na.rm = TRUE)
oP <- sum(as.numeric(all_data$other_eth_pop), na.rm = TRUE)

left_join(HR1, HR2) %>% 
  mutate(search_prop = searches/sum(searches) *100,
         arrest_prop = arrests/sum(arrests) *100,
         population = c(aP, bP, oP, wP, 0),
         population_prop = population/sum(population) *100,
         search_rate = searches/population *100,
         hit_rate = arrests/searches *100) %>% 
  dplyr::rename(race = "officer_defined_ethnicity") %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  kable(.,)
```

### Search rate & Hit rate comparison

```{r message=FALSE, warning=FALSE}
# select columns we need 
cols_s <- c("ward_code",
          "black_search_rate", 
          "white_search_rate", 
          "asian_search_rate", 
          "other_search_rate")
cols_h <- c("black_hit_rate",
            "white_hit_rate",
            "asian_hit_rate",
            "other_hit_rate",
            "ward_code")
search_data <- all_data[cols_s]
hit_data <- all_data[cols_h]

# pivot longer 
search_df <- search_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("search_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_') 

hit_df <- hit_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("hit_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_')
# join two df
main_data <- left_join(search_df, hit_df) %>% 
  rename(search_rate = "search",
         hit_rate = "hit") 

# convert NAs to zero
main_data[is.na(main_data)] <- 0
```

Compare search rate vs. hit rate for each race

```{r message=FALSE, warning=FALSE}
# We'll use this just to make our axes' limits nice and even
max_hit_rate <-
  main_data %>% 
  dplyr::select("hit_rate") %>% 
  max()

max_search_rate <-
  main_data %>% 
  dplyr::select("search_rate") %>% 
  max()

main_data %>% 
  ggplot(aes(
    x = search_rate,
    y = hit_rate
  )) +
  geom_point() +
  # This sets a diagonal reference line (line of equal search and hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("Search rate (%)", 
    limits = c(0, max_search_rate + 0.01)
  ) +
  scale_y_continuous("Hit rate (%)", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  facet_wrap(. ~ race, nrow = 2)
```

Compare white hit rate with minorities' hit rate

```{r}
# Reshape table to show hit rates of minorities vs white 
main_data_white <- main_data %>% 
  dplyr::select(-search_rate) %>% 
  spread(race, hit_rate, fill=0) %>% 
  rename(white_hit_rate = white) %>% 
  gather(minority_race, minority_hit_rate, c(black, asian, other)) %>%
  arrange(ward_code)
```

Plot

```{r}
max_hit_rate <-
  main_data_white %>% 
  dplyr::select(ends_with("hit_rate")) %>% 
  max()

main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point() +
  # This sets a diagonal reference line (line of equal hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  # This allows us to compare black v. white and Hispanic v. white side by
  # side, in panels
  facet_grid(. ~ minority_race)
```

Add the number of searches to the hit rate df

```{r}
search_count <- all_data %>% 
  dplyr::select("ward_code", "ss60_asian", "ss60_black", "ss60_white", "ss60_other") %>% 
  st_drop_geometry(.) %>% 
  pivot_longer(cols = ! (ward_code | ss60_white),
               names_to = "minority_race",
               names_prefix = "ss60_",
               values_to = "minority_search_counts") %>% 
  rename(white_search_counts = ss60_white) %>% 
  arrange(ward_code)

# convert NAs to zero
search_count[is.na(search_count)] <- 0

# join df
main_data_white <- left_join(main_data_white, 
                             search_count, 
                             by = c("ward_code", "minority_race")) %>% 
  mutate(all_search_count = minority_search_counts + white_search_counts)
```

plot

```{r}
main_data_white_nonzero <- main_data_white %>% 
  filter(all_search_count != 0 )

main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point(aes(color=all_search_count, size=all_search_count), pch = 21) +
  scale_fill_viridis_c(option = "viridis",
                       aesthetics = c("colour", "fill")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  coord_fixed() +
  facet_grid(. ~ minority_race)
```

### Investigate anomalies

There are many wards that do not have any of the S&S (60), let's extract them and compare with its violent crime rate

```{r message=FALSE, warning=FALSE}
# extract wards with zero S&S 60 and compare their violent crime rate with the London average
all_data[is.na(all_data)] <- 0
no_ss <- all_data %>% 
  filter(total_ss60 == 0) %>% 
  dplyr::select(ward_code, ward_name, total_violent_crime, total_vio_prop) %>% 
  arrange(desc(total_vio_prop)) %>% 
  mutate(LDcompare = case_when(total_vio_prop > 0.1602 ~ "above London average",
                               TRUE ~ "below London average")) 
no_ss %>% 
  st_drop_geometry() %>% 
  group_by(LDcompare) %>% 
  summarise(., count=n())
```

There are 9 wards that having violent crime rate above London's average but no S&S, what are their borough name?

```{r warning=FALSE, message=FALSE}
no_ss %>% 
  filter(LDcompare == "above London average") %>% 
  st_drop_geometry() %>% 
  left_join(., 
            wardMap %>% dplyr::select(GSS_CODE, BOROUGH),
            by = c("ward_code" = "GSS_CODE")) %>% 
  group_by(BOROUGH, ward_name) %>% 
  summarise(., count=n())
```

Create a subset data of all rates for later parts.

```{r}
all_rates <- all_data %>% 
  dplyr::select(ends_with("rate") | ward_code) 
```

## Spatial autocorrelation

Create a neighborhoods list and spatial weight matrix.

```{r message=FALSE, warning=FALSE}
library(spdep)
# calculate centroids 
coordsM<- all_rates%>%
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
Lward_nb <- all_rates %>%
  poly2nb(., queen=T, snap=0.00001)  
plot(Lward_nb, st_geometry(coordsM), col="red")

# create a spatial weight object
Lward_lw <- Lward_nb %>%
  nb2listw(., style="C")
```

### Global Moran's I

variables to test for spatial autocorrelation:

```{r}
# convert NA to zeros
all_rates[is.na(all_rates)] <- 0
colnames(all_rates)
```

Moran's I test tells us whether we have clustered values (close to 1) or dispersed values (close to -1).

```{r}
# variables used for the test
variable_names <- c( "total_vio_rate",
                     "total_search_rate",
                     "total_hit_rate",
                     "black_search_rate",
                     "white_search_rate",
                     "asian_search_rate",
                     "other_search_rate",
                     "black_hit_rate",
                     "white_hit_rate",
                     "asian_hit_rate",
                     "other_hit_rate",
                     "na_hit_rate"
                     )

# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    moran.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist

```

### Geary's C

It tells us whether similar values or dissimilar values are clusering. Geary's C falls between 0 and 2; 1 means no spatial autocorrelation, \<1 - positive spatial autocorrelation or similar values clustering, \>1 - negative spatial autocorreation or dissimilar values clustering.

```{r}
# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    geary.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist
```

### General G

It tells us whether high or low values are clustering. If G \> Expected = High values clustering; if G \< expected = low values clustering.

```{r message=FALSE}
# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    globalG.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist
```

### Local Moran's I

Calculate local Moran's I for several important variables for inspecting racial discrimination of search rate and hit rate.

```{r}
moranI_total_search <- all_rates %>%
  pull(total_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_total_hit <- all_rates %>%
  pull(total_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_black_search <- all_rates %>%
  pull(black_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_search <- all_rates %>% 
  pull(white_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_search <- all_rates %>% 
  pull(asian_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_black_hit <- all_rates %>% 
  pull(black_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_hit <- all_rates %>% 
  pull(white_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_hit <- all_rates %>% 
  pull(asian_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()
```

copy the Moran's I score and the z-score standard deviation back to all_rates dataframe

```{r}
all_rates <- all_rates %>% 
  mutate(total_search_Iz = as.numeric(moranI_total_search$Z.Ii)) %>% 
  mutate(total_hit_Iz = as.numeric(moranI_total_hit$Z.Ii)) %>% 
  mutate(black_search_Iz = as.numeric(moranI_black_search$Z.Ii)) %>% 
  mutate(white_search_Iz = as.numeric(moranI_white_search$Z.Ii)) %>% 
  mutate(asian_search_Iz = as.numeric(moranI_asian_search$Z.Ii)) %>% 
  mutate(black_hit_Iz = as.numeric(moranI_black_hit$Z.Ii)) %>% 
  mutate(white_hit_Iz = as.numeric(moranI_white_hit$Z.Ii)) %>% 
  mutate(asian_hit_Iz = as.numeric(moranI_asian_hit$Z.Ii))
```

We'll set the breaks manually based on the rule that data points \>2.58 or \<-2.58 standard deviations away from the mean are significant at the 99% level (\<1% chance that autocorrelation not present); \>1.96 - \<2.58 or \<-1.96 to \>-2.58 standard deviations are significant at the 95% level (\<5% change that autocorrelation not present). \>1.65 = 90% etc.

```{r}
breaks1<-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)
MoranColours<- rev(brewer.pal(8, "RdGy"))


t1 <- tm_shape(all_rates) + 
  tm_polygons("black_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Black S&S search rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(a)", position=c(0,0.8), size=1.5)


t2 <- tm_shape(all_rates) + 
  tm_polygons("white_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, White S&S search rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.8), size=1.5)

t3 <- tm_shape(all_rates) + 
  tm_polygons("asian_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Asian S&S search rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.8), size=1.5)

t4 <- tm_shape(all_rates) + 
  tm_polygons("black_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Black S&S hit rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.8), size=1.5)

t5 <- tm_shape(all_rates) + 
  tm_polygons("white_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, White S&S hit rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.8), size=1.5)

t6 <- tm_shape(all_rates) + 
  tm_polygons("asian_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Asian S&S hit rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.8), size=1.5)


t=tmap_arrange(t1, t2, t3, t4, t5, t6)

t
```

## Agglomerative Hierarchical Clustering

Detailed tutorial found at <https://uc-r.github.io/hc_clustering>

```{r message=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization

df <- subset(all_rates,select=-ward_code) %>% 
  st_drop_geometry() %>% 
  scale(.)
```

To determine which clustering method I choose use, agnes function can calculate the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac) 
```

Clearly, Ward method can identify the strongest clustering structures.

```{r}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
hc <- hclust(d, method = "ward.D2")

# Plot the obtained dendrogram
plot(hc, hang=-1, labels=FALSE)
```

To determine the optimal cluster number:

1\. Elbow Method 2. Average Silhouette Method (haven't explored yet)

```{r}
# use Elbow Method first  
library(factoextra)
fviz_nbclust(df, FUN = hcut, method = "wss")
```

```{r}
# Cut tree into 4 groups
sub_grp <- cutree(hc, k = 4)

# Number of members in each cluster
table(sub_grp)
```

### Explore cluster characteristics

```{r message=False, warning=False, results='asis'}
# assign clusters back to rate df
df_cluster <- subset(all_rates) %>% 
  mutate(cluster=sub_grp)

df_cluster%>% 
  st_drop_geometry() %>% 
  group_by(cluster) %>% 
  summarise(across(econ_inac_rate:na_hit_rate, median)) %>% 
  kable(., caption = 'Cluster Statistics')
```

Plot the cluster map

```{r}
tm_shape(df_cluster) + 
  tm_fill("cluster", palette = "viridis")
```

## Regression Models

From the hierarchical clustering result we can see variables such as unemployment rate, lone parent without employment rate, etc are positively correlated with search rate and hit rate. Therefore, it is reasonable to investigate their relationships more thoroughly.

### Observation plot

```{r}
tm1 <- tm_shape(all_rates) + 
  tm_fill("mean_income_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="mean income rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(a)", position=c(0,0.6), size=1)

tm2 <- tm_shape(all_rates) + 
  tm_fill("econ_inac_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="economic inactivity rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.6), size=1)


tm3 <- tm_shape(all_rates) + 
  tm_fill("unemployed_lone_parent_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="lone parent not in employment rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.6), size=1)

tm4 <- tm_shape(all_rates) + 
  tm_fill("no_qualification_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="no qualification rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.6), size=1)

tm5 <- tm_shape(all_rates) + 
  tm_fill("total_vio_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards violent crime rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.6), size=1)

tm6 <- tm_shape(all_rates) + 
  tm_fill("total_search_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards S&S (section 60) search rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.6), size=1)

tm7 <- tm_shape(all_rates) + 
  tm_fill("total_hit_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards S&S (section 60) hit rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(g)", position=c(0,0.6), size=1)

tm =tmap_arrange(tm1, tm2, tm3, tm4, tm5, tm6, tm7)

tm
```

### Research Hypothesis

1.  Is there a relationship between S&S (s60) search rate and arrest rate? -- Poisson regression

    -   Is increasing S&S (s60) potentially lead to more/less arrests across wards (i.e. are arrests distributed randomly or there's some pattern) ?

    -   What about for different minority groups, will the outcome be different?

2.  Is there a relationship between S&S (s60) search rate and violent crime rate?

    -   Is increasing S&S (s60) potentially lead to more/less violent crime across wards?

3.  What are the factors that might lead to variation in search rate across the city? -- Linear regression

```{r}
colnames(all_data)
```

### Linear regression

Assumption 1: there is a linear relationship between my variables --

-- yes, there are some linear patterns after log transformation but the histogram is still skewed.

```{r message=FALSE, warning=FALSE}
library(broom)
library(ggpubr)
library(car)
# first calculate the number of ss/arrest divided by population for 1000 population
regression_data <- all_data %>% 
  mutate(total_search_1000 = total_ss60/all_population *1000,
         total_arrest_1000 = total_arrest/all_population *1000,
         total_search_1000log = log(total_search_1000),
         total_arrest_1000log = log(total_arrest_1000), 
         population1000 = all_population/1000) %>% 
  mutate_if(is.numeric, ~round(., 2)) %>% 
  st_drop_geometry() 

# convert Inf to 1e-9 
regression_data[sapply(regression_data, is.infinite)] <- 1e-9

# filter out ward that both logged columns == le-9 (meaning they don't have search or arrest at all)
#regression_data <-regression_data %>% 
 # filter(total_search_1000log != 1e-9 | total_arrest_1000log != 1e-9)

# check data histogram 
symbox(~total_search_1000, 
       regression_data, 
       na.rm=T,
       powers=seq(-3,3,by=.5))

ggplot(regression_data, aes(x=log(total_search_1000)))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

symbox(~total_arrest_1000, 
       regression_data, 
       na.rm=T,
       powers=seq(-3,3,by=.5))

ggplot(regression_data, aes(x=log(total_arrest_1000)))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 0.5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

# scatter plot
qplot(x = log(total_search_1000), 
      y = log(total_arrest_1000),
      data = regression_data) +
stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()

```

Assumption 2: the residuals in my model should be normally distributed

-- yes?

```{r message=FALSE, warning=FALSE}
# compute linear regression model
model1 <- regression_data %>%
  lm(total_arrest_1000log ~ total_search_1000log, data=.)

# summary stats
tidy(model1)
glance(model1)

# check residuals
regression_data <- model1 %>%
  augment(., regression_data)

#plot residuals
regression_data%>%
  dplyr::select(.resid)%>%
  pull()%>%
  qplot()+ 
  geom_histogram()
```

Assumption 3: errors/residuals in the model exhibit constant / homogeneous variance

-- clearly there's some patterns in the residuals vs fitted plot, failing to fit to a linear regression model.

```{r}
par(mfrow=c(2,2))    #plot to 2 by 2 array
plot(model1)
```

Assumption 4: independent of errors, look for spatial autocorrelation

Durbin-Watson test tests whether residuals are correlated and produces a summary statistic that ranges between 0 and 4, with 2 signifiying no autocorrelation. A value greater than 2 suggesting negative autocorrelation and and value of less than 2 indicating postitve autocorrelation.

```{r}
# Durbin-Watson test: 
durbinWatsonTest(model1) %>% 
  tidy(.)

# transform regression data to sf object
regression_data <- regression_data %>% 
  left_join(.,wardMap %>% dplyr::select(GSS_CODE,geometry),
            by = c("ward_code" = "GSS_CODE"))
regression_data <- data.frame(regression_data) %>% 
  st_as_sf()

# calculate centroids 
coordsM<- regression_data %>% 
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
Lward_nb <- regression_data %>%
  poly2nb(., queen=T, snap=0.00001)  
plot(Lward_nb, st_geometry(coordsM), col="red")

# create a spatial weight object
Lward_lw <- Lward_nb %>%
  nb2listw(., style="C")

regression_data %>% 
    pull(.resid) %>% 
    as.vector() %>% 
    moran.test(., Lward_lw, zero.policy = TRUE)

```

Moran's I statistic is 0.1, showing a very week spatial autocorrelation in my residuals, no need to worry about. However, model's residuals do not have homogeneous residuals variance and show a curvilinear pattern so I turn to Poisson regression.

### Poisson regression

Assumptions (<https://bookdown.org/roback/bookdown-BeyondMLR/ch-poissonreg.html>)

1.  **Poisson Response** The response variable is a count per unit of time or space, described by a Poisson distribution.

2.  **Independence** The observations must be independent of one another.

3.  **Mean=Variance** By definition, the mean of a Poisson random variable must be equal to its variance.

4.  **Linearity** The log of the mean rate, log(λ), must be a linear function of x.

First checking the histogram and scatter plot of the original data

```{r}
ggplot(regression_data, aes(x=total_ss60))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 5) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

ggplot(regression_data, aes(x=total_arrest))+ 
  geom_histogram(aes(y = ..density..),
                 binwidth = 1) + 
  geom_density(colour="red", 
               size=1, 
               adjust=1)

qplot(x = total_ss60, 
      y = total_arrest,
      data = regression_data) +
stat_smooth(method="glm", se=FALSE, size=1) + 
  geom_jitter()
```

Both histogram reveal the pattern often found with distributions of counts of rare events. Many wards have few or none ss60 and arrests and a few wards have a large number of ss60 or arrests making for a distribution that appears to be far from normal. Therefore, Poisson regression should be used to model our data.

Next, checking mean and variance, they should be equal if they follow Poisson distribution.

```{r}
mean(regression_data$total_arrest)
var(regression_data$total_arrest)
```

The variance is slightly larger than the mean so should not be a big problem.

#### Standard Poisson model

While a Poisson regression model is a good first choice because the responses are counts per period of time (from April 2019 to April 2021), it is important to note that the counts are not directly comparable because they come from different size wards. This issue sometimes is referred to as the need to account for *sampling effort*; in other words, we expect wards with more population to have more reports of arrests since there are more people who could be affected. We can take the differences in ward population into account by including an **offset** in our model.

```{r}
library(stats)
model2 <- glm(total_arrest ~ total_ss60, 
              family = "poisson", 
              offset = log(population1000),
              data = regression_data)
summary(model2)
```

"Cameron and Trivedi (2009) recommended using robust standard errors for the parameter estimates to control for mild violation of the distribution assumption that the variance equals the mean. We use R package **`sandwich`** below to obtain the robust standard errors and calculated the p-values accordingly."

From: <https://stats.idre.ucla.edu/r/dae/poisson-regression/>

```{r}
library(sandwich)
cov.m1 <- vcovHC(model2, type="HC0")  
std.err <- sqrt(diag(cov.m1))
r.est <- cbind(Estimate= coef(model2), "Robust SE" = std.err,
"Pr(>|z|)" = 2 * pnorm(abs(coef(model2)/std.err), lower.tail=FALSE),
LL = coef(model2) - 1.96 * std.err,
UL = coef(model2) + 1.96 * std.err)

r.est
```

The standard Poisson regression model gives us,

log(λ) = -9.8 + 0.008 ss60,

by exponentiating the coefficient of ss60 we obtain the multiplicative factor by which the mean count changes. In this case, the mean number of S&S (s60) arrests changes by a factor of e\^0.008=1.01 or increases by 0.01 % (since 1.01-1=0.01) with each additional stop and search for violating section 60. 1.01 is referred to as a **rate ratio** or **relative risk**.

We can use standard error to construct a confidence interval for β1. A 95% CI provides a range of plausible values for the ss60 coefficient and can be constructed:

(^β1−Z∗⋅SE(^β1),^β1+Z∗⋅SE(^β1))

```{r}
0.008-1.96*0.0007728539
0.008+1.96*0.0007728539
```

Exponentiating the endpoints yields a confidence interval for the relative risk; i.e., the percent change in arrest counts for each additional percent increase in ss60.

( e^0.00649^ , e^0.00951^ ) = (1.00651 , 1.00956) suggesting that we are 95% confident that the mean number in the arrest increase between 0.65% and 0.96% for each additional stop and search for section 60. The interval does not include 1 so we can conclude stop and search is significantly associated with arrests.

Deviance is a way in which to measure how the observed data deviates from the model predictions, similar to sum of squared errors in linear regression. Because we want models that minimize deviance, we calculate the drop-in-deviance when adding age to the model with no covariates (the null model).

The deviances for the null model and the model with age can be found in the model output. A residual deviance for the model with age is reported as 1143.1 with 623 df. The output also includes the deviance and degrees of freedom for the null model (1879.7 with 624 df). The drop-in-deviance is 736.6 (1879.7-1143.1) with a difference of only 1 df, so that the addition of one extra term (ss60) reduced unexplained variability by 736.6.

If the null model were true, we would expect the drop-in-deviance to follow a χ2 distribution with 1 df. Therefore, the p-value for comparing the null model to the model with ss60 is found by determining the probability that the value for a χ2 random variable with one degree of freedom exceeds 736.6, which is essentially 0. Once again, we can conclude that we have statistically significant evidence (χ2 (df=1)=736.6, p\<.001) that average count of arrest increases as stop and search increases.

```{r}
# model0 is the null/reduced model
model0 <- glm(total_arrest ~ 1, family = poisson, offset = log(population1000), 
              data = regression_data)
anova(model0, model2, test = "Chisq")
```

More formally, we are testing:

Null (reduced) Model: log⁡(λ)=β0 or β1=0

Larger (full) Model: log⁡(λ)=β0+β1age or β1≠0

In order to use the drop-in-deviance test, the models being compared must be **nested**; e.g., all the terms in the smaller model must appear in the larger model. Here the smaller model is the null model with the single term β0 and the larger model has β0 and β1, so the two models are indeed nested. For nested models, we can compare the models' residual deviances to determine whether the larger model provides a significant improvement.

**Drop-in-deviance test to compare models**

-   Compute the deviance for each model, then calculate: drop-in-deviance = residual deviance for reduced model -- residual deviance for the larger model.

-   When the reduced model is true, the drop-in-deviance ∼χd2 where d= the difference in the degrees of freedom associated with the two models (that is, the difference in the number of terms/coefficients).

-   A large drop-in-deviance favors the larger model.

```{r}
rsid <- residuals(model2)
d <- deviance(model2)
par(mfrow=c(2,2))
plot(regression_data$total_ss60, rsid)
boxplot(rsid~regression_data,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)
```

**Goodness of fit:**

The model residual deviance can be used to assess the degree to which the predicted values differ from the observed. When a model is true, we can expect the residual deviance to be distributed as a χ2 random variable with degrees of freedom equal to the model's residual degrees of freedom. Our model has a residual deviance of 1143.1 with 623 df. The probability of observing a deviance this large if the model fits is esentially 0, saying that there is significant evidence of lack-of-fit.

There are several reasons why **lack-of-fit** may be observed. (1) We may be missing important covariates or interactions; a more comprehensive data set may be needed. (2) There may be extreme observations that may cause the deviance to be larger than expected; indeed, there are very few places with very high S&S counts and the majority of places with very little S&S. (3) Lastly, there may be a problem with the Poisson model. In particular, the Poisson model has only a single parameter, λ, for each combination of the levels of the predictors which must describe both the mean and the variance. This limitation can become manifest when the variance appears to be larger than the corresponding means. In that case, the response is more variable than the Poisson model would imply, and the response is considered to be **overdispersed**.

#### Negative binomial regression

```{r}
library(MASS)
modelinb <- glm.nb(total_arrest ~ total_ss60,
                   data = re)
summary(modelinb)
```

```{r}
pchisq(modelinb$deviance, modelinb$df.residual)  # GOF test
```

Check model residuals

```{r}
rsid <- residuals(model2)
d <- deviance(model2)
par(mfrow=c(2,2))
plot(regression_data$total_ss60, rsid)
boxplot(rsid~total_ss60,xlab="S&S",ylab = "Std residuals",data = regression_data)
qqnorm(rsid, ylab="Std residuals")
qqline(rsid,col="blue",lwd=2)
hist(rsid)
```
