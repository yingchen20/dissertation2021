---
title: "dissertation analysis"
author: "Ying Chen"
date: "19/06/2021"
output: html_document
---

```{r message=FALSE}
library(sf)
library(tidyverse)
library(dplyr)
library(janitor)
library(RColorBrewer)
library(classInt)
library(sp)
library(rgeos)
library(rgdal)
library(spatstat)
library(here)
library(maptools)
library(GISTools)
library(tmap)
library(geojson)
library(geojsonio)
library(tmaptools)
library(hexbin)
library(ggspatial)
library(ggsn)
library(raster)
library(fpc)
library(dbscan)
library(plotrix)
library(spdep)
library(ggplot2)
library(ggpubr)
library(factoextra) 
```

## Understand London Crime in general

-   spatial data: downloaded from London Datastore at <https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london> and stored in the local folder "data".
-   crime data: downloaded from <https://data.police.uk/data/> by selecting only Metropolitan Police Service for data from April 2019 to April 2021, renamed the folder as "crime_data" stored in the local folder "data".

```{r message=FALSE}
boroMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

lsoaMap  <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "LSOA_2011_London_gen_MHW.shp"))%>%
  st_transform(., 27700)

wardMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Ward_CityMerged.shp")) %>% 
  st_transform(., 27700)
```

The following function used to search files that follow a specific pattern is obtained from <https://github.com/sjaraha/clustering-spatiotemporal-data>.

```{r}
list_data_paths <- function(pattern, rec){
  # searches working directory for files that match the specified pattern
  # on match, adds file path to a list
  # returns list the list of matching file paths
  ## pattern (str): regex pattern to match
  ## rec (boolean): recurse into directories (True) or don't (False)
  
  # initialize list
  data_path_list <- c()
  # loop through directories
  for (pd in list.dirs(recursive = rec)){
    # loop through files in directories
    for (f in list.files(pd)){
      # find files that match the pattern
      if (grepl(pattern, f, ignore.case = FALSE)==TRUE){
        # construct path to matching file
        data_path <- paste(pd,f, sep="/")
        # add path to list
        data_path_list <- c(data_path_list,data_path)
      }}}
  # return list of paths to matching files
  return(data_path_list)
}
```

Use the function above and search for all crime data (ended with metropolitan-street.csv) in my local folder.

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
crime_list <- list_data_paths("\\-metropolitan-street.csv$",FALSE) 

crime <- crime_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

Remove duplicated rows, clean names and select points within London.

```{r message = FALSE}
crime<- crime %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

crime_london <- crime[boroMap,]
```

Create a table counting crime numbers for each type.

```{r message=False, warning=False, results='asis'}
library(knitr)
crime_london %>% 
  group_by(crime_type) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London crimes by types from April 2019 to April 2021')
```

I am interested in comparing violent crimes with stop and searches under one particular legislation (section 60) because section 60 is specifically targeted for violent crimes.

```{r}
violence_london <- crime_london %>% 
  filter(., crime_type=='Violence and sexual offences') 
```

What are the most common outcome for violent crimes in London?

```{r message=False, warning=False, results='asis'}
violence_london %>% 
  group_by(last_outcome_category) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by outcomes from April 2019 to April 2021')
```

I want to count the number of violent crimes by boroughs but they record crimes by LSOA, so I need to extract borough names from LSOA names. (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
violence_london$boro_name = substr(violence_london$lsoa_name,1,nchar(violence_london$lsoa_name)-4)

violence_london %>% 
  group_by(boro_name) %>% 
  summarise(., count=n(),) %>% 
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London violent crimes by boroughs from April 2019 to April 2021')
```

## Understand stop and search (S&S) data

Search for stop and search data (ends with -metropolitan-stop-and-search.csv).

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
ss_list <- list_data_paths("\\-metropolitan-stop-and-search.csv$",FALSE) 

ss <- ss_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

clean names, remove duplicated records and select points within London boroughs.

```{r message=FALSE, warning=False}
ss <- ss %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

ss_london <- ss[boroMap,]
```

What are the common legislation and search object during S&S?

```{r message=False, warning=False, results='asis'}
ss_london %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry() %>% 
  kable(., caption = 'London S&S by legislation and search objects from April 2019 to April 2021')
```

-   The Criminal Justice and Public Order 1994 (section 60) aims to reduce violence in London. From April 2014, the UK government implements the Best Use of Stop and Search Scheme (BUSSS) and it gives more police power to stop and search people without having reasonable grounds for suspicion if violence is anticipated by police. The main purpose of the scheme is to prevent violent crimes (such as knife crime) before it happens.
-   Therefore, comparing the police usage of section 60 in terms of their locations, ethnicity of the person being S&S, time of a day, season of a year, and actual violent crime locationns within MSOA, LSOA or ward can help understand whether S&S is an effective way of deterring violent crimes at local level.

```{r}
ss60_london <- ss_london %>% 
  filter(legislation == "Criminal Justice and Public Order Act 1994 (section 60)") 
```

What are the most common ethnicity of people being S&S for potential violence? (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
ss60_london %>% 
  group_by(outcome) %>% 
  summarise(., count=n(),) %>%
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(prop)) %>% 
  st_drop_geometry() %>% 
  kable(.)
```

```{r message=False, warning=False, results='asis'}
ss60_london %>%  
  filter(outcome == "Arrest") %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., count=n()) %>%
  mutate(prop = count/sum(count) *100) %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  arrange(desc(prop)) %>% 
  st_drop_geometry() %>% 
  kable(.)
```

```{r}
# ss60_london %>% 
#   count(year = year(date), officer_defined_ethnicity) %>% 
#   ggplot(aes(x = year, y = n, color = officer_defined_ethnicity)) +
#   geom_point() +
#   geom_line() 
```

## Simple point distribution

```{r}
tm_shape(wardMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(violence_london) +
  tm_dots(col = "blue") +
  tm_layout(main.title="Violent Crimes London April 2019 to April 2021",
            main.title.size=1)

tm_shape(wardMap) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss_london) +
  tm_dots(col = "yellow") +
  tm_layout(main.title="Stop and Search London April 2019 to April 2021", 
            main.title.size=1)
  
tm_shape(wardMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss60_london) +
  tm_dots(col = "red") +
  tm_layout(main.title= "S&S (section 60) London April 2019 to April 2021", 
            main.title.size=1)

tm_shape(all_data) + 
  tm_polygons(col = "black_hit_rate", alpha = 0.5) +
  tm_layout(main.title= "Black search", 
            main.title.size=1)
```

## dbscan

DBSCAN requires two parameters: 1. Epsilon - the radius within which the algorithm with search for clusters 2. MinPts - the minimum number of points that should be considered a cluster.

Two ways to determine Epsilon: 1. Ripley's K - where K values above the Poisson distribution indicates areas of clustering; needs to find the K values' cutoff between above and below the Poisson distribution. 2. KNNDistance - it is the distance of each point to its k-th nearest neighbor; needs to find a knee in the plot. "The idea behind this heuristic is that points located inside of clusters will have a small k-nearest neighbor distance, because they are close to other points in the same cluster, while noise points are more isolated and will have a rather large kNN distance (Hahsler et al., 2019).

Potential way to determine MinPts: The rule of thumb for setting minPts is to use at least the number of dimensions of the dataset plus one (Hahsler et al., 2019). However, by setting MinPts = 3 (which is what Hahsler et al suggest) it results in 229 unique cluster for all S&S (s60) points, which is too many.

### Black

Observations: 1. if setting the eps as Ripley's K test suggests, the number is small and resulting in some small clusters with very few points within. 2. if setting as kNNdistplot suggests, the "knee" is usually quite large and it will give us a large cluster + some small clusters + isolated noises.

```{r message=FALSE, warning=FALSE}
window <- as.owin(wardMap)

ss60_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") 

ss60_blackSP <- ss60_black %>%  
  as(., 'Spatial') 

ss60_blackPPP <- ppp(x=ss60_blackSP@coords[,1],
                     y=ss60_blackSP@coords[,2],
                     window=window)

ss60_blackPoints<- ss60_blackSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Choose Ripley's K method to select suitable parameters:

```{r message=FALSE, warning=FALSE}
ss60_blackPPP %>%
  Kest(., correction="border") %>%
  plot()
  #abline(v = 450, col = "blue", lty = 2)
```

Plot:

```{r}
library(factoextra)

db_b <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 1000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_b, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=450, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  Choose KNN distance method to select suitable parameters:

```{r}
ss60_blackPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```

plot:

```{r}
db_b_KNN <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_b_KNN, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=2000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

It gives us 2 unique clusters.

### White

```{r message=FALSE, warning=FALSE}
ss60_white<- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") 

ss60_whiteSP <- ss60_white %>%  
  as(., 'Spatial') 

ss60_whitePPP <- ppp(x=ss60_whiteSP@coords[,1],
                     y=ss60_whiteSP@coords[,2],
                     window=window)

ss60_whitePoints<- ss60_whiteSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K method:

```{r message=FALSE, warning=FALSE}
ss60_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 750, col = "blue", lty = 2)
```

```{r}
db_w <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 750, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_w, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=750, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

2.  KNNDistance method:

```{r}
ss60_whitePoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=3000)") %>% 
  abline(h = 3000, col = "red", lty = 2)
```

```{r}
db_w_KNN <- ss60_whitePoints %>%
  fpc::dbscan(.,eps = 3000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_w_KNN, ss60_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=3000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

### Asian

```{r message=FALSE, warning=FALSE}
ss60_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") 

ss60_asianSP <- ss60_asian %>% 
  as(., 'Spatial') 

ss60_asianPPP <- ppp(x=ss60_asianSP@coords[,1],
                     y=ss60_asianSP@coords[,2],
                     window=window)

ss60_asianPoints<- ss60_asianSP %>%
  geometry(.)%>%
  as.data.frame()
```

1.  Ripley's K:

```{r message=FALSE, warning=FALSE}
ss60_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 500, col = "blue", lty = 2)
```

```{r}
db_a <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 500, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_a, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=500, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
db_a2 <- ss60_asianPoints %>%
  dbscan::optics(.,eps = 500, minPts = 50)

plot(db_a2)

test <- extractDBSCAN(db_a2, eps_cl = 400)
plot(test)

hullplot(ss60_asianPoints, test,
         cex = TRUE,
         main="OPTICS Cluster Hulls") 
```

2.  KNNDistance

```{r}
ss60_asianPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=4000)") %>% 
  abline(h = 4000, col = "red", lty = 2)
```

```{r}
db_a_KNN <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 4000, MinPts = 50)

theme_set(theme_minimal())

fviz_cluster(db_a_KNN, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=4000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()
```

### With an openstreet basemap

```{r message=FALSE}
library(OpenStreetMap)

# before plotting, we need to transform geometry to (x,y) coordinates and the function was found at <https://maczokni.github.io/crimemapping_textbook_bookdown/more-on-thematic-maps.html>
sfc_as_cols <- function(x, names = c("x","y")) {
  stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
  ret <- sf::st_coordinates(x)
  ret <- tibble::as_tibble(ret)
  stopifnot(length(names) == ncol(ret))
  x <- x[ , !names(x) %in% names]
  ret <- setNames(ret,names)
  dplyr::bind_cols(x,ret)
}

# add (x,y) coordinate and clusters to new columns  
ss60_asian <- sfc_as_cols(ss60_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_a$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_a <- ss60_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# create a bounding box of London
LondonBB <- boroMap %>%
  st_transform(., 4326)%>%
  st_bbox()

# set the basemap showing London 
basemap <- OpenStreetMap::openmap(c(51.2867602,-0.5103751),c(51.6918741,0.3340156), 
                                  zoom=NULL,
                                  "stamen-toner")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

# plot
dbPlotAsian <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsian
```

How about for the black people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_black <- sfc_as_cols(ss60_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_b$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_b <- ss60_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotBlack <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_black, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlack
```

For white people?

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60_white <- sfc_as_cols(ss60_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_w$cluster)  # here I chose the Ripley's K method of generating clusters 

# create convex hull polygons to wrap around the points in our clusters
hulls_w <- ss60_white %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises


# plot
dbPlotWhite <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_white, 
             aes(longitude,latitude), 
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to White People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotWhite
```

### Overlapping map

```{r}
dbplotALL <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60_asian, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3) +
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  geom_point(data=ss60_black, 
             aes(longitude,latitude),
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_b, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  geom_point(data=ss60_white, 
             aes(longitude,latitude), 
             size=0.1, alpha=0.3)+
  geom_polygon(data = hulls_w, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("ALL")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALL
```

## dbscan - only arrest

### Asian

```{r message=FALSE,warning=FALSE}
ss60Arr_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_asianSP <- ss60Arr_asian %>% 
  as(., 'Spatial') 

ss60Arr_asianPPP <- ppp(x=ss60Arr_asianSP@coords[,1],
                     y=ss60Arr_asianSP@coords[,2],
                     window=window)

ss60Arr_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,400)) %>% 
  abline(v = 285, col = "blue", lty = 2)
```

```{r}
ss60Arr_asianPoints<- ss60Arr_asianSP %>%
  geometry(.)%>%
  as.data.frame()

db_aArr <- ss60Arr_asianPoints %>%
  fpc::dbscan(.,eps = 285, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_aArr, ss60Arr_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()
```

```{r}
db_aArr2 <- ss60Arr_asianPoints %>%
  dbscan::optics(.,eps = 500, minPts = 3)

plot(db_aArr2)

test <- extractDBSCAN(db_aArr2, eps_cl = 400)
plot(test)

hullplot(ss60Arr_asianPoints, test,
         cex = TRUE,
         main="OPTICS Cluster Hulls")
```

```{r message=FALSE, warning=FALSE}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_asian <- sfc_as_cols(ss60Arr_asian, c("longitude", "latitude")) %>% 
  mutate(dbcluster=test$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_aArr <- ss60Arr_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotAsianArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=1)+
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Asian People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotAsianArr
```

### Black People

```{r message=False, warning=False}
ss60Arr_black <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_blackSP <- ss60Arr_black %>% 
  as(., 'Spatial') 

ss60Arr_blackPPP <- ppp(x=ss60Arr_blackSP@coords[,1],
                     y=ss60Arr_blackSP@coords[,2],
                     window=window)

ss60Arr_blackPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1000)) %>% 
  abline(v = 400, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}
ss60Arr_blackPoints<- ss60Arr_blackSP %>%
  geometry(.)%>%
  as.data.frame()

db_bArr <- ss60Arr_blackPoints %>%
  fpc::dbscan(.,eps = 400, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_bArr, ss60Arr_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_black <- sfc_as_cols(ss60Arr_black, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_bArr$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_bArr <- ss60Arr_black %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotBlackArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=0.3)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "left") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotBlackArr
```

### White

```{r message=False, warning=False}
ss60Arr_white <- ss60_london %>% 
  filter(officer_defined_ethnicity == "White") %>% 
  filter(outcome == "Arrest")
  
ss60Arr_whiteSP <- ss60Arr_white %>% 
  as(., 'Spatial') 

ss60Arr_whitePPP <- ppp(x=ss60Arr_whiteSP@coords[,1],
                     y=ss60Arr_whiteSP@coords[,2],
                     window=window)

ss60Arr_whitePPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,600)) %>% 
  abline(v = 330, col = "blue", lty = 2)
```

```{r message=FALSE, warning=FALSE}
ss60Arr_whitePoints<- ss60Arr_whiteSP %>%
  geometry(.)%>%
  as.data.frame()

db_wArr <- ss60Arr_whitePoints %>%
  fpc::dbscan(.,eps = 330, MinPts = 2)

theme_set(theme_minimal())

fviz_cluster(db_wArr, ss60Arr_whitePoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=400, MinPts=2)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

```

```{r}
# add (x,y) coordinate and clusters to new columns  
ss60Arr_white <- sfc_as_cols(ss60Arr_white, c("longitude", "latitude")) %>% 
  mutate(dbcluster=db_wArr$cluster)  

# create convex hull polygons to wrap around the points in our clusters
hulls_wArr <- ss60Arr_white %>% 
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull) %>% 
  filter(dbcluster >=1) # since dbcluster < 1 are noises

# plot
dbPlotWhiteArr <- autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude),
             size=0.3, 
             alpha=0.3)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  theme(legend.position = "left") +
  ggtitle("DBSCAN Cluster for S&S (s60) Only Targeted to Black People")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbPlotWhiteArr
```

### Overlapping map

```{r}
dbplotALLArr <- autoplot.OpenStreetMap(basemap_bng) +
  geom_point(data=ss60Arr_asian, 
             aes(longitude,latitude),
             size=0.1) +
  geom_polygon(data = hulls_aArr, 
               aes(longitude,latitude,
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill="blueviolet") +
  geom_point(data=ss60Arr_black, 
             aes(longitude,latitude),
             size=0.1)+
  geom_polygon(data = hulls_bArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "tomato") +
  geom_point(data=ss60Arr_white, 
             aes(longitude,latitude), 
             size=0.1)+
  geom_polygon(data = hulls_wArr, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               fill = "seagreen") +
  theme(legend.position = "none") +
  ggtitle("ALL")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
dbplotALLArr
```

## st-dbscan

The 'stdbscanr' package and related functions was obtained from Dr.Gordon McDonald's github at <https://github.com/gdmcdonald/stdbscanr>

```{r message=FALSE}
# install.packages("devtools")
# devtools::install_github("gdmcdonald/stdbscanr")
# install.packages("data.table")          

library("data.table")
library(stdbscanr)

ss60_londonTEST <- ss60_london[sample(nrow(ss60_london), 100), ] %>%  # random select 100 rows
  st_transform(., 4326) %>% 
  sfc_as_cols(., c("longitude", "latitude")) %>%    # convert geometry to coordinates
  setDT(.) %>%     # convert to data.table
  setkey(., date)  # sort by data 

# add time intervals in minues between points
ss60_londonTEST[,time_inc := as.numeric(date - shift(date), units = "mins")]

# run st-dbscan
location_with_visits <- 
  get_clusters_from_data(df = ss60_londonTEST,
                         x = "longitude", 
                         y = "latitude", 
                         t = "date",
                         eps = 0.005,  # 0.005 latitude/longitude ~ 500m either way in London
                         eps_t = 1440, # 1440 minutes = 1day
                         minpts = 2)
```

```{r results='asis'}
#Define a mode function to get the most common label
mode <- function(x) { names(which.max(table(x))) }

#data.table summary table of visits
clusters <-                                   
  location_with_visits[     
    !is.na(cluster),                         
    .(n = .N,                               
      latitude = mean(latitude),            
      longitude = mean(longitude), 
      time_spent = sum(time_inc,na.rm = T),
      ethnicity_label = mode(officer_defined_ethnicity)            
    ),    
    by=cluster] 

kable(clusters, caption = "ST-DBSCAN Cluster Labels")
```

```{r message=FALSE}
# order by time
setkey(location_with_visits, date)

library(leaflet)
# plot on leaflet map
leaflet(data = clusters) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addMarkers(popup = ~paste0("Time spent: ",round(time_spent/60, 1), " hours.<br>",
                             "S&S Ethnicity Cluster ",cluster,": ",ethnicity_label,"<br>",
                             "Counts: ", n)) %>% 
  addPolylines(data = location_with_visits, 
               lat = ~latitude, 
               lng = ~longitude)
```

## Add demographic data

We have explored the point pattern analysis for London S&S and how about looking at MSOAs, wards or boroughs as a whole? Where are the hotspots and how to explain them? To do that, we need to count the number of crimes and S&S for each geographic unit. Here, I chose to group by MSOA (averaged with 8,346 population in 2010) and there are 983 MSOAs in London.

The demographic data was obtained from <https://data.london.gov.uk/dataset/msoa-atlas>.

```{r message=FALSE, warning=FALSE}
wardData <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/150584ff-3509-4e17-91d1-315ed4557419/ward-atlas-data.csv",na = c("NA", "n/a")) %>% 
  clean_names() 
```

Clean up column names

```{r}
wardData <- wardData %>% 
  rename(old_code = "x1",
         new_code = "x2",
         borough_name = "x3",
         ward_name = "x4")
```

Notice wardData has 629 rows while the wardMap has 625 rows. After inspection, wardData's last three rows are national statistics and the first row is a part of column name, which need to be removed.

```{r message=FALSE}
wardData <- wardData[2:626,]
```

Columns we need:

> -   "population_and_age_all_ages_2011"
>
> *the above can calculate violent crime rate and overall S&S rate by dividing by the number of crimes/S&S in each geographic unit*
>
> -   "diversity_ethnic_group_5\_groups_2011_census_white",
> -   "diversity_ethnic_group_5\_groups_2011_census_asian_or_asian_british",
> -   "diversity_ethnic_group_5\_groups_2011_census_black_or_black_british",
> -   "diversity_ethnic_group_5\_groups_2011_census_other",
>
> *the above can calculate specific S&S rate for each ethnic group because the S&S data also has ethnic data*
>
> -   "household_income_mean_modelled_household_income_2012_13",
> -   "employment_economic_activity_percentages_2011_census_economically_inactive_percent",
> -   "employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
> -   "qualifications_qualifications_and_students_2011_census_percent_no_qualifications",
>
> *the above might be useful if to investigate spatial autocorrelation between S&S rate and income/economically inactive rate*

```{r message=FALSE, warning=FALSE}
cols <- c(
"new_code",
"population_and_age_all_ages_2011",
"diversity_ethnic_group_5_groups_2011_census_white",
"diversity_ethnic_group_5_groups_2011_census_asian_or_asian_british",
"diversity_ethnic_group_5_groups_2011_census_black_or_black_british",                         
"diversity_ethnic_group_5_groups_2011_census_other",
"household_income_mean_modelled_household_income_2012_13",
"employment_economic_activity_percentages_2011_census_economically_inactive_percent",
"employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
"qualifications_qualifications_and_students_2011_census_percent_no_qualifications")

wardDataMerged <-
  left_join(wardMap %>% dplyr::select(NAME,GSS_CODE,geometry),
            wardData %>% dplyr::select(all_of(cols)),
            by = c("GSS_CODE" = "new_code")) %>% 
  rename(all_population = "population_and_age_all_ages_2011",
         white_pop = "diversity_ethnic_group_5_groups_2011_census_white",
         asian_pop = "diversity_ethnic_group_5_groups_2011_census_asian_or_asian_british",
         black_pop = "diversity_ethnic_group_5_groups_2011_census_black_or_black_british",
         otherEth_pop = "diversity_ethnic_group_5_groups_2011_census_other",
         econ_inac_rate = "employment_economic_activity_percentages_2011_census_economically_inactive_percent",
         mean_income_rate = "household_income_mean_modelled_household_income_2012_13",
         unemployed_loneParent_rate = "employment_lone_parent_not_in_employment_2011_census_lone_parent_not_in_employment_percent",
         no_qualification_rate = "qualifications_qualifications_and_students_2011_census_percent_no_qualifications",
         ward_name = "NAME",
         ward_code = "GSS_CODE") 

# convert column types to numeric
cols.num <- c("all_population",  "white_pop", "asian_pop",
              "black_pop", "otherEth_pop", "econ_inac_rate", "mean_income_rate",
              "unemployed_loneParent_rate", "no_qualification_rate")
wardDataMerged[cols.num] <- sapply(wardDataMerged[cols.num],as.numeric)


# calculate population percent
wardDataMerged <- wardDataMerged %>% 
  dplyr::mutate(white_pop_rate = white_pop / all_population*100,
         asian_pop_rate = asian_pop / all_population*100,
         black_pop_rate = black_pop / all_population*100,
         otherEth_pop_rate = otherEth_pop / all_population*100) %>% 
  mutate_if(is.numeric, ~round(., 2))
         
```

Count all S&S (section 60) for each ethnic group that fall within wards.

```{r message=FALSE}
ss60_count <- st_join(ss60_london, wardMap) %>% 
  group_by(GSS_CODE, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_ss60 = sum(c_across(Asian:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(ss60_black = "Black",
                ss60_white = "White",
                ss60_asian = "Asian",
                ss60_other = "Other",
                ss60_na = "NA_eth") 
```

Count all violent crimes that fall within wards.

```{r message=FALSE}
vio_count <- st_join(violence_london, wardMap) %>% 
  group_by(GSS_CODE) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(GSS_CODE)) %>% 
  st_drop_geometry() %>% 
  na.omit(.) %>% 
  dplyr::rename(total_violent_crime = "count")
```

Count successful S&S (s60) that leading to an arrest.

```{r message=FALSE, warning=FALSE}
arrest_count <- st_join(ss60_london, wardMap) %>% 
  filter(outcome=="Arrest") %>% 
  group_by(GSS_CODE, officer_defined_ethnicity) %>% 
  summarise(., arrest_count=n(),) %>%
  arrange(desc(GSS_CODE)) %>% 
  st_drop_geometry() %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = arrest_count) %>% 
  rename(NA_eth = "NA") %>% 
  mutate(total_arrest = sum(c_across(Black:NA_eth), na.rm = TRUE)) %>% 
  dplyr::rename(arrest_black = "Black",
                arrest_white = "White",
                arrest_asian = "Asian",
                arrest_other = "Other",
                arrest_na = "NA_eth") 

merged <- left_join(wardDataMerged, vio_count, by = c("ward_code" = "GSS_CODE")) %>% 
  left_join(., ss60_count, by = c("ward_code" = "GSS_CODE")) %>% 
  left_join(., arrest_count, by = c("ward_code" = "GSS_CODE"))
  
```

Calculate rates.

```{r message=FALSE}
all_data <- merged %>% 
  clean_names() %>%   
  # total violent crime rate 
  mutate(total_vio_rate = total_violent_crime/
           all_population *100) %>% 
  # total S&S rate
  mutate(total_search_rate = total_ss60 / 
           all_population *100) %>%
  # total hit rate 
  mutate(total_hit_rate = total_arrest /
           total_ss60 *100) %>% 
  # search rate for ethical groups 
  mutate(black_search_rate = ss60_black/ black_pop *100) %>% 
  mutate(white_search_rate = ss60_white/ white_pop *100) %>% 
  mutate(asian_search_rate = ss60_asian/ asian_pop *100) %>% 
  mutate(other_search_rate = ss60_other/ other_eth_pop *100) %>% 
  # hit rates for ethical groups 
  mutate(black_hit_rate = arrest_black/ ss60_black *100) %>% 
  mutate(white_hit_rate = arrest_white/ ss60_white *100) %>% 
  mutate(asian_hit_rate = arrest_asian/ ss60_asian *100) %>% 
  mutate(other_hit_rate = arrest_other/ ss60_other *100) %>% 
  mutate(na_hit_rate = arrest_na/ ss60_na *100) %>% 
  mutate_if(is.numeric, ~round(., 2))

```

### Overall trend

Hit rate is, what proportion of searches, by race, were successful? If racial groups have different hit rates, it can imply that racial groups are being subjected to different standards.

```{r message=False, warning=False, results='asis'}
# count the number of searches by race
HR1 <- ss60_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., searches=n()) %>% 
  st_drop_geometry()
  
# count the number of successful searches leading to arrest 
HR2 <- ss60_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  filter(outcome=="Arrest") %>% 
  summarise(., arrests=n()) %>% 
  st_drop_geometry()

# calculate total population for each race based on 2011 census 
aP <- sum(as.numeric(all_data$asian_pop), na.rm = TRUE)
bP <- sum(as.numeric(all_data$black_pop), na.rm = TRUE)
wP <- sum(as.numeric(all_data$white_pop), na.rm = TRUE)
oP <- sum(as.numeric(all_data$other_eth_pop), na.rm = TRUE)

left_join(HR1, HR2) %>% 
  mutate(search_prop = searches/sum(searches) *100,
         arrest_prop = arrests/sum(arrests) *100,
         population = c(aP, bP, oP, wP, 0),
         population_prop = population/sum(population) *100,
         search_rate = searches/population *100,
         hit_rate = arrests/searches *100) %>% 
  dplyr::rename(race = "officer_defined_ethnicity") %>% 
  mutate_if(is.numeric, ~round(., 1)) %>% 
  kable(.,)
```

### Search rate & Hit rate comparison

```{r message=FALSE, warning=FALSE}
# select columns we need 
cols_s <- c("ward_code",
          "black_search_rate", 
          "white_search_rate", 
          "asian_search_rate", 
          "other_search_rate")
cols_h <- c("black_hit_rate",
            "white_hit_rate",
            "asian_hit_rate",
            "other_hit_rate",
            "ward_code")
search_data <- all_data[cols_s]
hit_data <- all_data[cols_h]

# pivot longer 
search_df <- search_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("search_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_') 

hit_df <- hit_data %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = ends_with("hit_rate"),
               names_to = c("race", ".value"),
               names_sep = '\\_')
# join two df
main_data <- left_join(search_df, hit_df) %>% 
  rename(search_rate = "search",
         hit_rate = "hit") 

# convert NAs to zero
main_data[is.na(main_data)] <- 0
```

Compare search rate vs. hit rate for each race

```{r message=FALSE, warning=FALSE}
# We'll use this just to make our axes' limits nice and even
max_hit_rate <-
  main_data %>% 
  dplyr::select("hit_rate") %>% 
  max()

max_search_rate <-
  main_data %>% 
  dplyr::select("search_rate") %>% 
  max()

main_data %>% 
  ggplot(aes(
    x = search_rate,
    y = hit_rate
  )) +
  geom_point() +
  # This sets a diagonal reference line (line of equal search and hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("Search rate (%)", 
    limits = c(0, max_search_rate + 0.01)
  ) +
  scale_y_continuous("Hit rate (%)", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  facet_wrap(. ~ race, nrow = 2)
```

Compare white hit rate with minorities' hit rate

```{r}
# Reshape table to show hit rates of minorities vs white 
main_data_white <- main_data %>% 
  dplyr::select(-search_rate) %>% 
  spread(race, hit_rate, fill=0) %>% 
  rename(white_hit_rate = white) %>% 
  gather(minority_race, minority_hit_rate, c(black, asian, other)) %>%
  arrange(ward_code)
```

Plot

```{r}
max_hit_rate <-
  main_data_white %>% 
  dplyr::select(ends_with("hit_rate")) %>% 
  max()

main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point() +
  # This sets a diagonal reference line (line of equal hit rates)
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  # These next few lines just make the axes pretty and even
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)
  ) +
  # This makes sure that 1% on the x-axis is the same as 1% on the y-axis
  coord_fixed() +
  # This allows us to compare black v. white and Hispanic v. white side by
  # side, in panels
  facet_grid(. ~ minority_race)
```

Add the number of searches to the hit rate df

```{r}
search_count <- all_data %>% 
  dplyr::select("ward_code", "ss60_asian", "ss60_black", "ss60_white", "ss60_other") %>% 
  st_drop_geometry(.) %>% 
  pivot_longer(cols = ! (ward_code | ss60_white),
               names_to = "minority_race",
               names_prefix = "ss60_",
               values_to = "minority_search_counts") %>% 
  rename(white_search_counts = ss60_white) %>% 
  arrange(ward_code)

# convert NAs to zero
search_count[is.na(search_count)] <- 0

# join df
main_data_white <- left_join(main_data_white, 
                             search_count, 
                             by = c("ward_code", "minority_race")) %>% 
  mutate(all_search_count = minority_search_counts + white_search_counts)
```

plot

```{r}
main_data_white %>% 
  ggplot(aes(
    x = white_hit_rate,
    y = minority_hit_rate
  )) +
  geom_point(aes(color=all_search_count, size=all_search_count), pch = 18) +
  scale_fill_viridis_c(option = "viridis",
                       aesthetics = c("colour", "fill")) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  scale_x_continuous("White hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  scale_y_continuous("Minority hit rate", 
    limits = c(0, max_hit_rate + 0.01)) +
  coord_fixed() +
  facet_grid(. ~ minority_race)
```

Create a subset data of all rates for later parts.

```{r}
all_rates <- all_data %>% 
  dplyr::select(ends_with("rate") | ward_code) 
```

## Spatial autocorrelation

Create a neighborhoods list and spatial weight matrix.

```{r message=FALSE, warning=FALSE}
library(spdep)
# calculate centroids 
coordsM<- all_rates%>%
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
Lward_nb <- all_rates %>%
  poly2nb(., queen=T, snap=0.00001)  
plot(Lward_nb, st_geometry(coordsM), col="red")

# create a spatial weight object
Lward_lw <- Lward_nb %>%
  nb2listw(., style="C")
```

### Global Moran's I

variables to test for spatial autocorrelation:

```{r}
# convert NA to zeros
all_rates[is.na(all_rates)] <- 0
colnames(all_rates)
```

Moran's I test tells us whether we have clustered values (close to 1) or dispersed values (close to -1).

```{r}
# variables used for the test
variable_names <- c( "total_vio_rate",
                     "total_search_rate",
                     "total_hit_rate",
                     "black_search_rate",
                     "white_search_rate",
                     "asian_search_rate",
                     "other_search_rate",
                     "black_hit_rate",
                     "white_hit_rate",
                     "asian_hit_rate",
                     "other_hit_rate",
                     "na_hit_rate"
                     )

# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    moran.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist

```

### Geary's C

It tells us whether similar values or dissimilar values are clusering. Geary's C falls between 0 and 2; 1 means no spatial autocorrelation, \<1 - positive spatial autocorrelation or similar values clustering, \>1 - negative spatial autocorreation or dissimilar values clustering.

```{r}
# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    geary.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist
```

### General G

It tells us whether high or low values are clustering. If G \> Expected = High values clustering; if G \< expected = low values clustering.

```{r message=FALSE}
# function to compute coefficient
cal_coef <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    globalG.test(., Lward_lw, zero.policy = TRUE)
}

datalist = list()
for (aVar in variable_names) {
  dat <- list(cal_coef(x = aVar)$estimate)
  datalist[[aVar]] <- dat
}
datalist
```

### Local Moran's I

Calculate local Moran's I for several important variables for inspecting racial discrimination of search rate and hit rate.

```{r eval=FALSE, include=FALSE}
moranI_black_search <- all_rates %>%
  pull(black_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_search <- all_rates %>% 
  pull(white_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_search <- all_rates %>% 
  pull(asian_search_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_black_hit <- all_rates %>% 
  pull(black_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_white_hit <- all_rates %>% 
  pull(white_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()

moranI_asian_hit <- all_rates %>% 
  pull(asian_hit_rate) %>%
  as.vector()%>%
  localmoran(., Lward_lw, zero.policy = TRUE)%>%
  as_tibble()
```

copy the Moran's I score and the z-score standard deviation back to all_rates dataframe

```{r}
all_rates <- all_rates %>% 
  mutate(black_search_Iz = as.numeric(moranI_black_search$Z.Ii)) %>% 
  mutate(white_search_Iz = as.numeric(moranI_white_search$Z.Ii)) %>% 
  mutate(asian_search_Iz = as.numeric(moranI_asian_search$Z.Ii)) %>% 
  mutate(black_hit_Iz = as.numeric(moranI_black_hit$Z.Ii)) %>% 
  mutate(white_hit_Iz = as.numeric(moranI_white_hit$Z.Ii)) %>% 
  mutate(asian_hit_Iz = as.numeric(moranI_asian_hit$Z.Ii))
```

We'll set the breaks manually based on the rule that data points \>2.58 or \<-2.58 standard deviations away from the mean are significant at the 99% level (\<1% chance that autocorrelation not present); \>1.96 - \<2.58 or \<-1.96 to \>-2.58 standard deviations are significant at the 95% level (\<5% change that autocorrelation not present). \>1.65 = 90% etc.

```{r}
breaks1<-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)
MoranColours<- rev(brewer.pal(8, "RdGy"))


t1 <- tm_shape(all_rates) + 
  tm_polygons("black_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Black S&S search rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(a)", position=c(0,0.8), size=1.5)


t2 <- tm_shape(all_rates) + 
  tm_polygons("white_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, White S&S search rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.8), size=1.5)

t3 <- tm_shape(all_rates) + 
  tm_polygons("asian_search_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Asian S&S search rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.8), size=1.5)

t4 <- tm_shape(all_rates) + 
  tm_polygons("black_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Black S&S hit rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.8), size=1.5)

t5 <- tm_shape(all_rates) + 
  tm_polygons("white_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, White S&S hit rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.8), size=1.5)

t6 <- tm_shape(all_rates) + 
  tm_polygons("asian_hit_Iz",
        style="fixed",
        breaks=breaks1,
        palette= "viridis",
        midpoint=NA,
        title="Local Moran's I") +
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="Local Moran's I, Asian S&S hit rate in London", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.8), size=1.5)


t=tmap_arrange(t1, t2, t3, t4, t5, t6)

t
```

## Agglomerative Hierarchical Clustering

Detailed tutorial found at <https://uc-r.github.io/hc_clustering>

```{r message=FALSE}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization

df <- subset(all_rates,select=-ward_code) %>% 
  st_drop_geometry() %>% 
  scale(.)
```

To determine which clustering method I choose use, agnes function can calculate the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac) 
```

Clearly, Ward method can identify the strongest clustering structures.

```{r}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
hc <- hclust(d, method = "ward.D2")

# Plot the obtained dendrogram
plot(hc, hang=-1, labels=FALSE)
```

To determine the optimal cluster number:

1\. Elbow Method 2. Average Silhouette Method (haven't explored yet)

```{r}
# use Elbow Method first  
library(factoextra)
fviz_nbclust(df, FUN = hcut, method = "wss")
```

```{r}
# Cut tree into 4 groups
sub_grp <- cutree(hc, k = 4)

# Number of members in each cluster
table(sub_grp)
```

### Explore cluster characteristics

```{r message=False, warning=False, results='asis'}
# assign clusters back to rate df
df_cluster <- subset(all_rates) %>% 
  mutate(cluster=sub_grp)

df_cluster%>% 
  st_drop_geometry() %>% 
  group_by(cluster) %>% 
  summarise(across(econ_inac_rate:na_hit_rate, median)) %>% 
  kable(., caption = 'Cluster Statistics')
```

Plot the cluster map

```{r}
tm_shape(df_cluster) + 
  tm_fill("cluster", palette = "viridis")
```

## GWR

From the hierarchical clustering result we can see variables such as unemployment rate, lone parent without employment rate, etc are positively correlated with search rate and hit rate. Therefore, it is reasonable to investigate their relationships more thoroughly.

### Observation plot

```{r}
tm1 <- tm_shape(all_rates) + 
  tm_fill("mean_income_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="mean income rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(a)", position=c(0,0.6), size=1)

tm2 <- tm_shape(all_rates) + 
  tm_fill("econ_inac_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="economic inactivity rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(b)", position=c(0,0.6), size=1)


tm3 <- tm_shape(all_rates) + 
  tm_fill("unemployed_lone_parent_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="lone parent not in employment rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(c)", position=c(0,0.6), size=1)

tm4 <- tm_shape(all_rates) + 
  tm_fill("no_qualification_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="no qualification rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(d)", position=c(0,0.6), size=1)

tm5 <- tm_shape(all_rates) + 
  tm_fill("total_vio_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards violent crime rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(e)", position=c(0,0.6), size=1)

tm6 <- tm_shape(all_rates) + 
  tm_fill("total_search_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards S&S (section 60) search rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(f)", position=c(0,0.6), size=1)

tm7 <- tm_shape(all_rates) + 
  tm_fill("total_hit_rate", palette="viridis")+
  tm_legend(show=T)+
  tm_layout(frame=FALSE,
            main.title.position = "left",
            main.title.fontface = "bold",
            main.title="wards S&S (section 60) hit rate", 
            main.title.size=1,
            legend.title.size = 0.5,
            legend.text.size = 0.3,
            legend.outside = FALSE,
            legend.position = c(.8,0))+
  tm_credits("(g)", position=c(0,0.6), size=1)

tm =tmap_arrange(tm1, tm2, tm3, tm4, tm5, tm6, tm7)

tm
```

### Research Hypothesis

1.  Is search rate correlated (positively or negatively) to hit rate for any ethical group?
2.  Is search rate or hit rate correlated to violent crime rate -- proving police's actions are effective or not
3.  What are the factors that might lead to variation in search rate across the city?

```{r}
colnames(all_data)
```

```{r message=FALSE, warning=FALSE}
all_data[is.na(all_data)] <- 0

q <- qplot(x = ss60_black, 
           y = arrest_black,
           data = all_data)

#plot with a regression line - note, I've added some jitter here as the x-scale is rounded
q + stat_smooth(method="lm", se=FALSE, size=1) + 
  geom_jitter()
```
