---
title: "dissertation analysis"
author: "Ying Chen"
date: "19/06/2021"
output: html_document
---

```{r message=FALSE}
library(sf)
library(tidyverse)
library(dplyr)
library(janitor)
library(RColorBrewer)
library(classInt)
library(sp)
library(rgeos)
library(rgdal)
library(spatstat)
library(here)
library(maptools)
library(GISTools)
library(tmap)
library(geojson)
library(geojsonio)
library(tmaptools)
library(hexbin)
library(ggspatial)
library(ggsn)
library(raster)
library(fpc)
library(dbscan)
library(plotrix)
library(spdep)
library(ggplot2)
library(ggpubr)
```

## Understand London Crime in general

-   spatial data: downloaded from London Datastore at <https://data.london.gov.uk/dataset/statistical-gis-boundary-files-london> and stored in the local folder "data".
-   crime data: downloaded from <https://data.police.uk/data/> by selecting only Metropolitan Police Service for data from April 2019 to April 2021, renamed the folder as "crime_data" stored in the local folder "data".

```{r message=FALSE}
boroMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(., 27700)

lsoaMap  <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "LSOA_2011_London_gen_MHW.shp"))%>%
  st_transform(., 27700)

msoaMap <- st_read(here::here("data","statistical-gis-boundaries-london", "ESRI",
                              "MSOA_2011_London_gen_MHW.shp")) %>% 
  st_transform(., 27700)
```

The following function used to search files that follow a specific pattern is obtained from <https://github.com/sjaraha/clustering-spatiotemporal-data>.

```{r}
list_data_paths <- function(pattern, rec){
  # searches working directory for files that match the specified pattern
  # on match, adds file path to a list
  # returns list the list of matching file paths
  ## pattern (str): regex pattern to match
  ## rec (boolean): recurse into directories (True) or don't (False)
  
  # initialize list
  data_path_list <- c()
  # loop through directories
  for (pd in list.dirs(recursive = rec)){
    # loop through files in directories
    for (f in list.files(pd)){
      # find files that match the pattern
      if (grepl(pattern, f, ignore.case = FALSE)==TRUE){
        # construct path to matching file
        data_path <- paste(pd,f, sep="/")
        # add path to list
        data_path_list <- c(data_path_list,data_path)
      }}}
  # return list of paths to matching files
  return(data_path_list)
}
```

Use the function above and search for all crime data (ended with metropolitan-street.csv) in my local folder.

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
crime_list <- list_data_paths("\\-metropolitan-street.csv$",FALSE) 

crime <- crime_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

Remove duplicated rows, clean names and select points within London.

```{r message = FALSE}
crime<- crime %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

crime_london <- crime[boroMap,]
```

Create a table counting crime numbers for each type.

```{r message=False, warning=False, results='asis'}
library(knitr)
crimeTypes <- crime_london %>% 
  group_by(crime_type) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry()
kable(crimeTypes, caption = 'London crimes by types from April 2019 to April 2021')
```

I am interested in comparing violent crimes with stop and searches under one particular legislation (section 60) because section 60 is specifically targeted for violent crimes.

```{r}
violence_london <- crime_london %>% 
  filter(., crime_type=='Violence and sexual offences') 
```

What are the most common outcome for violent crimes in London?

```{r message=False, warning=False, results='asis'}
Tb_violenceOutcome <- violence_london %>% 
  group_by(last_outcome_category) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry()
kable(Tb_violenceOutcome, caption = 'London violent crimes by outcomes from April 2019 to April 2021')
```

I want to count the number of violent crimes by boroughs but they record crimes by LSOA, so I need to extract borough names from LSOA names. (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
violence_london$boro_name = substr(violence_london$lsoa_name,1,nchar(violence_london$lsoa_name)-4)

Tb_violenceBoro <- violence_london %>% 
  group_by(boro_name) %>% 
  summarise(., count=n(),) %>% 
  arrange(desc(count)) %>% 
  st_drop_geometry()
kable(Tb_violenceBoro, caption = 'London violent crimes by boroughs from April 2019 to April 2021')
```

## Understand stop and search (S&S) data

Search for stop and search data (ends with -metropolitan-stop-and-search.csv).

```{r echo=TRUE, message = FALSE}
setwd("/Users/yingchen/Documents2/CASA/dissertation/dissertation2021/data/crime_data")
ss_list <- list_data_paths("\\-metropolitan-stop-and-search.csv$",FALSE) 

ss <- ss_list %>% 
  lapply(read_csv) %>% 
  bind_rows
```

clean names, remove duplicated records and select points within London boroughs.

```{r message=FALSE, warning=False}
ss <- ss %>% 
  clean_names() %>% 
  distinct(.) %>% 
  filter(latitude != "NA" | longitude != "NA") %>% 
  st_as_sf(., coords = c("longitude", "latitude"), 
           crs = 4326) %>% 
  st_transform(., 27700) 

ss_london <- ss[boroMap,]
```

What are the common legislation and search object during S&S?

```{r message=False, warning=False, results='asis'}
Tb_ssLegis <- ss_london %>% 
  group_by(legislation, object_of_search) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry()
kable(Tb_ssLegis, caption = 'London S&S by legislation and search objects from April 2019 to April 2021')
```

-   The Criminal Justice and Public Order 1994 (section 60) aims to reduce violence in London. From April 2014, the UK government implements the Best Use of Stop and Search Scheme (BUSSS) and it gives more police power to stop and search people without having reasonable grounds for suspicion if violence is anticipated by police. The main purpose of the scheme is to prevent violent crimes (such as knife crime) before it happens.
-   Therefore, comparing the police usage of section 60 in terms of their locations, ethnicity of the person being S&S, time of a day, season of a year, and actual violent crime locationns within MSOA, LSOA or ward can help understand whether S&S is an effective way of deterring violent crimes at local level.

```{r}
ss60_london <- ss_london %>% 
  filter(legislation == "Criminal Justice and Public Order Act 1994 (section 60)") 
```

What are the most common ethnicity of people being S&S for potential violence? (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
Tb_ss60Outcome <- ss60_london %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry()
kable(Tb_ss60Outcome, caption = 'Ethnicities for people being S&S from April 2019 to April 2021')
```

What are the common ethnicity of a person being arrest by S&S? Any change in trend? (*later to break down into different years*)

```{r message=False, warning=False, results='asis'}
Tb_ss60Arrest <- ss60_london %>% 
  filter(outcome == "Arrest") %>% 
  group_by(officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(count)) %>% 
  st_drop_geometry()
kable(Tb_ss60Arrest, caption = 'Ethnicities for people being arrested for violating section 60 from April 2019 to April 2021')
```

## Simple point distribution

```{r}
tm_shape(msoaMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(violence_london) +
  tm_dots(col = "blue") +
  tm_layout(main.title="Violent Crimes London April 2019 to April 2021",
            main.title.size=1)

tm_shape(msoaMap) +
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss_london) +
  tm_dots(col = "yellow") +
  tm_layout(main.title="Stop and Search London April 2019 to April 2021", 
            main.title.size=1)
  
tm_shape(msoaMap) + 
  tm_polygons(col = NA, alpha = 0.5) +
  tm_shape(ss60_london) +
  tm_dots(col = "red") +
  tm_layout(main.title= "S&S (section 60) London April 2019 to April 2021", 
            main.title.size=1)
```

## Density plot

```{r eval=FALSE, include=FALSE}
# function to transform geometry to (x,y) coordinates from 
# https://maczokni.github.io/crimemapping_textbook_bookdown/more-on-thematic-maps.html
sfc_as_cols <- function(x, names = c("x","y")) {
  stopifnot(inherits(x,"sf") && inherits(sf::st_geometry(x),"sfc_POINT"))
  ret <- sf::st_coordinates(x)
  ret <- tibble::as_tibble(ret)
  stopifnot(length(names) == ncol(ret))
  x <- x[ , !names(x) %in% names]
  ret <- setNames(ret,names)
  dplyr::bind_cols(x,ret)
}
ss60_london <- sfc_as_cols(ss60_london, c("longitude", "latitude"))


ggplot(ss60_london, aes(x = longitude, y = latitude))+
  ggtitle("S&S(s60) Density London Apr 2019 to Apr 2021")+
  theme(plot.title=element_text(hjust = 0.5))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Longitude",y="Latitude")+
  annotation_map_tile(zoom=2) + 
  stat_density2d(aes(fill = ..level.., 
                     alpha = ..level..), 
                 geom = "polygon") +  
  scale_fill_gradientn(colours = c("white","red"), 
                       name = "Density")
```


## dbscan
DBSCAN requires two parameters: 
1. Epsilon -  the radius within which the algorithm with search for clusters 
2. MinPts - the minimum number of points that should be considered a cluster.

Two ways to determine Epsilon: 
1. Ripley's K - where K values above the Poisson distribution indicates areas of clustering; needs to find the K values' cutoff between above and below the Poisson distribution. 
2. KNNDistance - it is the distance of each point to its k-th nearest neighbor; needs to find a knee in the plot. "The idea behind this heuristic is that points located inside of clusters will have a small k-nearest neighbor distance, because they are close to other points in the same cluster, while noise points are more isolated and will have a rather large kNN distance (Hahsler et al., 2019).

Potential way to determine MinPts:
The rule of thumb for setting minPts is to use at least the number of dimensions of the dataset plus one (Hahsler et al., 2019). However, by setting MinPts = 3 (which is what Hahsler et al suggest) it results in 229 unique cluster for all S&S (s60) points, which is too many. 

### All S&S (s60)
```{r message=FALSE, warning=FALSE}
ss60_londonSP <- ss60_london %>%
  as(., 'Spatial') 

ss60_londonPoints <- ss60_londonSP %>%
  geometry(.)%>%
  as.data.frame()

window <- as.owin(msoaMap)
ss60_londonPPP <- ppp(x=ss60_londonSP@coords[,1],
                     y=ss60_londonSP@coords[,2],
                     window=window)
```

1. Ripley's K method
```{r message=FALSE}
ss60_londonPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 450, col = "blue", lty = 2)
```
From the output graph, we can see that up until distances of around 450 meters, S&S(s60) appear to be clustered in London MSOAs and the largest bulge in the graph at around 400 meters Increasing the distance beyond 450 meters leads the distribution becomes random and dispersed.
Therefore, setting eps to 400m or 450m is reasonable.

```{r warning=False, message=False}
db <- ss60_londonPoints %>%
  fpc::dbscan(.,eps = 450, MinPts = 50)  # 400m radius of at least 50 S&S(s60)

theme_set(theme_minimal())

dbPlot <- fviz_cluster(db, ss60_londonPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls All S&S (s60) \n (ep=450, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot
```
It gives us 33 unique clusters. 
The graph tells us this high intensity (450 meters radius --> 50 S&S for violence) is not widespread the country, only shown in few neighborhoods at Northen London.


2. KNNDistance method:

```{r message=FALSE, warning=FALSE}
ss60_londonPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=1600)") %>% 
  abline(h = 1600, col = "red", lty = 2)
```

```{r}
db_KNN <- ss60_londonPoints %>%
  fpc::dbscan(.,eps = 1600, MinPts = 50)

theme_set(theme_minimal())

dbPlot_KNN <- fviz_cluster(db_KNN, ss60_londonPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls All S&S (s60) \n (ep=1600, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_KNN
```
It gives us 5 unique clusters.
This graph tells us if we increase the neighborhood radius range to 1.6 km <more explaination needed>  



### Towards Black people

Observations: 
1. if setting the eps as Ripley's K test suggests, the number is small and resulting in some small clusters with very few points within. 
2. if setting as kNNdistplot suggests, the "knee" is usually quite large and it will give us a large cluster + some small clusters + isolated noises. 

```{r message=FALSE, warning=FALSE}
ss60_blackSP <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Black") %>% 
  as(., 'Spatial') 

ss60_blackPPP <- ppp(x=ss60_blackSP@coords[,1],
                     y=ss60_blackSP@coords[,2],
                     window=window)

ss60_blackPoints<- ss60_blackSP %>%
  geometry(.)%>%
  as.data.frame()
```

1. Choose Ripley's K method to select suitable parameters:

```{r message=FALSE, warning=FALSE}
ss60_blackPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 450, col = "blue", lty = 2)
```

Plot:
```{r}
db_b <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 450, MinPts = 50)

theme_set(theme_minimal())

dbPlot_b <- fviz_cluster(db_b, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=450, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_b
```
It gives us 14 unique clusters. The distribuion is very like the previous counting all S&S. 


2. Choose KNN distance method to select suitable parameters:

```{r}
ss60_blackPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=2000)") %>% 
  abline(h = 2000, col = "red", lty = 2)
```
plot:
```{r}
db_b_KNN <- ss60_blackPoints %>%
  fpc::dbscan(.,eps = 2000, MinPts = 50)

theme_set(theme_minimal())

dbPlot_b_KNN <- fviz_cluster(db_b_KNN, ss60_blackPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=2000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_b_KNN
```
It gives us 2 unique clusters.


### Towards Asian people

```{r message=FALSE, warning=FALSE}
ss60_asian <- ss60_london %>% 
  filter(officer_defined_ethnicity == "Asian") 

ss60_asianSP <- ss60_asian %>% 
  as(., 'Spatial') 

ss60_asianPPP <- ppp(x=ss60_asianSP@coords[,1],
                     y=ss60_asianSP@coords[,2],
                     window=window)

ss60_asianPoints<- ss60_asianSP %>%
  geometry(.)%>%
  as.data.frame()
```

1. Ripley's K:

```{r message=FALSE, warning=FALSE}
ss60_asianPPP %>%
  Kest(., correction="border") %>%
  plot(xlim=c(0,1500)) %>% 
  abline(v = 500, col = "blue", lty = 2)
```

```{r}
db_a <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 500, MinPts = 50)

theme_set(theme_minimal())

dbPlot_a <- fviz_cluster(db_a, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=500, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_a
```


2. KNNDistance

```{r}
ss60_asianPoints %>%
  dbscan::kNNdistplot(.,k=50) %>% 
  title(main="50-nearst Neighbor Distance Plot \n (MinPts=50, knee=4000)") %>% 
  abline(h = 4000, col = "red", lty = 2)
```

```{r}
db_a_KNN <- ss60_asianPoints %>%
  fpc::dbscan(.,eps = 4000, MinPts = 50)

theme_set(theme_minimal())

dbPlot_a_KNN <- fviz_cluster(db_a_KNN, ss60_asianPoints, 
                           geom = "point",
                           stand = FALSE, labelsize = NA,
                           outlier.pointsize = .8,
                           #xlab="x", ylab="y",
                           main="DBSCAN Cluster Hulls \n (ep=4000, MinPts=50)") +
theme(plot.title=element_text(size = 12, hjust = 0.5, face="bold"),
        axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())+
  coord_equal()

dbPlot_a_KNN
```

```{r}
ss60_asian <- ss60_asian %>% 
  mutate(dbcluster=db_a$cluster)

hulls_a <- ss60_asian %>%
  group_by(dbcluster) %>%
  dplyr::mutate(hull = 1:n(),
                hull = factor(hull, chull(longitude, latitude)))%>%
  arrange(hull)

hulls_a <- hulls_a %>%
  filter(dbcluster >=1)

# add a basemap
LondonBB <- boroMap %>%
  st_transform(., 4326)%>%
  st_bbox()

library(OpenStreetMap)
basemap <- OpenStreetMap::openmap(c(51.2867602,-0.5103751),c(51.6918741,0.3340156), 
                                  zoom=NULL,
                                  "stamen-toner")

# convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

# plot
autoplot.OpenStreetMap(basemap_bng) + 
  geom_point(data=ss60_asian, 
             aes(longitude,latitude, 
                 fill=dbcluster),
             size=0.3)+
  geom_polygon(data = hulls_a, 
               aes(longitude,latitude, 
                   group=dbcluster,
                   fill=dbcluster),
               alpha = 1,
               fill = "red") +
  theme(legend.position = "none") +
  ggtitle("DBSCAN-Extracted Cluster")+
  theme(plot.title=element_text(hjust = 0.5, face="bold"))+
  labs(caption = "Copyright OpenStreetMap contributors",
       x="Meter",y="Meter")
```


## st-dbscan (DISCUSSION)

```{r eval=FALSE, include=FALSE}
#install.packages("devtools")
#devtools::install_github("gdmcdonald/stdbscanr")
library(stdbscanr)

location_with_visits <- 
  get_clusters_from_data(df = ss60_londonDF,
                         x = "longitude", 
                         y = "latitude", 
                         t = "date",
                         eps = 5000,
                         eps_t = 360,
                         minpts = 1000)
```

## Make continuous observations

We have explored the point pattern analysis for London S&S and how about looking at MSOAs, wards or boroughs as a whole? Where are the hotspots and how to explain them? To do that, we need to count the number of crimes and S&S for each geographic unit. Here, I chose to group point obvervations into MSOA (averaged with 8,346 population in 2010) and there are 983 MSOAs in London.

```{r message=FALSE, warning=FALSE}
msoaData <- read_csv("https://data.london.gov.uk/download/msoa-atlas/20264159-36cb-4aa2-8371-ae884ae83e88/msoa-data.csv",na = c("NA", "n/a")) %>% 
  clean_names() 
```

Notice msoaData has 984 rows while the msoaMap has 983 rows. After inspection, the msoaData has an extra row showing the average statistics for all London MSOA, which needs to be removed.

```{r message=FALSE}
msoaData <- msoaData[1:983,]
```

Columns we need:

> -   "age_structure_2011_census_all_ages",
> -   "households_2011_all_households",
>
> *the above can calculate violent crime rate and overall S&S rate by dividing by the number of crimes/S&S in each geographic unit*
>
> -   "ethnic_group_2011_census_white",
> -   "ethnic_group_2011_census_mixed_multiple_ethnic_groups"，
> -   "ethnic_group_2011_census_asian_asian_british",
> -   "ethnic_group_2011_census_black_african_caribbean_black_british",
> -   "ethnic_group_2011_census_other_ethnic_group",
> -   "ethnic_group_2011_census_bame", ------------ ***what does "bame"mean???***
>
> *the above can calculate specific S&S rate for each ethnic group because the S&S data also has ethnic data*
>
> -   "economic_activity_2011_census_unemployment_rate",
> -   "economic_activity_2011_census_economically_inactive_percent",
>
> *the above might be useful if to calculate spatial autocorrelation between S&S rate and unemployment/economically inactive rate*

```{r message=FALSE}
cols <- c(
"middle_super_output_area",
"msoa_name",
"age_structure_2011_census_all_ages",
"households_2011_all_households",
"ethnic_group_2011_census_white",                                               
"ethnic_group_2011_census_mixed_multiple_ethnic_groups",                        
"ethnic_group_2011_census_asian_asian_british",
"ethnic_group_2011_census_black_african_caribbean_black_british",
"ethnic_group_2011_census_other_ethnic_group",
"ethnic_group_2011_census_bame",
"economic_activity_2011_census_unemployment_rate",                                
"economic_activity_2011_census_economically_inactive_percent")

msoaDataMerged <-
  left_join(msoaMap %>% dplyr::select(MSOA11CD,MSOA11NM,geometry),
            msoaData %>% dplyr::select(all_of(cols)),
            by = c("MSOA11CD" = "middle_super_output_area"))
```

Count all S&S (section 60) for each ethnic group that fall within MSOAs.

```{r message=FALSE}
ss60_joined <- st_join(ss60_london, msoaMap) %>% 
  group_by(MSOA11CD, officer_defined_ethnicity) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry()

ss60_ethnic <- ss60_joined %>% 
  pivot_wider(names_from = officer_defined_ethnicity, values_from = count)


ss60_ethnicAll <- ss60_joined %>% 
  group_by(MSOA11CD) %>% 
  summarise(., sum(count),) %>%
  arrange(desc(MSOA11CD)) 

ss60_ethicFinal <- left_join(ss60_ethnic, ss60_ethnicAll) %>% 
  dplyr::rename(ss60_black = "Black",
                ss60_white = "White",
                ss60_asian = "Asian",
                ss60_other = "Other",
                ss60_na = "NA",
                total_ss60 = "sum(count)")

```

Same thing, counted for S&S outcomes.

```{r message=FALSE}
ss60_joined2 <- st_join(ss60_london, msoaMap) %>% 
  group_by(MSOA11CD, outcome) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry()

ss60_outcome <- ss60_joined2 %>% 
  pivot_wider(names_from = outcome, values_from = count)

ss60_outcomeAll <- ss60_joined2 %>% 
  group_by(MSOA11CD) %>% 
  summarise(., sum(count),) %>%
  arrange(desc(MSOA11CD)) 

ss60_outcomeFinal <- left_join(ss60_outcome,ss60_outcomeAll) %>% 
  dplyr::rename(total_ss60_outcome = "sum(count)")
```

Count all violent crimes that fall within each MSOA

```{r message=FALSE}
vio_count <- st_join(violence_london, lsoaMap) %>% 
  group_by(MSOA11CD) %>% 
  summarise(., count=n(),) %>%
  arrange(desc(MSOA11CD)) %>% 
  st_drop_geometry() %>% 
  na.omit(.) %>% 
  dplyr::rename(total_violent_crime = "count")

merged <- left_join(msoaDataMerged, vio_count) %>% 
  left_join(., ss60_ethicFinal) %>% 
  left_join(., ss60_outcomeFinal)
```

Check if the sum of S&S by ethical group is equal to the sum of outcome

```{r}
merged$total_ss60[!(merged$total_ss60 %in% merged$total_ss60_outcome)]
```

"ineger(0)" means they are equal so we can remove one and then we can calculate the rates.

```{r message=FALSE}
all_data <- subset(merged, select = -c(total_ss60_outcome,msoa_name)) %>% 
  clean_names() %>%   
  mutate(avg_household_vio_rate = total_violent_crime/
           households_2011_all_households*100) %>% 
  mutate(avg_household_ss60_rate = total_ss60 / 
           households_2011_all_households*100) %>% 
  mutate(ss60_black_rate = ss60_black/
           ethnic_group_2011_census_black_african_caribbean_black_british*100) %>% 
  mutate(ss60_white_rate = ss60_white/
           ethnic_group_2011_census_white*100) %>% 
  mutate(ss60_asian_rate = ss60_asian/
           ethnic_group_2011_census_asian_asian_british*100) %>% 
  mutate(ss60_other_ethnicity_rate = ss60_other/
           ethnic_group_2011_census_other_ethnic_group*100) %>% 
  mutate(ss60_arrest_rate = arrest/total_ss60*100) %>% 
  mutate_if(is.numeric, ~round(., 2))

```

Create a subset data of all rates for later parts.

```{r}
all_rates <- all_data %>% 
  dplyr::select(ends_with("rate") | msoa11cd) 
```

## Spatial autocorrelation

Create a neighborhoods list and spatial weight matrix.

[**Q: when creating a neighborhoods list using poly2nb, before setting 'snap' gives me error and after setting it to 0.01 it runs successfully, but how to determine if 0.01 is a reasonable value since the out map cannot really tell anything.**]{.ul}

```{r message=FALSE, warning=FALSE}
library(spdep)
# calculate centroids of all msoas in London
coordsM<- all_rates%>%
  st_centroid()%>%
  st_geometry()
plot(coordsM,axes=TRUE)

# create a neighborhoods list
LMsoa_nb <- all_rates %>%
  poly2nb(., queen=T, snap=0.01)  
plot(LMsoa_nb, st_geometry(coordsM), col="red")

# create a spatial weight object
LMsoa_lw <- LMsoa_nb %>%
  nb2listw(., style="C")
```

### Moran's I

[**Q: Should I change all NAs in all_rates to 0? Some of the MSOAs have S&S but with a very low rate like 0.002%. Turning NAs to 0 will give places with no S&S the same color/interpretation as places with low rate, whereas keeping NAs will just show gray on the map. However, keeping NAs seems to create troubles in later statistical tests. For now, I chose to ignore NAs.**]{.ul}

```{r}
all_rates %>%
  pull(avg_household_ss60_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>%   # ignore the NA
  moran.test(., LMsoa_lw, zero.policy = TRUE)
```

The Moran's I statistic = 0.39 indicates we have a weak clustering for S&S (section 60).

[**Q: How to write a function which append the Moran I statistics to an empty dataframe for ALL variables at one click? (the following is my failed trial)**]{.ul}

```{r}
# variables used for the test
a <- c( "avg_household_vio_rate", "avg_household_ss60_rate",
        "ss60_black_rate","ss60_white_rate",
        "ss60_asian_rate","ss60_arrest_rate")

# function to compute coefficient
test <- function(x) {
  all_rates %>% 
    pull(x) %>% 
    as.vector() %>% 
    na.omit(all_rates) %>%
    moran.test(., LMsoa_lw, zero.policy = TRUE)
}

b <- c(test(x=a[2])$estimate)
b

c <- data.frame(A= numeric(0), B= numeric(0), C= numeric(0))
c <- rbind(b)
```

### Geary's C

[**Q: The following statistical tests give me errors**]{.ul}

```{r}
all_rates %>%
  pull(avg_household_vio_rate) %>%
  as.vector()%>%
  na.omit(all_rates) %>% 
  geary.test(., LMsoa_lw, zero.policy = TRUE)

```

### General G

```{r eval=FALSE, include=FALSE}
all_rates %>%
  na.omit(all_rates) %>% 
  pull(ss60_arrest_rate) %>%
  as.vector()%>%
  globalG.test(., LMsoa_lw, zero.policy = TRUE)
```

### Local Moran's I

```{r eval=FALSE, include=FALSE}
test <- all_rates %>%
  na.omit(all_rates) %>% 
  pull(avg_household_ss60_rate) %>%
  as.vector()%>%
  localmoran(., LMsoa_lw)%>%
  as_tibble()
```


## Clustering

```{r}
library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra) # clustering visualization
library(dendextend) # for comparing two dendrograms

df <- subset(all_rates,select=-msoa11cd) %>% 
  st_drop_geometry() %>% 
  scale(.)

#rownames(df) <- df$msoa11cd 
```

### Agglomerative Hierarchical Clustering

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(df, method = x)$ac
}

map_dbl(m, ac) 
```

```{r}
# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Ward Linkage
hc <- hclust(d, method = "ward.D2")

# Plot the obtained dendrogram
plot(hc, hang=-1, labels=FALSE)

# Find the optimal cluster number 
library(factoextra)
fviz_nbclust(df, FUN = hcut, method = "wss")

# Plot the desired number of clusters 
plot(hc, cex = 0.6, hang=-1, labels=FALSE)
rect.hclust(hc, k = 5, border = 2:5)
```

```{r}
# Cut tree into 5 groups
sub_grp <- cutree(hc, k = 5)

# Number of members in each cluster
table(sub_grp)

# assign clusters back to rate df
df1 <- subset(all_rates) %>% 
  mutate(cluster=sub_grp)
```

```{r message=FALSE}
# groupby cluster and see the statistical characteristics
df2 <- df1%>% 
  st_drop_geometry() %>% 
  group_by(cluster) %>% 
  na.omit(.) %>% 
  summarise(across(economic_activity_2011_census_unemployment_rate:ss60_other_ethnicity_rate, median))
df2
```

```{r}
tm_shape(df1) + 
  tm_fill("cluster",
          palette = "viridis")
```


